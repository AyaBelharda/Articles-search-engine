[
  {
    "titre": "AI Model for Computer games based on Case Based Reasoning and AI Planning",
    "resume": "Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory.",
    "auteurs": [
      "Case Based \nReasoning",
      "Vlado Menkovski",
      "Case Based \nReasoning",
      "Game Trees",
      "Game Trees and Minimax",
      "Game Trees"
    ],
    "institutions": [
      "Vlado Menkovski Athens Information Technology ",
      "Dimitrios Metafas Athens Information Technology "
    ],
    "mots_cles": [
      "Game AI",
      " Case Based Reasoning",
      " AI Planning",
      " Game Trees "
    ],
    "texte_integral": "AI Model for Computer games based on Case Based \nReasoning and AI Planning  \nVlado Menkovski \nAthens Information Technology \n0.8km Markopoulou Ave. \nPeania, 19002, Greece \nvmen@ait.edu.gr \nDimitrios Metafas \nAthens Information Technology \n0.8km Markopoulou Ave. \nPeania, 19002, Greece \ndmeta@ait.edu.gr \nAbstract\nMaking efficient AI models for games with imperfect \ninformation can be a particular challenge. Considering the large \nnumber of possible moves and the incorporated uncertainties \nbuilding game trees for these games becomes very difficult due to \nthe exponential growth of the number of nodes at each level. This \neffort is focused on presenting a method of combined Case Based \nReasoning (CBR) with AI Planning which drastically reduces the \nsize of game trees. Instead of looking at all possible combinations \nwe can focus only on the moves that lead us to specific strategies \nin effect discarding meaningless moves. These strategies are \nselected by finding similarities to cases in the CBR database. The \nstrategies are formed by a set of desired goals. The AI planning is \nresponsible for creating a plan to reach these goals. The plan is \nbasically a set of moves that brings the player to this goal. By \nfollowing these steps and not regarding the vast number of other \npossible moves the model develops Game Trees which grows \nslower so they can be built with more feature moves restricted by \nthe same amount of memory.  \nCategories and Subject Descriptors \nI.2.1 [Applications and Expert Systems]: Games\nGeneral Terms\nAlgorithms, Performance. \nKeywords\nGame AI, Case Based Reasoning, AI Planning, Game Trees \n1. Introduction \nThe goal of this effort is to explore a model for design and \nimplementation of an AI agent for turn based games. This model \nprovides for building more capable computer opponents that rely \non strategies that closely resemble human approach in solving \nproblems opposed to classical computational centric heuristics in \ngame AI. In this manner the computational resources can be \nfocused on more sensible strategies for the game play.  \nWith the advancement in computer hardware increasingly \nmore computing power is left for executing AI algorithms in \ngames. In the past AI in games was mainly a cheating set of \ninstructions that simulated the increasing difficulty in the game \nenvironment so that the player had the illusion of real counterpart. \nImprovement in available memory and processing power allows \nimplementation of more intelligent algorithms for building the \ngame environment as well as direct interaction with the human \nplayers.   \nIn this particular research the emphasis is put on the \ninteraction between the AI agent and a computer player in the \nrealm of the game rules. It is particularly focused on turn based \ngames that have the elements of uncertainty like dice or concealed \ninformation. At the beginning a description of Game AI \nalgorithms are given; such as Game Trees and Minimax. The \nfollowing section describes an approach of using AI Planning to \nimprove building Game Trees in games with imperfect \ninformation where Game Trees tend to be very large with high \ngrowth ratio. Section 4 discusses another approach that provides a \nsignificant reduction to the number of considered moves in order \nto find the favorable strategy of the AI player. This approach uses \nAI Planning techniques and Case Base Reasoning (CBR) to plan \nfor different scenarios in predetermined strategies which would be \nanalogous to human player experience in the particular game. The \nCBR database illustrates a set of past experiences for the AI \nproblem and the AI Planning illustrates the procedure to deal with \nthe given situation in the game. In the next two sections \nimplementations and evaluations of both approaches are given. \nThe AI Planning approach is implemented with the Tic-tac-toe \ngame and the combined AI Planning and CBR approach is \nimplemented with a model for the Monopoly game. The last part \ncontains conclusions and future work ideas.  \n2. Game Trees and Minimax \nGame Trees are common model for evaluating how different \ncombinations of moves from the player and his opponents will \naffect the future position of the player and eventually the end \nresult of the game. An algorithm that decides on the next move by \nevaluating the results from the built Game Tree is minimax [1]. \nMinimax assumes that the player at hand will always choose the \nbest possible move for him, in other words the player will try to \nselect the move that maximizes the result of the evaluation \nfunction over the game state. So basically the player at hand needs \nto choose the best move overall while taking into account that the \nnext player(s) will try to do the same thing. Minimax tries to \nmaximize the minimum gain. Minimax can be applied to multiple \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. To copy \notherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior specific permission and/or a fee. \nDIMEA\u201908, September 10\u201312, 2008, Athens, Greece. \nCopyright 2008 ACM 978-1-60558-248-1/08/09... $5.00 \nInteractive and Adaptable Media\n295\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\nlevels of nodes on the game tree, where the leaves bring the final \nknown (or considered) game state.  \nThe minimax theorem states: \nFor every two-person, zero-sum game there is a mixed strategy \nfor each player, such that the expected payoff for both is the same \nvalue V when the players use these strategies. Furthermore, V is \nthe best payoff each can expect to receive from a play of the \ngame; that is, these mixed strategies are the optimal strategies for \nthe two players. \nThis theorem was established by John von Neumann, who is \nquoted as saying \"As far as I can see, there could be no theory of \ngames \u2026 without that theorem \u2026 I thought there was nothing \nworth publishing until the Minimax Theorem was proved\" [2]. \nA simple example of minimax can be observed by building a \ngame tree of the tic-tac-toe game. The tic-tac-toe game is a simple \ngame which can end by the first player wining, the second player \nwining or a tie. There are nine positions for each of the players in \nwhich at each turn the player puts X or O sign. If the player has \nthree adjacent signs in a row, column or the two diagonals he or \nshe wins. This game has limited number of position and it is well \nsuited for building the whole game tree. The leaves of this tree \nwill be final positions in the game. A heuristics evaluation \nfunction will also need to be written to evaluate the value of each \nnode along the way. \n3. AI Planning for building Game Trees \n3.1.1 AI Planning \nAI Planning also referred as Automated Planning and \nScheduling is a branch of Artificial Intelligence that focuses on \nfinding strategies or sequences of actions that reach a predefined \ngoal [3]. Typical execution of AI Planning algorithms is by \nintelligent agents, autonomous robots and unmanned vehicles. \nOpposed to classical control or classification AI Planning results \nwith complex solutions that are derived from multidimensional \nspace. \n AI Planning algorithms are also common in the video game \ndevelopment. They solve broad range of problems from path \nfinding to action planning. A typical planner takes three inputs: a \ndescription of the initial state of the world, a description of the \ndesired goal, and a set of possible actions. Some efforts for \nincorporating planning techniques for building game trees have \nalso shown up, similar to the approach explored in this effort. In \naddition Cased Based Reasoning [4] techniques are also gathering \npopularity in developing strategies based in prior knowledge \nabout the problems in the games. One of the benefits from \nHierarchical Task Network (HTN) [5] planning is the possibility \nto build Game Trees based on HTN plans; this method is \ndescribed in the following section. \n3.2 Game Trees with AI Planning \nAn adaptation of the HTN planning can be used to build \nmuch smaller and more efficient game trees. This idea has already \nbeen implemented in the Bridge Baron a computer program for \nthe game of Contact Bridge [6]. \nComputer programs based on Game Tree search techniques \nare now as good as or better than humans in many games like \nChess [7] and checkers [8], but there are some difficulties in \nbuilding a game tree for games that have imperfect information \nand added uncertainty like card or games with dice. The main \nproblem is the enormous number of possibilities that the player \ncan choose from in making his move. In addition some of the \nmoves are accompanied with probabilities based on the random \nelements in the games. The number of possible moves \nexponentially grows with each move so the depth of the search \nhas to be very limited to accommodate for the memory \nlimitations.  \nThe basic idea behind using HTN for building game trees is \nthat the HTN provides the means of expressing high level goals \nand describing strategies how to reach those goals. These goals \nmay be decomposed in goals at lower level called sub-goals. This \napproach closely resembles the way a human player usually \naddresses a complex problem. It is also good for domains where \nclassical search for solution is not feasible due to the vastness of \nthe problem domain or uncertainties. \n3.2.1 Hierarchical Task Networks \nThe Hierarchical Task Network, or HTN, is an approach to \nautomated planning in which the dependency among actions can \nbe given in the form of networks [9] [Figure 1]. \nA simple task network (or just a task network for short) is an \nacyclic digraph \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd\ufffd in which U is the node set, E is the \nedge set, and each node \ufffd \ufffd \ufffd contains a task \ufffd\ufffd. The edges of \ufffd\ndefine a partial ordering of U. If the partial ordering is total, then \nwe say that \ufffd is totally ordered, in which case \ufffd can be written as \na sequence of tasks \ufffd \ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd.\nFigure 1: Simple Hierarchical Task Network \nA Simple Task Network (STN) method is a 4-tuple of its name, \ntask, precondition and a task network. The name of the method \nlets us refer unambiguously to substitution instances of the \nmethod, without having to write the preconditions and effects \nexplicitly. The task tells what kind of task can be applied if the \npreconditions are met. The preconditions specify the conditions \nthat the current state needs to satisfy in order for the method to be \napplied. And the network defines the specific subtasks to \naccomplish in order to accomplish the task. \nA method is relevant for a task if the current state satisfies the \npreconditions of a method that implements that task. This task can \nbe then substituted with the instance of the method. The \nsubstitution is basically giving the method network as a solution \nfor the task. \nIf there is a task \u201cGo home\u201d and the distance to home is 3km \n[Figure 2] and there exists a method walk-to and this method has a \nprecondition that the distance is less than 5km, then a substation \nto the task \u201cGo home\u201d can be made with this method instance.  \nFigure 2: HTN Method \nBuy milk\nGo to (shop)\nPurchase \nGo to (home)\nGo-to (from, to)\nWalk (to)\nIf (to \u2013 from) < 5km \n296\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\nIf the distance is larger than 5km another meth\nto be substituted [Figure 3]. \nFigure 3: HTN Method 2 \nAn STN planning domain is a set of operatio\nmethods M. A STN planning problem is a 4-tu\nstate S0, the task network w called initial task\nSTN domain. A plan \ufffd \ufffd \ufffd\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd is a soluti\nproblem if there is a way to decompose w into \u03c0\nand each decomposition is applicable in the ap\nthe world. The algorithm that is capable to \nnetworks into plans is called Total-forward-deco\n[9] or Partial-forward-decomposition (PFD). H\ncases where one does not want to use a forwa\nprocedure. HTN planning is generalization of S\ngives the planning procedure more freedom\nconstruct the task networks.  \nIn order to provide this freedom, a bookke\nis needed to represent constraints that the plann\nnot yet enforced. The bookkeeping is done by\nunenforced constraints explicitly in the task netw\nThe HTN generalizes the definition of a\nSTN. A task network is the pair \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd\ufffd w\ntask nodes and C is a set of constraints. Eac\nspecifies a requirement that must be satisfied by\na solution to a planning problem.  \nThe definition of a method in HTN also\ndefinition used in STN planning. A HTN pla\nname, task, subtasks, and constraints. The s\nconstraints form the task network. The HTN plan\nidentical to STN planning domains except they u\ninstead of STN methods. \nCompared to classical planners the prim\nHTN planners is their sophisticated knowledge r\nreasoning capabilities. They can represent and \nnon-classical planning problems; with a good\nguide them, they can solve classical planning p\nmagnitude more quickly than classical or neoc\nThe primary disadvantage of HTN is the nee\nauthor to write not only a set of planning opera\nof methods. \n3.2.2 HTN Planning in building Game \nFor a HTN planning algorithm to be adap\ntrees we need to define the domain (set of H\noperators) which is the domain of the game. Thi\na knowledge representation of the rules of the\nenvironments and possible strategies of game pla\nIn this domain the game rules as well as kn\ntackle specific task are defined.   The implem\nTree building with HTN is called Tign\nimplementation \nuses \na \nprocedure \nsimila\ndecomposition, but adapted to build up a game \nDrive(to\nIf(t\nGo-to (from, to) \nIf(to \u2013 from) < 5km \nWalk (to) \nhod instance needs \nons O and a set of \nuple of the initial \nk network and the \nion for a planning \n\u03c0 if \u03c0 is executable \nppropriate state of \ndecompose these \nomposition (TFD) \nHowever there are \nard-decomposition \nSTN planning that \nm about how to \neeping mechanism \nning algorithm has \ny representing the \nwork. \na task network in \nwhere \ufffd is a set of \nh constraint in C \ny every plan that is \no generalizes the \nan is a 4-tuple of \nsubtasks and the \nnning domains are \nuse HTN methods \nmary advantage of \nrepresentation and \nsolve a variety of \nd set of HTNs to \nproblems orders of \nclassical planners. \ned of the domain \nators but also a set \nTrees\nted to build game \nHTN methods and \nis is in some sense \ne game, the game \nay.\nnown strategies to \nmentation of Game \nnum2 [9]. This \nar \nto \nforward-\ntree rather than a \nplan. The branches of the game tree rep\nthe methods. Tignum2 applies all met\nstate of the world to produce new\ncontinues recursively until there are n\nhave not already been applied to th\nworld.  \nIn the task network generated by Tignu\nactions will occur is determined by th\nBy listing the actions in the order \nnetwork can be \u201cserialized\u201d into a gam\n4. Case Based Reasoning in\n4.1 Case Based Reasoning\nCase-based reasoning (CBR) is a \nArtificial Intelligence (AI), both as \nproblems and as a basis for standalone \nCase-based reasoning is a paradigm\nsolving and learning that has became \napplied subfield of AI of recent yea\nintuition that problems tend to recur. I\nare often similar to previously en\ntherefore, that past solutions may be of\n[10].  \nCBR is particularly applicable to probl\navailable, even when the domain is n\nfor a deep domain model. Helpdesks,\nsystems have been the most successfu\nto determine a fault or diagnostic \nattributes, or to determine whether or\nrepair is necessary given a set of past s\nFigure 5: Game Tree built fr\nFigure 4: HTN to Game Tr\n)\nto \u2013 from) < 200km \npresent moves generated by \nthods applicable to a given \nw states of the world and \nno applicable methods that \nhe appropriate state of the \num2, the order in which the \ne total-ordering constraints. \nthey will occur, the task \nme tree [Figure 4] [Figure 5]. \nn Game Strategies\nwell established subfield of \na mean for addressing AI \nAI technology.\nm for combining problem-\none of the most successful \nars. CBR is based on the \nIt means that new problems \nncountered problems and, \nf use in the current situation \nlems where earlier cases are \nnot understood well enough \n, diagnosis or classification \nul areas of application, e.g., \nan illness from observed \nr not a certain treatment or \nolved cases [11]. \nrom HTN\nree Algorithm\nInteractive and Adaptable Media\n297\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\nCentral tasks that all CBR methods have to deal with are [12]: \"to \nidentify the current problem situation, find a past case similar to \nthe new one, use that case to suggest a solution to the current \nproblem, evaluate the proposed solution, and update the system by \nlearning from this experience. How this is done, what part of the \nprocess that is focused, what type of problems that drives the \nmethods, etc. varies considerably, however\".  \nWhile the underlying ideas of CBR can be applied \nconsistently \nacross \napplication \ndomains, \nthe \nspecific \nimplementation of the CBR methods \u2013in particular retrieval and \nsimilarity functions\u2013 is highly customized to the application at \nhand. \n4.2 CBR and Games \nMany different implementations of CBR exist in games. \nCBR technology is nicely suited for recognizing complex \nsituations much easier and more elegant than traditional parameter \ncomparison or function evaluation. There are especially evident \ncases in real time strategies where different attack and defense of \nglobal strategies are nicely defined by CBR datasets and later used \nin the running games. Also intelligent bots behavior is also \nanother typical example. Depending on the number of enemy bots \nthe layout of the terrain and position of human players the CBR \nsystem finds the closest CBR case and employs that strategy \nagainst the human players which in prior evaluation was proved to \nbe highly efficient. \n5. Game Trees with AI Planning \u2013 Tic-tac-toe \nIn order to show the expressive power of AI Planning in \ndefining strategies for games, and the use of these plans to build \nGame Trees I implemented an algorithm that builds Game Trees \nfor the Tic-Tac-Toe game. \nThe game tree of Tic-Tac-Toe shows 255,168 possible \ngames of which 131,184 are won by X (the first player), 77904 \nare won by O and the rest 46,080 are draw [13]. All these games \ncan be derived from building a complete Game Tree.  \nEven though it is possible to build a complete game tree of \nTic-tac-toe it is definitely not an optimal solution. Many of the \nmoves in this tree would be symmetrical and also there are a many \nmoves that would be illogical or at least a bad strategy to even \nconsider.  \nSo what strategy should X (the first player) choose in order \nto win the game? \nThere are few positions that lead to certain victory. These \npositions involve simultaneous attack on two positions so the \nother player could not defend, basically the only trick in Tic-Tac-\nToe. \nFigure 6: Tic-tac-toe winning strategy positions \nPosition 1 leads to victory if the two of the three fields: top \nmiddle, bottom left corner and bottom right corner are free \n[Figure 6]. \nPosition 2 lead to victory if two of the three fields: top right \ncorner, bottom right corner and bottom middle are free [Figure ]. \nAnd in the third position if the two of center, middle top and \nmiddle left are available the position is a certain victory. \nThere are many different arrangements of the player\u2019s tokens \nthat give equivalent positions as these three positions. By using \nplanning we do not need to consider all possible layouts but just \nconsider these three similar to what a human would consider. \n The game starts from an empty table. \nThe two relevant strategies that would lead to these positions \nare to take one corner or to take the center [Figure 7]. \nFigure 7: Tic-tac-toe Two starting moves \nThe center position as we can see in the simulation results \nlead to a bigger number of victorious endings but it is also a \nstraight forward strategy with obvious defense strategy. \nAt this point we need to consider the moves of the opponent. \nIf we take the left branch the opponent moves can be a center, a \ncorner or a middle field. We also need to differentiate with a \nmove to a corner adjacent with our like top left or bottom right or \nacross the center to bottom right [Figure 8]. \nFigure 8: Tic-tac-toe opponent response to corner move \nIn cases one and two, we have a clear path to executing \nstrategy 3 so we need to capture the diagonally opposite field. \nAnd as for the third case the best way to go is to capture the center \nand go for strategy 1 or 2 depending of the opponent\u2019s next move.  \nFigure 9: Tic-tac-toe move 2 after corner opening \nThe first move leads to certain victory, O will have to go to \nthe center and X will achieve strategy 3 [Figure 9]. The second \nmove is a possible way to strategy 3 if O makes a mistake in the \nnext loop, so X goes to the opposite corner. For the third case \nsince O is playing a valid strategy the only move that leaves a \npossible mistake from O would be to take the center and wait for \nO to go to the middle and then achieve strategy 1 or 3 which will \nbe a symmetric situation to the one that we will find if we \nbranched with the center. \nFigure 10: Tic-tac-toe opponent response to center move \nIf we go back to the second branch [Figure 10], a possible \nway for the second player to engage is corner or middle. The first \n298\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\nmove is a valid strategy for O and can be mee\ncorner move from X to try a mistake from O in \nthe same as in the third case above from the pre\nanother move would be go to the middle wh\nachieves strategy 1 or 2.  \nFigure 11: Tic-tac-toe Move 2 after cent\nThe fist move will lead to win if O moves \ndraw if it goes for the corners [Figure 11]. In t\nhas to block the lower left corner which leave\nmiddle left or corner left which are strategy 1 and\nTo sum the strategies for the planning, first \ncorner strategy for the beginning. Then for the ce\nthe corners with the particularly the one oppo\nholds. If the center is empty for the second strate\nwe go for the opposite corner. After this point w\nopponent or try to implement strategies 1, 2 or\nvictory.  \nPlan 1: Take center  \nPreconditions: Center empty \nPlan 2: Take corner  \nPreconditions: All corners empty \nPlan 3: Take corner after center \nPreconditions: We have center take corner oppos\nopponent has \nPlan 4: Take diagonal corner \nPreconditions: We have a corner, the opponent ha\n the corner opposite to the one we have is free. \nPlan 5: Block \nPrecondition: The opponent has tree tokens in a r\nagonal \nPlan 6: Win \nPreconditions: We have two tokens in a row, colu\nnd the third place is free \nPlan 7: Tie \nPreconditions: If all places are taken, it\u2019s a tie. \n5.1 Hierarchical Task Network \nTop level task is Play [Figure 12]. This is a \ncan be derived into: Win, Block, Tie or Sear\nSearch for plan is derived to both Plan 1 and Pla\nPlan 4, which later leads to a call for the oppon\nrecursive call to Play. \nFigure 12: Tic-tac-toe HT\net with a opposite \nthe future exactly \nevious branch, and \nhere X eventually \nter opening\nto the middle or a \nthe second case O \nes X to go for the \nd 2.\nwe have center or \nenter we try to get \nosite to the one O \negy we go for it or \nwe either block the \nr 3 which lead to \nsite to the  one the \nas the ce\u2212nter and\nrow, colu\u2212mn or di\nmn or dia\u2212gonal a\na complex task and \nrch for Plan. The \nan 2 or Plan 3 and \nnent\u2019s move and a \nTN\nThis HTN when executed will re\ngame scenarios. By creating nodes from\nthem with branches with the move of t\ntree for the Tic-tac-toe game over whi\nalgorithm. \nThis set up with 7 plans with 3 ta\nfor Tic-tac-toe which considers all pos\nplayer with only 457 games, 281 of w\nand 0 where the second opponent w\nreduction over the 255, 168 possible g\ntree. These reductions can be very use\ncomputing capabilities but also we pr\nthat planning can be very efficient if d\ntrees by applying reasoning very \nreasoning. \nFurther improvements to the gam\nthe opponents moves are also planned\nall the meaningless and symmetrical m\n6. Game AI in Monopoly \n6.1 Overview of the AI Imp\nThe AI agent is responsible for \nplayers in the game. The core principle\na Game Tree with all the sensible move\nmake from the current point of time\nminimax algorithm the agent selects t\nwould bring the computer player mo\nwith the highest probability. Building \nthat would be big enough to consider \nis obstructed by the vastness of poss\nwith all the possible random landings \nnodes of the game tree exponentially\ntackle this problem the AI agents \ndiscussed technologies: Case Based Re\nThe technologies are employed \nFirst the agent searches the CBR datab\nlargest similarity with the current state\nassociated with a playing strategy. Th\nthat the planner needs to build plans f\nconsecutive player moves that bring th\nway only moves that are part of that str\nbeing a small fraction of the overall po\nedges of the game tree at each level dec\nAt each level of the game tree the\nof a single player. After the strateg\nconsidered the response to those strate\nby the opponent(s). The move of the \nprobability distribution of the dice as \nplayer. A more general strategy needs\nopponent\u2019s (human player) moves sin\nthe expertise of the opponent. This ge\nmore plausible moves than the focused\nAfter covering all opponents t\ndeducting a feature move of the com\nCBR selected plan strategy. After \nstrategies and reaching a reasonable s\ninto account the memory limits an\nprobabilities that the move is possible\nthe dice the building of the Game Tre\nalgorithm searches the Game Tree \nfavorable move for the AI player usi\nThe process is repeated each time the A\nesult with plans for possible \nm each position and linking \nthe player we create a game \nich we can run the minimax \narget strategies creates a tree \nssible moves for the second \nwhich X wins 176 are draw \nwins. This is a significant \names with a complete game \neful for devices with limited \nrove a very important point \ndesigning meaningful game \nsimilar to human player \nme tree are also possible if \nd, in other words if we drop \nmoves of the opponent. \nplementation\nthe moves of the artificial \ne of the AI agent is building \nes that all the players would \ne forward. Then using the \nthe move that in the future \nost favorable game position \na Game Tree in this game \nsufficient number of moves \nsible moves in combination \nof the dice. The number of \ny grows at each level. To \nincorporates two already \neasoning and AI Planning. \nin the following manner. \nbase to find the case with the \ne of the board. This case is \nhe strategy consists of goal \nfor, and the plans consist of \nhe player to that goal. This \nrategy are considered, those \nossible moves the number of \ncreases immensely. \ne model considers the moves \ngies of the AI player are \negies needs to be considered \nopponent(s) depends of the \nwell as the strategy of the \ns to be implemented for the \nnce we cannot be aware of \neneral strategy would bring \nd strategy of the AI player.  \nthe agent comes back to \nmputer player by using the \ncreating several loops of \nsize of a Game Tree taking \nnd the rapidly decreasing \ne due to the distribution of \nee stops. Then the minimax \nand decides on the most \ning the minimax algorithm. \nAI player is up. \nInteractive and Adaptable Media\n299\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\nBuying, auctioning and trading game moves are always \naccompanied by return of investment calculations in making the \nplans. These calculations represent adaptation of the more general \nplanning associated with the cases in the CBR database. These \nadaptations are necessary due to the fact that the cases do not \nidentically correspond to the situation on the table. In addition \ncalculating the game position value of each node of the game tree \nis done by heuristic functions that incorporate economic \ncalculations of net present value, cash, and strategic layout and so \non. For example railroads in monopoly are known to be \nstrategically effective because they bring constant income even \nthough the income can be smaller than building on other \nproperties.  \n6.2 Details on the CBR Implementation \nThe implementation of the CBR is by using the JColibri2 \nplatform.  JColibri2 is an object-oriented framework in Java for \nbuilding CBR systems that is an evolution of previous work on \nknowledge intensive CBR [14].  \nFor this implementation we need to look into three particular \nclasses of the JColibri2 platform. The StandardCBRApplication, \nConnector, CBRQuery. For a JColibri2 implementation the \nStandardCBRApplication interface needs to be implemented.  \nThe CBR cycle executed accepts an instance of CBRQuery. \nThis class represents a CBR query to the CBR database. The \ndescription component (instance of CaseComponent) represents \nthe description of the case that will be looked up in the database. \nAll \ncases \nand \ncase \nsolutions \nare \nimplementing \nthe \nCaseComponent interface. \nThe JColibri2 platform connects to the CBR database via a \nConnector class. Each connector implements all the necessary \nmethods for accessing the database, retrieval of cases, storing and \ndeletion of cases. This implementation uses a custom XML \nstructure for holding the CBR cases. Since the game will not \nupdate the CBR database only read it, a XML solution satisfies \nthe needs. The XML file to a certain extent is similar to the XML \nrepresentation of the board. We are interested in finding one \nCBRCase that is the most similar case to the situation in the game \nat the time of the search. This procedure is done in the cycle \nmethod of the CBRApplication. The JColibri2 CBR comparison is \ndone by Nearest Neighbor (NN) search method.  \nJColibri2 offers implementations for NN search algorithms \nof simple attributes. These implementations are called local \nsimilarities. For complex attributes like in our case global \ncustomized similarity mechanisms need to be implemented. \nThe MonopolyDescription class [Figure 13] is basically a \nserialization of the GameState. It holds all the information about \nthe state of the board, the players, their amount of cash etc.  \nFigure 13: Class diagram of the Monopoly Case component \nmodels \nOn the other hand the MonopolySolution class holds the \nthree particular attributes that are needed for the planning, the \nplanning Domain, State and TaskList. \nThe game is implemented by using the Model-View-\nController software development pattern. The controller is \nresponsible for implementing the game rules and handling all of \nthe events in the game like roll of dice, input commands for \ntrading, auctioning and etc from the players. The View layer is \nresponsible for displaying the board and all of the input widgets \non to the game screen, and the models are data structures \nrepresenting the game state [Figure 14]. \nFigure 14: Class diagram of the Monopoly models \n6.2.1 Complex Similarity representation in CBR \nThe similarity measurement part of the Nearest Neighbor \nalgorithm JColibri2 is implemented by implementing the \nLocalSimiralrityFunction \nand \nthe \nGlobalSimiralityFunction \ninterface. A local similarity function is applied to simple attributes \nby the NN algorithm, and a global similarity function is applied to \ncompound attributes. In the case of our implementation the \nattributes of the MonopolyDescription are compound attributes \ndescribing the state of the board, number of players, amount of \ncash for every player and etc. Since MonopolyDescription is a \ncustom CaseComponent a global similarity function needs to be \nimplemented to accurately find the distance between different \nCBR cases. \nThe similarity mechanism is inseparable core element of the \nCBR system. This mechanism represents how the CBR decides \nwhich strategy is best suited for the particular situation by \n300\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\ncalculating the distance or similarity to other cases in the \ndatabase.  \nFor the monopoly implementation we need to consider \nseveral basic strategies. Monopoly is based on investing in \nproperties and receiving revenues from those investments. One of \nthe basic strategies of the game is to build a set of properties that \nwill bring constant income larger than the one of the opponents. \nSo in time the opponents will have to declare bankruptcy. But on \nthe other hand over investment can lead to too stretched resources \nwith low income that will eventually drove the player to \nbankruptcy. To decide on these two we need a clear separation \ninto two groups of cases in the CBR database. The first group of \ncases will represent a situation on the board where the player has \nsignificant income per loop formed of one or more color group \nproperties, maybe railroads, some buildings on them and so on. It \nis important to note that in this case the player is better situated \nthan his opponents so he only needs to survive long enough to win \nthe game. In the other group of cases either the opponent is not \nwell positioned on the board or its opponents are better situated. \nIn this case further investments are necessary to improve the \nsituation so the player can have a chance of winning in the long \nrun.  \nThese metrics can be owning color groups, valuing groups of \nrailroads, evaluating the other opponents as well, and considering \nthe amount of cash. As it is obvious in monopoly the number of \nstreets is not as nearly as important as the combination of streets \nthe player owns. It is also important to note that one CBR case \ndoes not hold only a single strategy in place, but its solution can \nhave multiple different strategic goals. For example one CBR case \nmight simultaneously say buy this land to form a color group but \nalso trade some other unimportant property to increase cash \namount.  \nThe cases do not represent all possible combinations of board \npositions. They are only representation of typical game scenarios. \nThe CBR Case solutions do not give exact instructions in general \nbut rather strategic goals. For example one CBR Solution might \nsay trade the streets that you only have one of each for the ones \nthat you have two of that color already. Then the planner based on \nthe situation on the board needs to decompose this high level task \nto a low level operations. Like offer \"Mediterranean Avenue\" for \n\"Reading Railroad\" and offer $50. The exact amounts and actual \nstreets are left to the planer to evaluate.  \nThe monopoly CBR database is currently in development on \na monopoly clone game called Spaceopoly. The cases are \narchitected based on human player experience and knowledge. \nThere is a plan of making a number of slightly different strategies \nthat differ on the style of playing and then running simulation \ntests that would determine the particular validity of each database \nas well as validity of certain segments of the strategy or even \nparticular cases in the database.  \nThe actual execution of the strategies will not differ from \nstrategy to strategy since the plan execution is more related to the \nstructure and rules of the game than to the actual playing strategy. \n6.3 Details on the Planning Implementation \nFor the purpose of planning this implementation uses a \nmodification of the JSHOP2 planner. The Java Simple \nHierarchical Ordered Planner 2 is a domain independent HTN \nplanning system [15].  \nJSHOP2 uses ordered task decomposition in reducing the \nHTN to list of primitive tasks which form the plans. An ordered \ntask decomposition planner is an HTN planner that plans for tasks \nin the same order that they will be executed. This reduces the \ncomplexity of reasoning by removing a great deal of uncertainty \nabout the world, which makes it easy to incorporate substantial \nexpressive power into the planning algorithm. In addition to the \nusual HTN methods and operators, the planners can make use of \naxioms, can do mixed symbolic/numeric conditions, and can do \nexternal function calls. \n In order for the JSHOP2 planer to generate plans it needs \ntree crucial components: Domain, State and Tasks. The Domain \ndefines all the functionalities that the particular domain offers. \nThese are simple and complex tasks. The complex tasks also \ncalled methods create the hierarchy with the fact that they can be \nevaluated by simple tasks of other complex tasks. This is how a \nhierarchical structure of tasks is formed. The problem reduction is \ndone by reducing the high level complex tasks to simpler until all \nthe tasks are primitive. The list of primitive tasks forms the plan. \nThe State represents the state of the system. It is a simple \ndatabase of facts that represent the state of the system. The State \nis necessary to determine the way the problems or tasks are \nreduced to their primitive level. The reduction is done by \nsatisfying different prerequisites set in the methods; these \nprerequisites are defined in the state. The Tasks are high level \ntasks or methods defined in the Domain. The planner based on the \nState and the goals selects one or more high level tasks that need \nto be reduced to plans [Figure  15]. \nFigure 15: Diagram of a Planner \nThe plans then generate the game moves. The number of \nmoves generated by the plans is just a fraction of the possible \nmoves at that point. This reduces the game tree providing the \nopportunity to generate smaller and deeper game trees and making \nmore efficient decisions in general.  \n7. Conclusion \nEven though the results from the CBR database are not \ncomplete at this time partial strategies are implemented as cases \nand recognized during game play by the CBR system. These \nsmaller local strategies coupled with more global higher level \nstrategies that are particularly important at the beginning of the \ngame would form a complete CBR database and represent a \nknowledge engineered style of playing of the AI player.  \nThe AI Planning approach is a proven method by the tic-tac-\ntoe experiment and is suitable for implementing the strategies \nassociated with the CBR cases. \nThis approach in general benefits from both technologies, \nCBR as well as AI Planning and comprises an elegant solution. \nEven though AI Planning can be enough as a single technology \nfor some simpler problems like tic-tac-toe the complexity of \nMonopoly would mean that the Planner would have to incorporate \nCore Planner \nTasks\nPlan\nState\nInteractive and Adaptable Media\n301\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\nlarge and complex domain and a very big state model. The CBR \napplication helps reduce this complexity by focusing the planning \non smaller domain of the game. Basically the CBR reduces the \noverall goal of the play (wining the game) to smaller more \nconcrete goals suitable to the particular state of the game, thus \nreducing the need for global planning strategies and complex \nplanning domain.  \nFurthermore this symbiosis of technologies gives way for \nmore precise and finely tuned strategies which can be difficult to \ninclude into global plan for the whole game. One simple example \nfor the Monopoly game would be this: Sometimes it\u2019s better to \nstay in jail because rolling double increases the probability of \nlanding on some field (two, four, six, eight, ten or twelve steps \nfrom the jail) that can be of great importance to the rest of the \ngame. These and similar small local strategies can be easily \nrecognized by similar cases in the CBR database.  \nIn other words the system is flexible enough so that new \nstrategies can be incorporated easily missing strategies can be also \nrecognized by the distance metrics as well as wrong assumptions \nin the strategies can be easily recognized. \nOne other important property of the system is that is highly \nconfigurable. The game its self can be diversely different \ndepending on the configuration of the board. Even though the \nplatform is restricted to Monopoly type of games, changing the \nlayout and values of the fields effectively brings completely \ndifferent properties of the game. In addition the CBR database \nrepresents the entire experience of the AI Player. It can be filled \nwith rich set of strategies or even configured with different flavors \nof difficulties of play, this of course coupled with the domain of \nthe planner which can differ from a case to a case as well.  \n8. Future Work \nFurther exploration of this technology would go towards \ncomplete implementation of an AI aware agent for monopoly. \nInitial results from the local cases with more specific strategies \nshow CBR as a capable tool for representing expertise in playing \nthe game. Completing the more general strategies and coupling \nthem with the planning domain will give precise results on the \nbenefits from this architecture. \nThere is also need for exploring the planning of strategies of \nopponents. This task is to some extent different because we \ncannot always expect the opponent to select the best move we \nthink. In the Tic-tac-toe example all possible moves of the \nopponent were taken into consideration, if we used the same \nplanner for the opponent only tie games would result from the \ngame tree. In other words mistakes of the players also need to be \nconsidered.  \nThe CBR Platform brings other functionalities well worth of \nexploring as well. The revision stage of the JColibri2 platform is \nbasically capable of fine tuning strategies or even developing new \nstrategies for the games. A well written underlying AI planning \nmodel with a capable feedback of the game tree evaluation back \nto the CBR revision capability can be an interesting concept in \nautomatic experience acquisition for the AI model. \nThere are also many other fields were combined CBR and \nplanning approach can be incorporated into a problem solution. \nThis combination is analogous in a big extent to a human way of \nreasoning. People in addition to logic of reasoning in situations \nwith lack of information rely to planning strategies and prior \nexperience, exactly the intuition behind CBR \u2013 AI Planning \narchitecture.  \n9. ACKNOWLEDGMENTS \nWe would like to thank Prof. Sofia Tsekeridou for her \ninvolvement in the valuable discussions we had on the topic of \nCBR. \n10. REFERENCES \n[1]\nMinimax. Wikipedia. [Online] [Cited: April 23, 2008.] \nhttp://en.wikipedia.org/wiki/Minimax. \n[2]\nVon Neumann, J: Zur theorie der gesellschaftsspiele Math. \nAnnalen. 100 (1928) 295-320 \n[3]\nAutomated Planning. Wikipedia. [Online] [Cited: April 23, \n2008.] http://en.wikipedia.org/wiki/Automated_planning. \n[4]\nSanchez-Ruiz, Antonio, et al. Game AI for a Turn-based \nStrategy Game with Plan Adaptation and Ontology-based \nretrieval.\n[5]\nK. Erol, J. Hendler, and D. Nau (1994). Semantics for \nhierarchical task-network planning. Technical Report TR-94-\n31, UMIACS. \n[6]\nSmith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning \napproach decrarer play in contract bridge. Computational \nIntelligence. 1996, Vol. 12, 1. \n[7]\nOne Jump Ahead: Challenging Human Supremacy in \nCheckers. J.Schaeffer. s.l. : Springer-Verlag, 1997. \n[8]\nIBM. How Deep Blue works. [Online] 1997. [Cited: April \n23, 2008.] \nhttp://www.research.ibm.com/deepblue/meet/html/d.3.2.html\n[9]\nGhallab, Malik, Nau, Dana and Traverso, Paolo.\nAutomated Planning theory and practice. s.l. : Morgan \nKaufmann Publishers, May 2004. ISBN 1-55860-856-7. \n[10] Case Based Reasoning. Experiences, Lessons and Future. \nLeake, David. s.l. : AAAI Press. MIT Press., 1997. \n[11] Applying case-based reasoning: techniques for enterprise \nsystems. Watson, I. San Francisco, CA, USA : Morgan \nKaufmann Publishers Inc., 1998. \n[12] Plaza, A. Aamodt and E. Case-based reasoning: \nFoundational issues, methodological. AI Communications. \n1994, 7(i). \n[13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.] \nhttp://en.wikipedia.org/wiki/Tic-tac-toe. \n[14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An \narchitecture for knowledge intensive CBR systems. Advances \nin Case-Based Reasoning \u2013 (EWCBR\u201900). New York : \nSpringer-Verlag, Berlin Heidelberg, 2000. \n[15] Ilghami, Okhtay and Nau, Dana S. A General Approach to \nSynthesize Problem-Specific Planners. 2003. \n302\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "pdf_url": "",
    "references": [
      "[1]",
      "Minimax. Wikipedia. [Online] [Cited: April 23, 2008.] ",
      "http://en.wikipedia.org/wiki/Minimax. ",
      "[2]",
      "Von Neumann, J: Zur theorie der gesellschaftsspiele Math. ",
      "Annalen. 100 (1928) 295-320 ",
      "[3]"
    ],
    "publication_date": "05-11-2023"
  },
  {
    "titre": "Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model",
    "resume": "The emergence of novel types of communication, such as email, hasbeen brought on by the development of the internet, which radicallyconcentrated the way in that individuals communicate socially andwith one another. It is now establishing itself as a crucial aspect ofthe communication network which has been adopted by a varietyof commercial enterprises such as retail outlets. So in this researchpaper, we have built a unique spam-detection methodology basedon email-body sentiment analysis. The proposed hybrid model isput into practice and preprocessing the data, extracting the proper-ties, and categorizing data are all steps in the process. To examinethe emotive and sequential aspects of texts, we use word embed-ding and a bi-directional LSTM network. this model frequentlyshortens the training period, then utilizes the Convolution Layer toextract text features at a higher level for the Bi LSTM network. Ourmodel performs better than previous versions, with an accuracyrate of 9798%. In addition, we show that our model beats not justsome well-known machine learning classifiers but also cutting-edgemethods for identifying spam communications, demonstrating itssuperiority on its own. Suggested Ensemble models results areexamined in terms of recall, accuracy, and precision",
    "auteurs": [
      "Khushbu Doulani",
      "GRU",
      "Shivangi Sachan",
      "Khushbu Doulani",
      "Mainak Adhikari",
      "Noida",
      "Decision Trees",
      "Bi-LSTM",
      "GRU",
      "BiLSTM+GRU"
    ],
    "institutions": [
      "Mainak Adhikari Department of CSEIIIT LucknowUP, ",
      "Department of CSEIIIT Lucknow Lucknow, UP, "
    ],
    "mots_cles": [
      "Dataset",
      " KNN",
      " Gaussian Naive Bayes",
      " LSTM",
      " SVM",
      " Bidirectional"
    ],
    "texte_integral": "Semantic Analysis and Classification of Emails through\nInformative Selection of Features and Ensemble AI Model\nShivangi Sachan\u2217\nDepartment of CSE\nIIIT Lucknow\nLucknow, UP, India\nmcs21025@iiitl.ac.in\nKhushbu Doulani\nVardhaman College of Engineering\nHyderabad, India\nkhushidoulani@gmail.com\nMainak Adhikari\nDepartment of CSE\nIIIT Lucknow\nUP, India\nmainak.ism@gmail.com\nABSTRACT\nThe emergence of novel types of communication, such as email, has\nbeen brought on by the development of the internet, which radically\nconcentrated the way in that individuals communicate socially and\nwith one another. It is now establishing itself as a crucial aspect of\nthe communication network which has been adopted by a variety\nof commercial enterprises such as retail outlets. So in this research\npaper, we have built a unique spam-detection methodology based\non email-body sentiment analysis. The proposed hybrid model is\nput into practice and preprocessing the data, extracting the proper-\nties, and categorizing data are all steps in the process. To examine\nthe emotive and sequential aspects of texts, we use word embed-\nding and a bi-directional LSTM network. this model frequently\nshortens the training period, then utilizes the Convolution Layer to\nextract text features at a higher level for the BiLSTM network. Our\nmodel performs better than previous versions, with an accuracy\nrate of 97\u201398%. In addition, we show that our model beats not just\nsome well-known machine learning classifiers but also cutting-edge\nmethods for identifying spam communications, demonstrating its\nsuperiority on its own. Suggested Ensemble model\u2019s results are\nexamined in terms of recall, accuracy, and precision\nCCS CONCEPTS\n\u2022 Computer systems organization \u2192 Embedded systems; Re-\ndundancy; Robotics; \u2022 Networks \u2192 Network reliability.\nKEYWORDS\nDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional\nLSTM, GRU, Word-Embeddings, CNN\nACM Reference Format:\nShivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. Semantic\nAnalysis and Classification of Emails through Informative Selection of\nFeatures and Ensemble AI Model. In 2023 Fifteenth International Conference\non Contemporary Computing (IC3-2023) (IC3 2023), August 03\u201305, 2023, Noida,\nIndia. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.\n3607979\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nIC3 2023, August 03\u201305, 2023, Noida, India\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0022-4/23/08...$15.00\nhttps://doi.org/10.1145/3607947.3607979\n1\nINTRODUCTION\nOver the past few years, a clear surge of both the amount of spam-\nmers as well as spam emails. This is likely due to a fact that the\ninvestment necessary for engaging in the spamming industry is\nrelatively low. As a result of this, we currently have a system that\nidentifies every email as suspicious, which has caused major expen-\nditures in the investment of defense systems [12]. Emails are used\nfor online crimes like fraud, hacking, phishing, E-mail bombing, bul-\nlying, and spamming. [16]. Algorithms that are based on machine\nlearning (ML) are now the most effective and often used approach to\nthe recognition of spam. Phishing, which is defined as a fraudulent\nattempt to acquire private information by masquerading as a trust-\nworthy party in electronic communication, has rapidly advanced\npast use of simple techniques and the tactic of casting a wide net;\ninstead, spear phishing uses a variety of sophisticated techniques\nto target a single high-value individual. Other researchers used NB,\nDecision Trees, and SVM to compare the performance of supervised\nML algorithms for spam identification [6]. Spam emails clog up re-\ncipients\u2019 inboxes with unsolicited communications, which frustrate\nthem and push them into the attacker\u2019s planned traps [7]. As a re-\nsult, spam messages unquestionably pose a risk to both email users\nand the Internet community. In addition, Users may occasionally\nread the entire text of an unsolicited message that is delivered to\nthe target users\u2019 inboxes without realizing that the message is junk\nand then choosing to avoid it. Building a framework for email spam\ndetection is the aim of this project. In this approach, we combine the\nWord-Embedding Network with the CNN layer, Bi-LSTM, and GRU\n(BiLSTM+GRU). CNN layers are used to speed up training time\nbefore the Bi-LSTM network, and more advanced textual character-\nistics are extracted with the use of this network in comparison to\nthe straight LSTM network, in less time. Gated recurrent neural net-\nworks (GRUs) are then added because they train more quickly and\nperform better for language modeling. To evaluate and investigate\nvarious machine learning algorithms for predicting email spam,\nand develop a hybrid classification algorithm to filter email spam\nbefore employing an ensemble classification algorithm to forecast\nit. To put an innovative technique into practice and compare it to\nthe current method in terms of various metrics. Ensemble learn-\ning, a successful machine learning paradigm, combines a group of\nlearners rather than a single learner to forecast unknown target\nattributes. Bagging, boosting, voting, and stacking are the four main\ntypes of ensemble learning techniques. To increase performance,\nan integrated method and the combining of two or three algorithms\nare also suggested. Extraction of text-based features takes a long\ntime. Furthermore, it can be challenging to extract all of the crucial\ninformation from a short text. Over the span associated with this\n181\nIC3 2023, August 03\u201305, 2023, Noida, India\nSachan et al.\nresearch, we utilize Bidirectional Large Short-Term Memories (Bi-\nLSTM) in conjunction with Convolutional Neural Networks (CNN)\nto come up with an innovative method to the detection of spam.\nBagging and boosting approaches were widely preferred in this\nstudy. Contribution and paper organization is as follows: section 1.1\ndescribes literature study, section 1.2 describe motivation for this\nresearch work, section 2 sketches procedure of details implemen-\ntation, Section 3 present experimental setup, dataset description\nand evaluation metrics, and section 4 summarizing outcomes of the\nexperiment.\n1.1\nRelated Work\nEmail is indeed the second most frequently utilized Internet appli-\ncation as well as the third most common method of cyberbullying,\nclaims one study. Cybercriminals exploit it in a number of ways,\nincluding as sending obscene or abusive messages, adding viruses\nto emails, snatching the private information of victims, and ex-\nposing it to a broad audience. Spam letters made up 53.95% of all\nemail traffic in March 2020. We examine three main types of un-\nlawful emails in our study. First are fake emails, which are sent\nto manipulate recipients to submit sensitive information. The sec-\nond as being cyberbullying\u2019s use of harassing emails to threaten\nindividuals. Suspicious emails that describe illegal activities belong\nto the third category. Many researchers have earlier contributed\nmassively to this subject. The researcher claims there is some proof\nthat suspicious emails were sent before to the events of 9/11. [14].\nWhen it comes to data labeling, there are also convinced rule-based\napproaches and technologies ( like VADER) that are used, even\nthough their efficiency of the are together is adversely affected. A\nhidden layer, which itself is essential for vectorization, is the top\nlayer of the model. We use oversampling methods for this minority\nclass because of the absence of data. Sampling techniques can help\nwith multicollinearity, but they have an impact on simulation re-\nsults. Oversampling causes data to be randomly repeated, which\naffects test data because dividing data may result in duplicates. Un-\ndersampling may result in the loss of some strong information. In\norder to advance email research, it is crucial to provide datasets on\ncriminal activity. P. Garg et al. (2021) [5], which revealed that spam\nin an email was detected in 70 percent of business emails, spam was\nestablished as an obstacle for email administrators. Recognizing\nspam and getting rid of it were the primary concerns, as spam can\nbe offensive, may lead to other internet sites being tricked, which\ncan offer harmful data, and can feature those who are not particu-\nlar with their content using NLP. To select the best-trained model,\neach mail transmission protocol requires precise and effective email\nclassification, a machine learning comparison is done. Our study\nhas suggested that innovative deep learning outperforms learning\nalgorithms like SVM and RF. Current studies on the classification\nof emails use a variety of machine learning (ML) techniques, with\na few of them focusing on the study of the sentiments consisted of\nwithin email databases. The lack of datasets is a significant obstacle\nto email classification. There are few publicly accessible E-mail\ndatasets, thus researchers must use these datasets to test their hy-\npotheses or gather data on their own. Authors[15] describe supplied\ntwo-phased outlier detection models to enhance the IIOT network\u2019s\ndependability. Artificial Neural Network, SVM, Gaussian NB, and\nRF (random forest) ensemble techniques were performed to forecast\nclass labels, and the outputs were input into a classifying unit to\nincrease accuracy. A method for content-based phishing detection\nwas presented by the authors in [2], to classify phishing emails,\nthey employed RF. They categorize spam and phishing emails. They\nenhanced phishing email classifiers with more accurate predictions\nby extracting features. They showed some effective Machine learn-\ning spam filtering techniques. When the PCA method is used, it will\nlower the number of features in the dataset. The collected features\ngo through the PCA algorithm to reduce the number of features.\nThe PCA method is used to make a straightforward representation\nof the information which illustrates the amount of variability there\nis in the data. The authors of [20] presented the Fuzzy C-means\nmethod for classifying spam email. To stop spam, they implemented\na membership threshold value. A methodology to identify unla-\nbeled data was put forth by the authors of [1] and applied motive\nanalysis to the Enron data collection. They divided the data into\ncategories that were favorable, negative, and neutral. They grouped\nthe data using k-means clustering, an unsupervised ML technique\nand then classified it using the supervised ML techniques SVM and\nNB. Hina, Maryam, and colleagues (2021) implemented Sefaced:\nDeep learning-based semantic analysis and categorization of e-mail\ndata using a forensic technique. For multiclass email classification,\nSeFACED employs a Gated Recurrent Neural Network (GRU) based\non Long Short-Term Memory (LSTM). Different random weight ini-\ntializations affect LSTMs [9]. Zhang, Yan, et al.(2019) Experiments\non three-way game-theoretic rough set (GTRS) email spam filter-\ning show that it is feasible to significantly boost coverage without\ndecreasing accuracy [23]. According to Xia et al. [22], SMS spam\nhas been identified using machine learning model such as naive\nbayes , vector-space modeling, support vector machines (SVM),\nlong selective memory machines (LSTM), and convolutional neural\nnetworks including every instance of a method for categorizing\ndata. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic\ngradient descent (sgd) algorithms for e-mail filtering with R and\norange software spam [3]. Orange software was used to create the\nclassifications, which included Adaboost and SGD. The majority of\nresearchers focused on text-based email spam classification meth-\nods because image-based spam can be filtered in the early stages\nof pre-processing. There are widely used word bag (BoW) model,\nwhich believes that documents are merely unordered collections\nof words, is the foundation for these techniques. Kumaresan [11]\nexplains SVM with a cuckoo search algorithm was used to extract\ntextual features for spam detection. Renuka and Visalakshi made\nuse of svm [17] spam email identification, followed by selecting\nfeatures using Latent Semantic Indexing (LSI). Here we have used\nlabeled dataset to train the hybrid classifier. We used TF-IDF for\nfeature extraction [20] and Textual features for spam detection\nwere extracted using SVM and a cuckoo search algorithm. [4] for\nfiltering out the spam email. Combining the integrated strategy to\nthe pure SVM and NB methods, overall accuracy is really improved.\nMoreover, accurate detection for spam email has been proposed\nusing the Negative Selection Algorithm (NSA) and Particle Swarm\nOptimization\u2019s (PSO) algorithm. PSO is used in this instance to\nimprove the effectiveness of the classifier.\n182\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\nIC3 2023, August 03\u201305, 2023, Noida, India\n1.2\nMotivation and Novelty\nEmail is most common form of communication between people\nin this digital age. Many users have been victims of spam emails,\nand their personal information has been compromised. The email\nClassification technique is employed to identify and filter junk\nmail, junk, and virus-infected emails prior to reach a user\u2019s inbox.\nExisting email classification methods result in irrelevant emails\nand/or the loss of valuable information. Keeping these constraints\nin mind, the following contributions are made in this paper:\n\u2022 Text-based feature extraction is a lengthy process. Further-\nmore, extracting every important feature from text is difficult.\nIn this paper, we show how to employ GRU with Convo-\nlutional Neural Networks and Bidirectional-LSTM to find\nspam.\n\u2022 Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-\nral Networks to examine the relationships, sentimental con-\ntent, and sequential way of email contents.\n\u2022 Applied CNN before the Bi-LSTM network, training time can\nbe sped up. This network can also extract more advanced\ntextual features faster than the Bi-LSTM network alone when\ncombined with the GRU network.\n\u2022 We use Enorn Corpora datasets and compute precision, re-\ncall, and f-score to assess how well the suggested technique\nperforms. Our model outperforms several well-known ma-\nchine learning techniques as well as more contemporary\nmethods for spam message detection.\n2\nPROPOSED SYSTEM ARCHITECTURE AND\nMODEL\nE-mail is a valuable tool for communicating with other users. Email\nallows the sender to efficiently forward millions of advertisements\nat no cost. Unfortunately, this scheme is now being used in a variety\nof organizations. As a result, a massive amount of redundant emails\nis known as spam or junk mail, many people are confused about the\nemails in their E- Mailboxes. Each learning sequence is given for-\nward as well as backward to two different LSTM networks that are\nattached to the same outputs layer in order for bidirectional Lstms\nto function. This indicates that the Bi-LSTM has detailed sequential\ninformation about all points before and following each point in a\nspecific sequence. In other words, we concatenate the outputs from\nboth the forward and the backward LSTM at each time step rather\nthan just encoding the sequence in the forward direction. Each\nword\u2019s encoded form now comprehends the words that come before\nand after it. This is a problem for the Internet community. The di-\nagram depicts various stages that aid in the prediction of email spam:\nBecause real-world data is messy and contains unnecessary infor-\nmation and duplication, data preprocessing is critical in natural\nlanguage processing (NLP). The major preprocessing steps are de-\npicted below.\n2.1\nNLP Tokenization\nTokenization of documents into words follows predefined rules.\nThe tokenization step is carried out in Python with spacy library.\n2.2\nStop Words Removal\nStop words appear infrequently or frequently in the document, but\nthey are less significant in terms of importance. As a result, these\nare removed to improve data processing.\n2.3\nText Normalization\nA word\u2019s lexicon form or order may differ. Thus, they must all be\nchanged to their root word to be correctly analyzed. Lemmatization\nand stemming are the two methods that can be used for normal-\nization. When a word\u2019s final few characters are removed to create\na shorter form, even if that form has no meaning, the procedure\nis known as stemming. lemmatization [21] is a mixture of corpus-\nbased an rule-based methods, and it retains the context of a term\nwhile changing it back to its root.\n2.4\nFeature Extraction\nfeature extraction which transforms the initial text into its features\nso that it may be used for modeling after being cleaned up and\nnormalized. Before predicting them, we use a specific way to give\nweights to specific terms in our document. While it is simple for a\ncomputer to process numbers, we choose to represent individual\nwords numerically. In such cases, we choose word embeddings. IDF\nis the count of documents containing the term divided by the total\nnumber of documents, and occurrence is the amount of instances a\nword appears in a document. We derive characteristics based on\nequations. 1,2,3,4,5, and 6. We use equations to derive properties.\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51\ud835\udc53 = \ud835\udc61\ud835\udc53 \u2217\n\ufffd 1\n\ud835\udc51\ud835\udc53\n\ufffd\n(1)\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51\ud835\udc53 = \ud835\udc61\ud835\udc53 \u2217 Inverse(\ud835\udc51\ud835\udc53 )\n(2)\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51\ud835\udc53 (\ud835\udc61,\ud835\udc51, \ud835\udc37) = \ud835\udc47 \ud835\udc53 (\ud835\udc61,\ud835\udc51).\ud835\udc3c\ud835\udc51\ud835\udc53 (\ud835\udc61, \ud835\udc37)\n(3)\n\ud835\udc47\ud835\udc3c\ud835\udc51\ud835\udc53 (\ud835\udc61,\ud835\udc51) = log\n\ud835\udc41\n|\ud835\udc51\ud835\udf16\ud835\udc37\ud835\udc61\ud835\udf16\ud835\udc37|\n(4)\nA word2vec neural network-based approach is the method that is\nutilized for this goal as the tool. The following equation, referred\nto as 5, shows how word2vec handles word context through the\nuse of probability-accurate measurements. Here letter D stands for\nthe paired-wise display of a set of words, while the letters w and c0\nor c1 represent paired word context that originated from a larger\ncollection of set D.\n\ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64,\ud835\udc5011:\ud835\udc58) =\n1\n1 + \ud835\udc52\u2212(\ud835\udc64\u00b7\ud835\udc5011+\ud835\udc64\u00b7\ud835\udc5012+...+\ud835\udc64\u00b7\ud835\udc501\ud835\udc58)\n(5)\n\ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64,\ud835\udc501:\ud835\udc58) =\n1\n1 + \ud835\udc52\u2212(\ud835\udc64\u00b7\ud835\udc500)\n(6)\n183\nIC3 2023, August 03\u201305, 2023, Noida, India\nSachan et al.\n2.5\nWord-Embeddings\nWord-Embedding helps to improve on the typical \"bag-of-words\"\nworldview, which requires a massive sparse feature vector to score\nevery word individually to represent this same entire vocabulary.\nThis perception is sparse because the vocabulary is large, and each\nword or document is defined by a massive vector. Using a word\nmap-based dictionary, word embedding needs to be converted terms\n(words) into real value feature vectors. There are two basic issues\nwith standard feature engineering techniques for deep learning.\nData is represented using sparse vectors, and the second is that\nsome of the meanings of words are not taken into consideration.\nSimilar phrases will have values in embedding vectors that are\nalmost real-valued. The Input length in our proposed study is set\nto 700 for our suggested model. If the texts seemed to be integer\nencoded with value systems between 10 and 20, the vocabulary\ndistance would be 11. Our data is encoded as integers, and the input\nand output dimensions are both set to 50,000. The embedding layer\noutcome will be used in successive layers and for BiLSTM and GRU\nlayers.\n2.6\nMachine Learning Model\nWithin the scope of the research, we are using the subsequent ma-\nchine learning techniques, to examine and compare the overall\nefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-\nchine, Gaussian NB, Logistic Regression, K - nearest neighbors, and\nRandom Forest (RF).\n2.7\nConvolution Network\nThe popular RNN model generally performs well but takes too\nlong to train the model incorporating the textual sequential data.\nWhen a layer is added after the RNN layer, the model\u2019s learning\nduration is considerably decreased. Higher-level feature extraction\nis another benefit. [19] additionally possible using the convolutional\nlayer. In essence, the convolution layer looks for combinations of\nthe various words or paragraphs in the document that involve the\nfilters. We use features with 128 dimensions and a size 10 for each.\nFor this task, the Relu activation function is utilized. After that, the\none-dimensional largest pooling layers with a pooling size of 4 are\nput on the data in order to obtain higher-level features.\n2.8\nBiLSTM Network with GRU\nRecurrent Neural Network (RNN) technique of text sentiment anal-\nysis is particularly well-liked and frequently applied. Recurrent\nneural networks (RNN) surpass conventional neural networks. be-\ncause it can remember the information from earlier time steps\nthanks to its memory. A state vector is combined with an RNN\u2019s\ndata to create a new state vector. The resulting state vector uses the\npresent to recollect past knowledge. The RNN is straightforward\nand is based on the following equations:\n\u210e\ud835\udc61 = tanh (\ud835\udc4a\u210e\u210e\u210e\ud835\udc61\u22121 +\ud835\udc4a\ud835\udf0b\u210e\ud835\udc65\ud835\udc61)\n(7)\n\ud835\udc66\ud835\udc61 = \ud835\udc4a\u210e\ud835\udc66\u210e\ud835\udc61\n(8)\nThe vanilla RNN[18]is not very good at remembering previous\nsequences. In addition to that, RNN struggles with diminishing\ngradient descent. A kind of RNN is a long short-term recall network\n(LSTM), solves a vanishing gradient descent problem and learns\nlong-term dependencies[10]. LSTM was actually created to address\nthe problem of long-term reliance. LSTM has the unique ability to\nrecall. The cell state is the LSTM model\u2019s central concept. With\nonly a small amount of linear interaction, the cell state follows the\nsequence essentially unmodified from beginning to end. gate of\nan LSTM is also significant. Under the command of these gates,\ninformation is safely inserted to or eliminated from the cell stated.\nThe following equations are used by the LSTM model to update\neach cell:\n\ud835\udc53\ud835\udc61 = \ud835\udf0e\n\ufffd\n\ud835\udc4a\ud835\udc53 \u00b7 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc53\n\ufffd\n(9)\nIn this case, Xt denotes input, and ht is the hidden state at the t\ntime step. The following is the revised cell state Ct:\n\ud835\udc56t = \ud835\udf0e (\ud835\udc4a\ud835\udc56 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc56)\n(10)\n\ud835\udc36\ud835\udc47 = tanh (\ud835\udc4a\ud835\udc50 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc50\ud835\udc61)\n(11)\n\ud835\udc36\ud835\udc61 = \ud835\udc53\ud835\udc61 \u2217 \ud835\udc36\ud835\udc61\u22121 + \ud835\udc56\ud835\udc61 \u2217 \ud835\udc36\ud835\udc47\n(12)\nHere, we may compute the output and hidden state at t time steps\nusing the point-wise multiplication operator *.\n\ud835\udc5c\ud835\udc61 = \ud835\udf0e (\ud835\udc4a\ud835\udc5c \u00b7 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc5c)\n(13)\n\u210e\ud835\udc61 = \ud835\udc5c\ud835\udc61 \u2217 tanh (\ud835\udc36\ud835\udc61)\n(14)\nDue to the reality it only considers all prior contexts from the\npresent one, LSTM does have a few drawbacks. As a result of this,\nit may accept data from preceding time steps through LSTM as well\nas RNN. Therefore, in order to avoid this issue, further improve-\nments are carried out with the help of a bidirectional recurrent\nneural network(Bi-RNN). BiRNN [13] can handle two pieces of in-\nformation from both the front and the back. Bi-LSTM is created\nby combining the Bi-RNN and LSTM. As a result, operating LSTM\nhas advantages such as cell state storage so that BiRNN have way\nto acknowledge from the context before and after. As a conse-\nquence of this, it provides the Bi-LSTM with the advantages of an\nLSTM with feedback for the next layer. Remembering long-term\ndependencies is a significant new benefit of Bi-LSTM. The output,\nwhich is a feature vector, will be based on the call state. Finally,\nwe forecast the probability of email content as Normal, Fraudu-\nlent, Harassment, and Suspicious Emails using as an input to the\nsoftmax activation function, which is a weighted sum of the dense\nlayer\u2019s outputs. To regulate the information flow, GRU employs\nthe point-wise multiplying function and logistic sigmoid activation.\nThe GRU has hidden states of storage memory and does not have\ndistinct memory cells or units for state control. The W, U, and b\nvectors, which stand for weights, gates, and biases, respectively, are\ncrucial variables that must be calculated during the creation of the\nGRU model. For training reasons, the pre-trained word embedding\nknown as the Glove vector is used. They made it clear that GRU\nis the superior model when there is a large amount of training\ndata for textual groups and word embedding is available. BiLSTM,\nCNN, and GRU is required so as to compensate for the deletion\nof the document\u2019s long-term and short-term connections. In our\ncase, the embedding dimension, maximum sequence length, and\nlexicon size were used to start the LSTM embedding layer in three\nseparate LSTM models. The input vector was modified to make it\nappropriate for such a Conv1D layer, prior situations\u2019 sequences are\nreturned by LSTM layer. The \"return sequences\" of the LSTM layer\nmust be set to False when the subsequent state is free of the gated\n184\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\nIC3 2023, August 03\u201305, 2023, Noida, India\narchitecture. Quantity of learning parameters must be taken into\nconsideration. A 350-unit LSTM layer was set - up, and different\nLSTM unit combinations were tested. More importantly, because\nit has more parts, the model made with BiLSTM will take longer\nto train. Bidirectional LSTM is the name of a particular kind of\nrecurrent neural network that is primarily used for the processing\nof natural languages. (BiLSTM). It is able to use data from both\nsides, and, in contrast to regular LSTM, it enables input flow in\nboth directions. It is an effective instrument for demonstrating the\nlogical relationships between words and phrases, and this involves\nboth the forward and backward directions of the sequence. In con-\nclusion, BiLSTM works by adding one extra layer of LSTM, causing\nthe information flow to travel in the other direction. It only denotes\nthat the input sequence runs in reverse at the next LSTM layer. Mul-\ntiple operations, including averaging, summation, multiplication,\nand concatenation, are then applied to the results of the two LSTM\nlayers. The gated design of Bi-LSTM and GRU networks solves\nthe disappearing gradient and exploding problems. A good way to\nhandle more long sequences is to use Bi-LSMT and GRU together.\nGRU works well with datasets that don\u2019t have text. In two to three\nrounds, the complicated CNN+BiLSTM+GRU model learns the long\nsequence of email text well. We have used word embedding, cnn,\nbidirectional lstm and gru networks as our three building blocks\nto separate email messages based on their sentiment and text\u2019s\nsequential features. Also, we succinctly demonstrate below why\nthese blocks help identify email spam:\n\u2022 First, We have used the Sequence - to - sequence Lstm as the\ncurrent block in the networks since it can retrieve both the\nprevious and next sequences from the current. More so than\na straightforward LSTM network, it can also recognize and\nextract text sentiment and sequential properties.\n\u2022 Second, we extract the more complex and advanced charac-\nteristics for Bi-LSTM network using Convolutional Network\nblock, which is the network\u2019s second block after the Bi-LSTM\nblock. Bi-LSTM takes a long time to extract text-based fea-\ntures, hence one of the reasons for using this block is to\nreduce the network\u2019s overall training time.\n3\nEXPERIMENTAL EVALUATION\n3.1\nExperimental Setup\nWe divided the information into training and testing groups of\n80/20. We divided the remaining 20% of the 80 percent training\ndata into test data for the model. Construct, compute, and evaluate\nthe efficacy of the suggested method using the Pythonic packages\nKeras, as TensorFlow and Scikit learn.\n3.2\nDataset Description\nEmail spam detection is the foundation of this research project. The\ndataset includes normal emails from the Enron corpora, deceptive\nemails from phished email corpora, harassment emails chosen from\nhate speech, and the offensive dataset. Only the content of the email\nbody is used for analysis; all header information, including sender,\ntopic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word\nEmbedding are used to extract characteristics from the email mes-\nsage and classify them. This dataset[8] is publicly available. The\npresented model is implemented using Python, and several metrics,\nincluding accuracy, precision, and recall, are used to examine the\noutcomes.\n3.3\nEvaluation Metrics and Results\nClassifier performance is assessed Using metrics such as accuracy,\nprecision, and recall. Four terms make up a confusion matrix that\nis used to calculate these metrics.\n\u2022 True positives (TP) are positive values that have been accu-\nrately assigned the positive label.\n\u2022 The negative values that are accurately identified as negative\nare known as True Negatives (TN).\n\u2022 True Negative values are those that can be accurately identi-\nfied as being negative (TN).\n\u2022 Positive readings that have been mistakenly labeled as nega-\ntive are known as False Negatives (FN).\nAssess the efficacy of the suggested model is listed below:\n3.3.1\nAccuracy. Accuracy reveals how frequently the ML model\nwas overall correct.\nAccuracy =\n\ud835\udc47\ud835\udc43 +\ud835\udc47\ud835\udc41\n\ud835\udc47\ud835\udc43 +\ud835\udc47\ud835\udc41 + \ud835\udc39\ud835\udc43 + \ud835\udc39\ud835\udc41\n(15)\n3.3.2\nPrecision. The accuracy of the model gauges how effectively\nit can predict a specific category.\nPrecision =\n\ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc43\n(16)\n3.3.3\nRecall. Recall tells us how often the model was able to rec-\nognize a specific category.\nRecall =\n\ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41\n(17)\nModel\nAccuracy\nPrecision\nRecall\nGaussian NB\n91.3\n90.1\n91.8\nRandom Forest\n88.41\n90\n88\nKNN\n86.6\n89\n87\nSVM\n92.4\n91\n92\nLSTM\n95.2\n95\n95.7\nProposed\nEnsemble\n(CNN,BiLSTM+GRU)\n97.32\n95.6\n95.3\nTable 1: Differet Model\u2019s Score on Test Data\nAccuracy, Precision, and Recall metrics are computed. In the\ngiven Table 1 where six different classifiers are Gaussian NB, Ran-\ndom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid\nModel (CNN+BiLSTM+GRU) have been used in this work. In the\nCNN, Bi-LSTM, and GRU architectures which enable sequence pre-\ndiction, CNN strands for feature extraction on data input which are\ncombined with LSTM. It requires less time training and a higher\nexpandable model. Any bottlenecks are created by predictions and\nthe increasing number of distinct units of information. This model\nis useful for dealing with issue-related classifications that consist\nof two or more than two classes. So suggested Ensemble model, out\nof these six classifiers, produces more accurate findings.\n185\nIC3 2023, August 03\u201305, 2023, Noida, India\nSachan et al.\nFigure 1: Performance Analysis\n3.4\nComparative Analysis\nA model\u2019s ability to fit new data is measured by the validation\nloss, whereas its ability to fit training data is determined by the\ntraining loss. The two main variables that decide whether in which\nlearning is efficient or not are validation loss and training loss.\nLSTM and Suggested Ensemble hybrid Models have equivalent loss\nand accuracy. In this context, we are contrasting the LSTM with the\nproposed model (CNN, Bilstm, and GRU) in terms of their respective\nvalidation accuracies and losses. The model\u2019s accuracy was at its\nhighest after 14 epochs of operation when it achieved an accuracy\nof roughly 97-98% while minimizing model loss.\nFigure 2: LSTM Model Training and Validation Accuracy\nFigure 3: LSTM Model Training and Validation Loss\nFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Training\nand Validation Accuracy\nFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Training\nand Validation Loss\n186\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\nIC3 2023, August 03\u201305, 2023, Noida, India\nIn this Proposed ensemble hybrid model\u2019s train accuracy is 98.7%\nValidation accuracy is 97.32% and LSTM has train accuracy of 97.41%\nand validation accuracy is 95.2%. So based on figures 3 and 5 indicate\nthe validation loss for LSTM and the proposed ensemble hybrid\nmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show the\nvalidation accuracy to be 95.2% and 97.3%, respectively. LSTM and\nthe proposed hybrid model used ensemble artificial intelligence,\nwith the proposed hybrid model outperforming the LSTM. We\ndecide on dense architecture as the final model for identifying the\ntext messages as spam or nonspam based on loss, accuracy, and the\naforementioned charts. The loss and accuracy over epochs are more\nstable than LSTM, and the Proposed classifier has a straightforward\nstructure.\n4\nCONCLUSION\nThe model is composed of four networks Word-Embeddings, CNN,\nBi-LSTM, and GRU. We may train the model more quickly by using\nthe convolutional layer first, followed by the word-embedding layer,\nand then the BiLSTM network. The Bidirectional LSTM network\nalso has higher-level properties that we can extract. We have used\na bidirectional LSTM(BiLSTM)and GRU network to memorize a\nsentence\u2019s contextual meaning and sequential structure, which im-\nproves the model\u2019s performance accuracy to roughly 97.32 percent.\nREFERENCES\n[1] Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-\nbeled email data. In 2019 International Conference on Computational Intelligence\nand Knowledge Economy (ICCIKE). IEEE, 328\u2013333.\n[2] Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-\nrithm to filter spam using machine learning techniques. Pacific Science Review A:\nNatural Science and Engineering 18, 2 (2016), 145\u2013149.\n[3] Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic\ngradient descent (sgd) algorithms with R and orange software for filtering e-mail\nspam. In 2019 11th Computer Science and Electronic Engineering (CEEC). IEEE,\n41\u201346.\n[4] Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. A\nsupport vector machine based naive Bayes algorithm for spam filtering. In 2016\nIEEE 35th International Performance Computing and Communications Conference\n(IPCCC). IEEE, 1\u20138.\n[5] Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam Filtering\nTechniques based on Natural Language Processing Framework. In 2021 11th Inter-\nnational Conference on Cloud Computing, Data Science & Engineering (Confluence).\nIEEE, 30\u201335.\n[6] Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-\nlyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316\u2013342.\n[7] Radicati Group et al. 2015. Email Statistics Report 2015\u20132019. Radicati Group.\nAccessed August 13 (2015), 2019.\n[8] Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensic\nanalysis and classification of e-mail data using deep learning. IEEE Access 9\n(2021), 98398\u201398411.\n[9] Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali\nKhan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis and\nclassification of e-mail data using deep learning. IEEE Access 9 (2021), 98398\u2013\n98411.\n[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and Yuan\nZhang. 2017. Short-term residential load forecasting based on LSTM recurrent\nneural network. IEEE transactions on smart grid 10, 1 (2017), 841\u2013851.\n[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoo\nsearch and support vector machine. International Journal of Bio-Inspired Compu-\ntation 9, 3 (2017), 142\u2013156.\n[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification of\nspam emails using deep learning. In 2021 1st Babylon International Conference on\nInformation Technology and Science (BICITS). IEEE, 63\u201368.\n[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural\nnetwork language model. In 2012 IEEE Spoken Language Technology Workshop\n(SLT). IEEE, 234\u2013239.\n[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.\n2014. Detection of fraudulent emails by employing advanced feature abundance.\nEgyptian Informatics Journal 15, 3 (2014), 169\u2013174.\n[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,\nand Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT using\nensemble classifier. arXiv preprint arXiv:2102.01515 (2021).\n[16] Justinas Rastenis, Simona Ramanauskait\u02d9e, Justinas Janulevi\u010dius, Antanas \u010cenys,\nAsta Slotkien\u02d9e, and K\u0119stutis Pakrijauskas. 2020. E-mail-based phishing attack\ntaxonomy. Applied Sciences 10, 7 (2020), 2363.\n[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVM\nmodel for email spam classification. (2014).\n[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequence\nmodeling for intelligent typing assistant with Bangla and English keyboard. In\n2018 International Conference on Innovation in Engineering and Technology (ICIET).\nIEEE, 1\u20136.\n[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Ha\u015fim Sak. 2015. Convolu-\ntional, long short-term memory, fully connected deep neural networks. In 2015\nIEEE international conference on acoustics, speech and signal processing (ICASSP).\nIeee, 4580\u20134584.\n[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spam\nmessages and mails using fuzzy C means algorithm. In 2019 4th International\nConference on Internet of Things: Smart Innovation and Usages (IoT-SIU). IEEE,\n1\u20135.\n[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-\nzation and part-of-speech prediction. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP. 486\u2013494.\n[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boosting\nthroughput on rule-based filtering systems. IEEE Access 8 (2020), 82653\u201382661.\n[23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filtering\nwith game-theoretic rough sets. In 2019 International conference on computing,\nnetworking and communications (ICNC). IEEE, 552\u2013556.\nReceived 15 April 2023\n187\n",
    "pdf_url": "",
    "references": [
      "[1] Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-",
      "beled email data. In 2019 International Conference on Computational Intelligence",
      "and Knowledge Economy (ICCIKE). IEEE, 328\u2013333.",
      "[2] Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-",
      "rithm to filter spam using machine learning techniques. Pacific Science Review A:",
      "Natural Science and Engineering 18, 2 (2016), 145\u2013149.",
      "[3] Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic"
    ],
    "publication_date": "06-11-2023"
  },
  {
    "titre": "Towards a Quantum Software Modeling Language",
    "resume": "We set down the principles behind a modeling language for quan-tum software. We present a minimal set of extensions to the well-known Unified Modeling Language (UML) that allows it to effec-tively model quantum software. These extensions are separate andindependent of UML as a whole. As such they can be used to ex-tend any other software modeling language, or as a basis for acompletely new language. We argue that these extensions are bothnecessary and sufficient to model, abstractly, any piece of quantumsoftware. Finally, we provide a small set of examples that showcasethe effectiveness of the extension set.",
    "auteurs": [
      "Carlos A. P\u00e9rez-Delgado\u2217",
      "G. Perez-Gonzalez",
      "Luis Potos\u00ed",
      "Modeling Language",
      "Carlos A. P\u00e9rez-Delgado",
      "G. Perez-Gonzalez"
    ],
    "institutions": [
      "uage User Guide, The (2nd Edition) (Addison-Wesley Object Technology Series).Addison-Wesley Professional.",
      "University of Kent"
    ],
    "mots_cles": [
      "quantum computing",
      " software engineering",
      " UML"
    ],
    "texte_integral": "Towards a Quantum Software Modeling Language\nCarlos A. P\u00e9rez-Delgado\u2217\nUniversity of Kent\nCanterbury, Kent, United Kingdom\nc.perez@kent.ac.uk\nHector G. Perez-Gonzalez\nUniversidad Aut\u00f3noma de San Luis Potos\u00ed\nSan Luis Potos\u00ed, SLP, M\u00e9xico\nhectorgerardo@uaslp.mx\nABSTRACT\nWe set down the principles behind a modeling language for quan-\ntum software. We present a minimal set of extensions to the well-\nknown Unified Modeling Language (UML) that allows it to effec-\ntively model quantum software. These extensions are separate and\nindependent of UML as a whole. As such they can be used to ex-\ntend any other software modeling language, or as a basis for a\ncompletely new language. We argue that these extensions are both\nnecessary and sufficient to model, abstractly, any piece of quantum\nsoftware. Finally, we provide a small set of examples that showcase\nthe effectiveness of the extension set.\nCCS CONCEPTS\n\u2022 General and reference \u2192 General conference proceedings;\nDesign; \u2022 Software and its engineering \u2192 System descrip-\ntion languages; Unified Modeling Language (UML); Software\ndesign engineering; \u2022 Theory of computation \u2192 Quantum\ncomputation theory; Quantum information theory.\nKEYWORDS\nquantum computing, software engineering, UML\nACM Reference Format:\nCarlos A. P\u00e9rez-Delgado and Hector G. Perez-Gonzalez. 2020. Towards a\nQuantum Software Modeling Language. In IEEE/ACM 42nd International\nConference on Software Engineering Workshops (ICSEW\u201920), May 23\u201329, 2020,\nSeoul, Republic of Korea. ACM, New York, NY, USA, 3 pages. https://doi.org/\n10.1145/3387940.3392183\n1\nINTRODUCTION\nQuantum computation rose to prominence after the discovery of\nquantum algorithms[5, 7] that can efficiently perform tasks that\nare intractable classically. These discoveries propelled research and\ninterest in quantum computation. Today, there exists prototype\nquantum hardware with computational capabilities beyond that of\nany classical machine[1]. Further applications of quantum theory\nto computation have also been made in several areas of theory of\ncomputing, such as models of computation[6], data structures[8],\nand cryptography[2].\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICSEW\u201920, May 23\u201329, 2020, Seoul, Republic of Korea\n\u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-7963-2/20/05...$15.00\nhttps://doi.org/10.1145/3387940.3392183\nQuantum computation has, until today, been studied almost\nexclusively \u2018in the small.\u2019 A general understanding of quantum\ncomputation, or, quantum programming \u2018in the large\u2019 is yet to be\ndeveloped. Here we aim to set the foundations of a general frame-\nwork for studying, developing, and conveying quantum programs.\nWe aim to do so by developing a universal modeling language\nfor quantum software. Rather than develop such a language from\nscratch, we have decided to start from the well-known Unified\nModeling Language (UML)[3], and introduce a minimum set of\nextensions that allow it to effectively model quantum software.\nAssuming UML to be a shared common-language upon which\nwe can build, allows us to convey our original extensions much\nmore succinctly. Our extension set can, however, be applied with\nlittle or no modification to any other modeling language.\n2\nQ-UML\nBefore discussing in depth the extensions we are introducing, we\nmake a few fundamental observations on which we base the guiding\nprinciples for our extension set.\nOur first observation is about the nature of quantum computa-\ntion. The central difference between quantum and classical com-\nputation is in how it achieves its goals. Quantum computers have\naccess to quantum algorithms[7], and quantum data-structures[8],\nthat are unavailable to classical computers\u2014hence their perfor-\nmance advantage. Algorithms and data-structures are, however,\nimplementation details. Algorithms are an essential design choice\nwhile programming in the small. However, they are more often\nthan not completely ignored in large-scale software architectural\ndesign. For instance, UML diagrams seldom portray algorithms and\ndata-structures beyond a very high-level design perspective.\nIt would seem then that quantum computation introduces noth-\ning to computation that needs to be captured in a software design\ndiagram. This is not the case, and the reason for this is our second\nobservation. Quantum computation changes the very nature of in-\nformation itself. Quantum information is much richer than classical\ninformation. It is also much more challenging to store, transmit,\nand receive. If a module (class, object, etc.) needs to store, transmit\nor receive quantum information, then this is an important design\nconsideration\u2014which needs to be included in any effective software\ndesign.\nA third observation here is that the classical vs. quantum nature\nof the information used by a module is an important consideration\nboth when discussing its internal implementation and its interface.\nFurthermore, these two are separate and independent considera-\ntions.\nA classical module, implementing some classical behavior, would\nhave no need, or capability, to communicate quantum data. A quan-\ntum module may or may not have to; i.e. a module\u2019s quantum\nbehavior may be completely part of its internal implementation\n442\n2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops (ICSEW)\nand not appear as part of its interface. For instance, take a module\nimplementing Shor\u2019s algorithm. Shor\u2019s algorithm uses quantum\neffects to efficiently factor a large integer into its prime factors.\nThe implementation of this module must necessarily be quantum.\nBoth the input (the large integer) and the output (the prime factors),\nconsist of classical information. And hence, the interface of such a\nmodule can be strictly classical.\nMore generally, we can conceive of quantum software modules\nthat have all classical inputs and outputs (like the above example),\nall quantum inputs and outputs, or a mix of both. A quantum soft-\nware design must address, for each individual interface element,\nwhether it is classical input/output, or if it is quantum. In short,\nwhether a module communicates classically or via quantum infor-\nmation, and whether its internal implementation requires quantum\nhardware are important considerations that need to be captured in\na design document.\nThe importance of such labelling should be clear. Quantum data\ncan only be stored and transmitted with special hardware designed\nto do so. More importantly, from an abstract, device-independent,\nstrictly software perspective: quantum and classical information\nare not interchangeable. Classical information is clone-able and\nadmits fanout operations, while quantum information (in general)\ndoes not. On the other hand, quantum information has a much\nlarger state-space.\nFinally, it is true that quantum information is strictly a super-set\nof classical information\u2014and hence a quantum module can commu-\nnicate any classical information it desires using a quantum interface\nelement. We argue, however, that using a quantum interface ele-\nment and messaging when classical would suffice is bad quantum\nsoftware design, for the reasons stated above.\nIn summary, the guiding principles behind any quantum software\nmodeling language must include the following:\n(1) (Quantum Classes): Whenever a software module makes\nuse of quantum information, either as part of its internal\nstate/implementation, or as part of its interface, this must be\nclearly established in a design document.\n(2) (Quantum Elements): Each module interface element (e.g.\npublic functions/methods, public variables) and internal state\nvariables can be either classical or quantum, and must be\nlabelled accordingly.\n(a) (Quantum Variables): Each variable should be labelled\nas classical or quantum. If the model represents data types,\nthe variables should also specify the classical (e.g. integer,\nstring) or quantum (e.g. qubit, qubit array, quantum graph\nstate) data type,\n(b) (Quantum Operations): For each operation, both the in-\nput and output should be clearly labelled as either classical\nor quantum. Whether the operation internally operates\nquantumly should also be labelled.\n(3) (Quantum Supremacy): A module that has at least one\nquantum element is to be considered a quantum software\nmodule, otherwise it is a classical module. Quantum and\nclassical modules should be clearly labelled as such.\n(4) (Quantum Aggregation): Any module that is composed of\none or more quantum modules will itself be considered a\nquantum module, and must be labelled as such.\n(5) (Quantum Communication): Quantum and classical mod-\nules can communicate with each other as long as their inter-\nfaces are compatible, i.e. the quantum module has classical\ninputs and/or outputs that can interface with the classical\nmodule.\nWe will argue in Sec. 2.3 how these extensions are not only nec-\nessary, but also sufficient in order to design and represent quantum\nsoftware. First, in the following two sections we put these principles\ninto practice as a set of concrete extensions to UML.\n2.1\nClass Diagram Extensions\nUML is a very graphical language, meant to convey a lot of meaning\nin a very small amount of space. As such, it makes sense to use a\ngraphical way to represent quantum software elements. We chose to\ndo this by use of bold text to denote quantum elements, and double\nlines to denote a quantum relationship or quantum communication.\nFigure 1: Q-UML class diagram of Shor\u2019s Algorithm. Quan-\ntum classes and interface elements are presented in bold\ntext, and quantum relationships use double-lines.\nFor attributes, the name will be bold if it is represented using\nquantum information. For methods, we use the following conven-\ntion. If any of the inputs are quantum, these are bold. If the output\nor datatype of the method is quantum, then the datatype should also\nbe bold. For backwards compatibility with regular UML, whenever\nthe input or output datatypes of a method are omitted, these will be\nassumed to be classical in nature. If a class/object has any quantum\nattributes or methods then it itself is considered quantum, and its\nname shall also be bold.\nRelationships between classes will use double-lines whenever the\nrelationship is quantum in nature. For inheritance, if the superclass\nis quantum then the subclass, and the inheritance relationship, will\nalso be quantum. (the converse is not necessarily true however).\nIn the case of aggregation and composition, if a class/object being\naggregated/composed is quantum, then the class/object to which\nit is aggregated/composed into, as well as that relationship will\n443\nalso be quantum. Association relationships do not have any special\nrules, beyond the need of a quantum class/object to have a classical\ninterface if it is to associate with classical classes/objects.\nFig. 1 showcases a Q-UML diagram that exemplifies the above\nrules.\n2.2\nSequence Diagram Extensions\nSequence diagrams in UML allow us to portray the dynamic rela-\ntionship between modules in a software program. As we did before\nfor static relationships, we extend the existing language in order to\nallow us to differentiate between classical and quantum messages.\nAs previously discussed, this is essential information. Quantum\ninformation behaves differently from classical information; it can\nstore/portray different data; it admits different operations; and, it\nrequires different hardware to store, send, and receive.\nFigure 2: Q-UML sequence diagram of Shor\u2019s Algorithm.\nQuantum classes are presented in bold text, and quantum\nmessages use double-lines.\nLike before, we make use of bold text to markup quantum mod-\nules, and double lines to portray quantum messages. Fig. 2 shows a\nQ-UML sequence diagram. Note how even though the relationship\nbetween Shorfactor and ShorOrder is quantum, the messaging\nbetween them is not. This illustrates an important point. A module\nis marked as quantum if it uses quantum resources in any form,\neither directly as part of its internal implementation or as part of\nan aggregated module. If a sub-module (in UML a composed class\nor object) is quantum, then the encompassing module must also be\nmarked as quantum. In a static (e.g. class) diagram, the quantum\ncomposition relationships inform us\u2014especially in the case of a\nseemingly classical module that does not in itself use quantum\nresources\u2014which composed modules are using quantum resources.\nAlso, note the communication between the objects ShorOrder\nand QFT_n. The module QFT_n operates on a quantum state.\nHence, both \u2018set\u2019 messages are quantum. Likewise, the return mes-\nsages \u03c1 and \u03c1\u2032 are quantum states. However, the request to perform\na quantum Fourier transform (QFT) or a QFT inverse operation\ncan (and therefore should) be communicated classically. This dia-\ngram showcases the level of granularity available to us using these\ndiagrams with the proposed extensions.\n2.3\nDiscussion\nWe have proposed a minimal series of extensions to existing soft-\nware modeling languages. We exemplify our additions in UML,\nbut these extensions are easily applicable to any other modeling\nlanguage, or be used as the basis for a new modeling language.\nWe\u2019ve argued the necessity of each of the extensions in previous\nsections. We can argue as well, that these extensions are not only\nnecessary, but also sufficient to fully model quantum software.\nTo make this argument, we appeal to the fact that all quantum\ncomputation is simulable using classical computation albeit with\nan efficiency loss. Other than their use of quantum information and\nalgorithms, quantum computers are indistinct from classical ones.\nHence, from a high-level design perspective, the only information\nelement that needs to be considered when developing quantum\nsoftware is when quantum (rather than classical) information is\nbeing used.\nThe one remaining information element we have not discussed\nis algorithm efficiency. If quantum computation is to be used, it\nwill most likely be due to the efficient algorithms at its disposal.\nThat said, algorithm efficiency is not a solely quantum consider-\nation. UML itself does not inherently have language elements for\nalgorithm efficiency (beyond user-defined notes). It does, however,\nhave several extensions used and proposed for this purpose(see\ne.g.[4]). Other modeling languages may also have definite algorithm\nefficiency elements. We argue that it is best to use existing language\nelements when they are available.\nACKNOWLEDGMENTS\nCP-D would like to acknowledge funding through the EPSRC Quan-\ntum Communications Hub (EP/T001011/1). The authors would also\nlike to thank Joanna I. Ziembicka for useful comments during the\npreparation on this manuscript.\nREFERENCES\n[1] Frank Arute et. al. 2019. Quantum supremacy using a programmable supercon-\nducting processor. Nature 574, 7779 (2019), 505\u2013510.\nhttps://doi.org/10.1038/\ns41586-019-1666-5\n[2] Charles H Bennett and Gilles Brassard. 2014. Quantum cryptography: public key\ndistribution and coin tossing. Theor. Comput. Sci. 560, 12 (2014), 7\u201311.\n[3] Grady Booch, James Rumbaugh, and Ivar Jacobson. 2005. Unified Modeling Lan-\nguage User Guide, The (2nd Edition) (Addison-Wesley Object Technology Series).\nAddison-Wesley Professional.\n[4] C. Canevet, S. Gilmore, J. Hillston, M. Prowse, and P. Stevens. 2003. Performance\nmodelling with the Unified Modelling Language and stochastic process algebras.\nIEE Proceedings - Computers and Digital Techniques 150, 2 (March 2003), 107\u2013120.\nhttps://doi.org/10.1049/ip-cdt:20030084\n[5] Lov K. Grover. 1996.\nA Fast Quantum Mechanical Algorithm for Database\nSearch. In Proceedings of the Twenty-eighth Annual ACM Symposium on The-\nory of Computing (STOC \u201996). ACM, New York, NY, USA, 212\u2013219.\nhttps:\n//doi.org/10.1145/237814.237866\n[6] Carlos A. P\u00e9rez-Delgado and Donny Cheung. 2007. Local unitary quantum cellular\nautomata. Phys. Rev. A 76 (Sep 2007), 032320. Issue 3. https://doi.org/10.1103/\nPhysRevA.76.032320\n[7] Peter W Shor. 1994. Algorithms for quantum computation: Discrete logarithms\nand factoring. In Proceedings 35th annual symposium on foundations of computer\nscience. Ieee, 124\u2013134.\n[8] Liming Zhao, Carlos A. P\u00e9rez-Delgado, and Joseph F. Fitzsimons. 2016. Fast graph\noperations in quantum computation. Phys. Rev. A 93 (Mar 2016), 032314. Issue 3.\nhttps://doi.org/10.1103/PhysRevA.93.032314\n444\n",
    "pdf_url": "",
    "references": [
      "[1] Frank Arute et. al. 2019. Quantum supremacy using a programmable supercon-",
      "ducting processor. Nature 574, 7779 (2019), 505\u2013510.",
      "https://doi.org/10.1038/",
      "s41586-019-1666-5",
      "[2] Charles H Bennett and Gilles Brassard. 2014. Quantum cryptography: public key",
      "distribution and coin tossing. Theor. Comput. Sci. 560, 12 (2014), 7\u201311.",
      "[3] Grady Booch, James Rumbaugh, and Ivar Jacobson. 2005. Unified Modeling Lan-"
    ],
    "publication_date": "07-11-2023"
  },
  {
    "titre": "A Prototype Implementation of an Orthographic Software Modeling Environment",
    "resume": "Orthographic Software Modeling (OSM) is a view-centricsoftware engineering approach that aims to leverage the or-thographic projection metaphor used in the visualization ofphysical objects to visualize software systems. Although thegeneral concept of OSM does not prescribe specic sets ofviews, a concrete OSM environment has to be specic aboutthe particular views to be used in a particular project. Atthe University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views denedby the KobrA 2.0 method, a version of KobrA adapted forOSM. In this paper we provide an overview of the KobrA 2.0metamodel underpinning nAOMi and give a small exampleof its use to model a software system.",
    "auteurs": [
      "Colin Atkinson",
      "Dietmar Stoll",
      "Jacques Robin",
      "Recife",
      "Brasil",
      "D.2.2"
    ],
    "institutions": [
      "Colin Atkinson University of Mannheim,",
      "Orthographic Software Modeling (OSM) is a view-centricsoftware engineering approach that aims to leverage the or-thographic projection metaphor used in the visualization ofphysical objects to visualize software systems. Although thegeneral concept of OSM does not prescribe specic sets ofviews, a concrete OSM environment has to be specic aboutthe particular views to be used in a particular project. Atthe University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views denedby the KobrA 2.0 method, a version of KobrA adapted forOSM. In this paper we provide an overview of the KobrA 2.0metamodel underpinning nAOMi and give a small exampleof its use to model a software system.",
      "Dietmar Stoll University of Mannheim, TunjicUniversity of Mannheim,"
    ],
    "mots_cles": [
      "Orthographic Software Modeling",
      " View-based Modeling"
    ],
    "texte_integral": "A Prototype Implementation of an Orthographic Software\nModeling Environment\nColin Atkinson\nUniversity of Mannheim,\nGermany\natkinson@informatik.uni-\nmannheim.de\nDietmar Stoll\nUniversity of Mannheim,\nGermany\nstoll@informatik.uni-\nmannheim.de\nChristian Tunjic\nUniversity of Mannheim,\nGermany\ntunjic@informatik.uni-\nmannheim.de\nJacques Robin\nUniversidade Federal de\nPernambuco, Recife, Brasil\njr@cin.ufpe.br\nABSTRACT\nOrthographic Software Modeling (OSM) is a view-centric\nsoftware engineering approach that aims to leverage the or-\nthographic projection metaphor used in the visualization of\nphysical objects to visualize software systems. Although the\ngeneral concept of OSM does not prescribe speci\ufb01c sets of\nviews, a concrete OSM environment has to be speci\ufb01c about\nthe particular views to be used in a particular project. At\nthe University of Mannheim we are developing a prototype\nOSM environment, nAOMi, that supports the views de\ufb01ned\nby the KobrA 2.0 method, a version of KobrA adapted for\nOSM. In this paper we provide an overview of the KobrA 2.0\nmetamodel underpinning nAOMi and give a small example\nof its use to model a software system.\nCategories and Subject Descriptors\nD.1.7 [Programming Techniques]: Visual Programming;\nD.2.2 [Design Tools and Techniques]: Computer-aided\nsoftware engineering (CASE); D.2.6 [Software Engineer-\ning]: Programming Environments\u2014Graphical environments\nKeywords\nOrthographic Software Modeling, View-based Modeling\n1.\nINTRODUCTION\nOrthographic Software Modeling (OSM) is based on three\nfundamental hypotheses \u2014 (a) that it is feasible to inte-\ngrate the many di\ufb00erent kinds of artifacts used in contempo-\nrary software engineering methods within a single coherent\nmethodology in which they are treated as views, (b) that it\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for pro\ufb01t or commercial advantage and that copies\nbear this notice and the full citation on the \ufb01rst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior speci\ufb01c\npermission and/or a fee.\nVAO \u201913, July 2, 2013, Montpellier, France\nCopyright 2013 ACM 978-1-4503-2041-2 ...$15.00.\nis feasible to create an e\ufb03cient and scalable way of support-\ning these views by generating them dynamically, on-the-\ufb02y,\nfrom a Single Underlying Model (SUM) using model-based\ntransformations and (c) that it is feasible to provide an in-\ntuitive metaphor for navigating around these many views\nby adapting the orthographic projection technique under-\npinning the CAD tools used in other engineering disciplines.\nFigure 1: Orthographic Projection.\nAs shown in Figure 1, the main advantages of using the\nidea of orthographic projection to de\ufb01ne the views used\nto visualize and described a system are that they (a) can\nbe organized according to a simple and easy-to-understand\nmetaphor and (b) collectively represent all the properties of\na system with minimal overlap and redundancy. In practice\nthis translates into a set of \u201cdimensions\u201d, each containing\nwell de\ufb01ned choices (or so called \u201cdimension elements\u201d) that\ncan be used to select individuals views.\nAs shown in Figure 2, the main advantage of making the\nartifacts used to describe a software system views of a SUM\nis that the number of pairwise coherence relationships that\nhave to be maintained is reduced and new views can be in-\ntroduced by simply de\ufb01ning their relationship to the SUM.\nMoreover, the importance of this advantage grows quickly\nas the size of the system and the complexity of the deployed\ndevelopment methodology increase. Another important ad-\nvantage is that the dominance of one particular kind of view\nover the development process (e.g. code) at the expense of\nother kinds of views (e.g. graphical models) is reduced so\nthat any appropriate type of views can be used to enrich\nthe underlying description of the system, depending on the\nneeds and skills of the stakeholder involved. This makes it\npossible to subsume all view types under the same, overarch-\nSUM\nSUM / View Centric Environment\nArtifact / Tools Centric Environment\nFigure 2: Consistency Dependencies in Artifact-oriented versus View-oriented Environments.\ning development process and methodology (e.g. agile-driven,\nfocusing on small development cycles, or model-driven de-\nvelopment, based on transformations between abstraction\nlevels). Although the details of how the views are created\nfrom the SUM and how the SUM is updated from the views\nare not central to the approach, a natural implementation\nis to use the visualization and transformation technologies\no\ufb00ered by model driven software engineering (MDSE).\nTo explore the validity of these hypotheses at the Uni-\nversity of Mannheim we have been developing a prototype\nOSM modeling environment based on an enhanced version\nof the KobrA method for model-driven, component-oriented\ndevelopment, KobrA 2.0 [1]. This was chosen as a basis for\nthe prototype, known as the Open, Adaptable, Orthographic\nModeling Environment (nAOMi) [13] because its views were\ndesigned with the precise goals of being (a) genuine pro-\njections of a subject containing carefully selected subsets\nof information about that subject, (b) minimalistic in the\nsense that they should overlap to the smallest extent possible\nand contain the minimum necessary models elements, and\n(c) selectable via a set of independent \u201cdimensions\u201d which\nre\ufb02ect di\ufb00erent fundamental concerns of development (i.e.\nabstraction levels, composition or variants). In other words,\nKobrA already provided one of the \u201cmost orthogonal\u201d sets\nof views for visualizing software systems of any contempo-\nrary method. More details about the actual views and di-\nmensions de\ufb01ned in KobrA are presented in the following\nsections. More information on OSM can be found in [2] and\n[3].\nnAOMi is implemented as an Eclipse plugin using the\nEclipse Modeling Framework (EMF) as the underlying mod-\neling platform and UML 2.0 tools [4] to generate and edit\nviews.\nThe KobrA 2.0 metamodel on which the current\nversion of nAOMi is based is a specialization of the UML\nmetamodel composed of three separate packages \u2014 one for\nthe SUM, one for the views and one for the transformations\n(Figure 3). The UML was chosen as the base language be-\ncause of its maturity and widespread acceptance, making the\nenvironment usable to the largest possible body of develop-\ners. UML elements not needed in KobrA 2.0 are excluded\nusing OCL constraints while new elements or properties are\nKobrA2\nTransformation\nSUM\nViews\nFigure 3: KobrA 2.0 Top Level Packages.\nintroduced by specializing existing elements.\nThe unique contribution of this paper is to elaborate on\nthe structure of the KobrA 2.0 metamodel and how it is used\nto drive nAOMi. The three following sections each focus on\none of the three main components of the metamodel \u2014 the\nSUM, the views and the transformations . This is followed\nby a brief overview of the OSM navigation paradigm in Sec-\ntion 5 before a small example of the approach is presented in\nSection 6. Section 7 then concludes the paper with related\nand future work.\n2.\nSUM PACKAGE\nFigure 4 depicts the internal structure of the SUM pack-\nage which is based on the UML metamodel. There are three\nmain subpackages, two containing the structural and behav-\nioral constructs respectively, and one containing the con-\nstraints that ensure that the metaclasses are used according\nto the KobrA conventions and rules.\nThe Classes subpackage of the Structure package contains\nsome of the most fundamental elements of the KobrA meta-\nmodel, such as Class and ComponentClass.\nThe internal\nstructure of this package is illustrated in Figure 5. Com-\nponentClass represents objects with complex and reusable\nbehaviors, while Class captures simple \u201cdata type\u201d objects\nthat have only very simple or non-reusable behaviors. The\nmodeler has to decide whether it is necessary to model a\nspeci\ufb01c part of the system as a ComponentClass and include\nstate charts and activity diagrams, or whether it is su\ufb03cient\nto use a Class (which is limited to using OCL constraints).\nComponentClass inherits (indirectly via Class) from Com-\nmunications so it also has the isActive attribute. This makes\nKobrA2::SUM::Constraint::Behavioral\nKobrA2::SUM::Constraint::Structural\nKobrA2::SUM::Constraint\nKobrA2::SUM::Constraint::Common\nKobrA2::SUM::Behavior::ProtocolStateMachines\nKobrA2::SUM::Behavior::Common\nKobrA2::SUM::Behavior::Activities\nKobrA2::SUM::Behavior::Actions\nKobrA2::SUM::Behavior\nKobrA2::SUM::Structure::Classes\nKobrA2::SUM::Structure::Types\nKobrA2::SUM::Structure::Instances\nKobrA2::SUM::Structure::Elements\nKobrA2::SUM::Structure\nKobrA2::SUM::Constraint::OclExpressions\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\n<<merge>>\nFigure 4: KobrA 2.0 SUM Package.\nit possible to model whether its instances are active or pas-\nsive. Active objects, which can be used to model threads and\nprocesses ([8] p. 438), start to execute their behavior as soon\nas they are created and perform operations spontaneously.\nA ComponentClass may exhibit complex behavior. In Ko-\nbrA, this behavior may be speci\ufb01ed in the form of\nUML\nState Diagrams (de\ufb01ning acceptable operation invocation\nsequences), and in the form of Activities (de\ufb01ning algorithms\nof operations). UML Interaction elements (in sequence dia-\ngrams) can be derived from the activity elements and thus\nare not included in the SUM. As KobrA aims to facilitate\nautomatic checking of allowed sequences of operation calls,\nProtocol State Machines are supported instead of general\nstate machines. Since the latter include a large variety of\nelements not needed for specifying acceptable operation se-\nquences or automatic checking, OCL constraints are used to\nprohibit the use of unwanted features.\ncontext\nComponentClass\n-- only\nallow\nActivity\nelements\nor\nProtocolStateMachines\ninv: ownedBehavior ->forAll( oclIsKindOf( Actitivity) or\noclIsKindOf ( ProtocolStateMachine ))\nFor example, since KobrA has no concept of roles for com-\nponents, the use of role also needs to be prohibited. The part\nassociation refers to owned properties of components whose\nattribute isComposite is true. As KobrA uses associations\nlike nests and creates for components, part, required and\nprovided are not needed. Connectors (i.e. delegation and\nassembly) are not used in KobrA either so ownedConnector\nis excluded.\nClass\nKobrA2::SUM::Structure::Classes\nGeneralizationSet\nAssociationClass\nComponentClass\nProperty\nUsage\nAssociation\nOperation\nPackageable\nElement\nParameter\nAcquires\nCreates\nNests\nUML::Component::PackagingComponents::Component\nUML::CommonBehaviors::Communications::Class\n+ownedOperation\n*\n+class\n0..1\n+supplier\n1..*\n{subsets supplierDependency}\n+supplierUsage\n*\n+client\n1..*\n{subsets clientDependency}\n+clientUsage\n*\n+ownedAttribute\n*\n+class\n0..1\n+powertype\n0..1\n+powertypeExtent\n*\n+packagedElement\n*\n{subsets component}\n+componentClass\n0..1\n+/superClass\nFigure 5: KobrA 2.0 Classes Package.\ncontext\nComponentClass\ninv: role ->union(part)->union( ownedConnector )\n->union( collaborationUse )-> union( representation )\n->union( realization)->union(required)\n->union(provided)->isEmpty ()\n3.\nVIEWS PACKAGE\nThe structure of the Views package is illustrated in Figure\n6. Again, since most of the views de\ufb01ned in KobrA 2.0 are\nbased on UML diagrams, the view metamodels have similar\nelements to the SUM metamodel. The big di\ufb00erence to the\nSUM is that there are no restrictions on the use of the view\nmetamodel elements.\nFor instance, views for a particular\npurpose such as supporting model checkers can be supported\nby adding elements unrelated to the UML.\nThe substructure of the Views package re\ufb02ects the types\nand organization of the KobrA views according to the view\n\u201cdimensions\u201d supported in nAOMi (cf. example in Section\n6). At the top level, the Views package is thus decomposed\ninto the Speci\ufb01cation and Realization options of the encap-\nsulation dimension.\nThese, in turn are both decomposed\ninto the Structural, Behavioral and Operational options of\nthe Projection dimension.\nFinally, with the exception of\nthe behavioral option, these are also all subdivided into the\nService and Type options of the granularity dimension. This\ndimension, with its two options, is an addition to the original\nversion of KobrA.\nThe Service view shows the direct, publicly visible rela-\ntionships of the subject ComponentClass to other Compo-\nnentClasses, while the Type view shows the publicly visi-\nble relationships of the subject to simple Classes. As with\nthe SUM, constraints have been de\ufb01ned to control what can\ngo into each view and when they are well formed. For ev-\nery view, a constraint enumerates all allowed elements (not\nshown in this paper).\nIn the following, some of the other constraints for the\nService view are elaborated. Since this view is a black-box\nview, the internals of ComponentClasses (nestedClassi\ufb01er)\nare not shown.\ncontext\nComponentClass\n-- no nested\nclassifiers , no\nprotocol\ninv: nestedClassifier ->union(protocol)->isEmpty ()\nClasses are only allowed if they are generalizations of Com-\nponentClasses, (or any of its superclasses, since a Compo-\nnentClass may inherit from a class as shown in the con-\nstraints with context Class. The following invariants ensure\nthat only publicly visible attributes and operations are in\nthis view, for both classes and ComponentClasses (which\ninherit from Class).\nClass\nService\nType\nInstance\nService\nType\nStructural\nSpecification\nOperational\nService\nType\nProtocol\nBehavioral\nKobrA2::Views::Derived\nComponentClassDependencies\nOperationDependencies\nInstance\nService\nType\nClass\nService\nType\nStructural\nRealization\nOperational\nService\nType\nBehavioral\nAlgorithm\nViews\nConcreteSyntax\nSubject\n<<import>>\n<<merge>>\n<<merge>>\n<<import>>\n<<merge>>\n<<import>>\nFigure 6: KobrA 2.0 Views package nesting.\ncontext\nClass\n-- only\nallow\nclasses\nthat\nare\ndirect or\nindirect\ngeneralizations\nof\nComponentClasses\nin this\nview\ndef: ccGeneralization : generalization .specific ->\nexists( oclIsKindOf ( ComponentClass ))\ninv:\ngeneralization .specific ->select( oclIsTypeOf (\nClass))->exists(s|s. ccGeneralization )\nor\nccGeneralization\n-- only\npublic\nattributes\nin this\nview\ninv: ownedAttribute ->forAll(visibility =# public)\n-- only\npublic\nOperations\nare\nallowed\nin the\nspecification\ninv: ownedOperation ->forAll(visibility =# public)\nOnly operation signatures are shown in this view, so pre-,\npost- and bodyconditions, as well as activities are omitted,\nwhich is re\ufb02ected in the last constraint.\ncontext\nOperation\n-- only\nthe\nsignature\nof the\nOperation\nis shown , not\nits\nbehavior (role\nname \"method\" refers to the\nActivities\nof the\noperation), or\ndependencies\ninv: method ->union( precondition )->union(body)->union(\npostcondition )->isEmpty ()\n4.\nTRANSFORMATIONS PACKAGE\nThe package AllViews provides the foundation for speci-\nfying the transformations between the SUM and the views\nin both directions. Part of the package\u2019s contents are shown\nin Figure 7.\nThe Abstraction concept (which is in fact a\nKobrA2::Transformation::Common::AllViews\nAbstraction\nTransformationExpression\nViewElement\nSumElement\nView\nKobrA2::SUM::Structure::Elements::Element\nKobrA2::Views::ConcreteSyntax::Element\nKobrA2::SUM::Constraint::Behavioral::Exp\nressionInOcl\nKobrA2::Views::Subject::View\n{subsets mapping}\n0..1\n0..1\n{subsets clientDependency}\n+abstraction 1\n{subsets client}\n+ve 1\n1..*\n1\n{subsets supplier}\n+se 1\n{subsets supplierDependency}\n+abstraction 1..*\nFigure 7: Transformation abstractions.\ndependency reused from the UML but with additional con-\nstraints) plays the key role in relating elements from the\nSUM to elements of a view. Abstraction is actually mapped\nto ExpressionInOcl.\nWhen appearing in transformations,\nthe equals sign links elements in the SUM to the respective\nelements in the view, and vice versa. For instance, equal-\nity of the general meta-association of a Generalization in\na transformation invariant means that, when following gen-\neral, there must be an element in the SUM and in the view\nfor which similar transformation expressions are speci\ufb01ed.\nIn the case of KobrA 2.0, which has many projections that\njust select a subset of elements using one-to-one abstrac-\ntions, this allows concise declarative TransformationExpres-\nsions. Together with the view constraints, a CASE tool can\nbe implemented which uses a transformation language of the\nimplementor\u2019s choice, for instance the Atlas Transformation\nLanguage (ATL) [11] or QVT [9]. The role names se and ve\nare short for SumElement and ViewElement, respectively.\nThese roles subset the client and supplier roles from the\nUML.\nSUM elements are translated into UML elements with\nstereotypes, so that the views are easy to manage for de-\nvelopers familiar with the UML. The bidirectional mappings\nbetween stereotyped view elements and non-stereotyped SUM\nelements are expressed in the constraints of the Association-\nAbstraction, a subclass of the Abstraction from the AllViews\npackage. This is also an example of a transformation which\nis reused in other views.\ncontext\nAssociationAbstraction\ninv: ve.memberEnd = se.memberEnd\ninv: ve.ownedEnd = se.ownedEnd\nivn: ve. navigableOwnedEnd = se. navigableOwnedEnd\ninv: se. oclIsKindOf(Acquires) implies ve.\nhasStereotype (\u2019acquires \u2019)\ninv: ve. hasStereotype (\u2019acquires \u2019)\nimplies\nse.\noclIsKindOf (Aquires)\ninv: se. oclIsKindOf(Nests) implies\nve. hasStereotype (\u2019\nnests \u2019)\ninv: ve. hasStereotype (\u2019nests \u2019)\nimplies se. oclIsKindOf\n(Nests)\ninv: se. oclIsKindOf (Creates) implies\nve. hasStereotype\n(\u2019creates \u2019)\ninv: ve. hasStereotype (\u2019creates \u2019)\nimplies se.\noclIsKindOf (Creates)\nFigure 8 shows the main elements involved in the trans-\nformation of the black box structural view for Component-\nClasses. The \ufb01rst transformation constraint is on the view\nand declares the starting point for the transformation. It\nstates that the subject ComponentClass and its generaliza-\ntions (using a SUM utility function, superClosure) are in the\nview.\nThe following transformation rules illustrate how to create\nthe output (i.e. view) elements from the input (i.e. SUM) el-\nements, such as the publicly visible attributes and operations\nof the ComponentClass and the acquired ComponentClasses.\nThe \ufb01rst constraint for ComponentClassAbstraction states\nthat references to potential general classes (and Component-\nClasses) of ComponentClasses are mirrored in the view. In\naddition, ComponentClasses will be shown with the corre-\nsponding stereotypes.\nThe ComponentClass owns various\ntypes of associations, so in this view only the acquires asso-\nciations are selected (whose transformation rules are cov-\nered in the common transformation packages).For classes\nand ComponentClasses, only publicly visible attributes and\noperations appear in the view.\nClass invariants are also\ncopied. Classes that may appear in this view (e.g. as gener-\nalizations of ComponentClasses) may have a powertype (role\nname powertypeExtent) which will be displayed.\nThe last transformation statement copies the class refer-\nences of operations. As with all views, the transformation\nrules, the common transformation statements (which also\ncover operations) and the view constraints serve as a speci-\n\ufb01cation for the implementation of a view. Individual CASE\ntools can use di\ufb00erent implementation techniques as long as\nthey conform to the semantics of these rules and constraints.\nKobrA2::Transformation::Specification::Structural::Class::Service\nComponentClassAbstraction\nKobrA2::Transformation::Common::Feature::OperationAbstraction\nKobrA2::Transformation::Common::AllViews::Abstraction\nKobrA2::SUM::Structure::Classes::ComponentClass\nKobrA2::SUM::Structure::Classes::Operation\nKobrA2::SUM::Structure::Classes::Class\nOperationAbstraction\nClassAbstraction\n+se\n1\n1..*\n+se\n1\n1..*\n+se\n1\n1..*\nFigure 8: Transformation to the Speci\ufb01cation Structural Service View.\ncontext\nKobrA2 :: Views :: Subject ::\nSpecificationStructuralClassService\ninv: ownedMember ->select( oclIsKindOf(Class)) =\nsubject.superClosure ->union(subject.acquires.\nsuperClosure )\ncontext\nComponentClassAbstraction\ninv: ve.superClass = se. superClass\ninv: ve. hasStereotype (\u2019ComponentClass \u2019)\ninv: se.isSubject\nimplies (ve. hasStereotype (\u2019subject\n\u2019) and ve.ownedMember ->select( oclIsKindOf (\nAssociation )) = se.ownedMember ->select(\noclIsKindOf (Acquires)))\ncontext\nClassAbstraction\ninv: ve. ownedAttribute = se.ownedAttribute ->select(\nvisibility =# public)\ninv: ve. ownedOperation = se.ownedOperation ->select(\nvisibility =# public)\ninv: ve.\u2018inv \u2019 = se.\u2018inv \u2019\n-- copy\npowertypeExtent\nthat is only\nallowed\nfor\nclass\ninv: ve. powertypeExtent = se. powertypeExtent\ncontext\nOperationAbstraction\ninv: ve.class = se.class\nFor the black box type view, only publicly visible at-\ntributes and operations of classes (as opposed to Compo-\nnentClasses) used by the subject can be seen. This is spec-\ni\ufb01ed in the \ufb01rst rule which de\ufb01nes owned members of the\nview and thus serves as the starting point of the transfor-\nmation. cbbTypes is a utility function de\ufb01ned in the SUM\nwhich computes the black box types by selecting the types\nof the subject\u2019s public attributes and parameter types of its\npublic operations.\nClass invariants and potential powertypes and connections\nto the classes in this view are shown as well. There may\nalso be Enumerations, for which the EnumerationLiterals\nare displayed.\nThe transformation rules for this view are almost the same\nas the realization transformation constraints from the pack-\nage Transformation::Realization::Structural::Class::Type. The\ndi\ufb00erences are the select(visibility=#public) statements for\noperations and attributes.\ncontext\nKobrA2 :: Views :: Subject ::\nSpecificationStructuralClassType\ninv: ownedMember ->select( oclIsKindOf(Class) or\noclIsKindOf(\u2018Enumeration \u2019) or\noclIsKindOf (\nAssociation)) = subject ->union(subject.cbbTypes)\ncontext\nComponentClassAbstraction\ninv: se.isSubject\nimplies\nve. hasStereotype (\u2019subject \u2019)\ncontext\nClassAbstraction\ninv: not se.oclIsKindOf ( ComponentClass ) implies (\nve. ownedAttribute = se.ownedAttribute ->select(\nvisibility =# public)\nve. ownedOperation = se.ownedOperation ->select(\nvisibility =# public))\ninv: ve. powertypeExtent = se. powertypeExtent\ninv: ve. superClass = se.superClass\ninv: \u2018ve.inv \u2019 = \u2018se.inv \u2019\ncontext\nComponentClassAbstraction\ninv: se.isSubject\nimplies\nve. hasStereotype (\u2019subject \u2019)\ncontext\nEnumerationAbstraction\ninv: ve. ownedLiteral = se. ownedLiteral\ncontext\nEnumerationLiteralAbstraction\ninv: ve. specification = se. specification .\nstringInSignature\n5.\nNAVIGATION\nMost of today\u2019s tools use some combination of trees to\norganize the content of models as well as the views used to\nvisualize a software system or component. In an any envi-\nronment incorporating a number of di\ufb00erent tools there is\ninvariably a large number of di\ufb00erent trees storing a het-\nerogeneous mix of artifacts including model elements (e.g.\nclasses, instances, associations), diagrams (e.g.\nclass dia-\ngrams, state diagrams) and other artifact types (source code,\nXML \ufb01les, con\ufb01guration \ufb01les ). To work with all the views in\na traditional development environment, therefore, engineers\ntypically have to learn about the organization structures of\nall the incorporated tools.\nIn contrast to conventional paradigms for organizing and\nnavigating the many views used to visualize a system, OSM\nemploys the metaphor of a multi-dimensional cube. More\nspeci\ufb01cally, as illustrated in Figure 9, OSM regards dimen-\nsion of the underlying methodology as representing a di\ufb00er-\nent dimension of the cube, and each independently variable\naspect of that dimension is a selectable dimension element.\nSelecting a view thus simply corresponds to selecting a single\ncell within the cube. In general, three types of dimensions\nare supported: static dimensions in which the number of\nFigure 9: Dimension-based navigation.\nselectable elements (i.e. coordinates) is \ufb01xed, dynamic di-\nmensions in which the number of elements is dynamic (i.e.\nderived from the SUM), and mixed dimensions which have\nboth static and dynamic elements.\nTo support the OSM dimension based navigation metaphor\nfor KobrA, we de\ufb01ned the seven dimensions indicated on the\nleft hand side of Figure 10 which is a sceenshot of nAOMI.\nThe Abstraction dimension (not expanded here), which has\nthree static dimension elements, PIM (platform independent\nmodel), PSM (platform speci\ufb01c model) and Code, captures\nthe model-driven development concern of KobrA. The ver-\nsion dimension captures the state of the modeled system at\nspeci\ufb01c points in time. The Component dimension, which\nhas dynamic dimension elements de\ufb01ned by instances of the\nclass ComponentClass in the SUM, captures the component-\nbased development concern of KobrA.\nThe Encapsulation dimension, which has two \ufb01xed ele-\nments, supports the distinction between Speci\ufb01cation (black\nbox) and Realization (white box) views of components, while\nthe Projection dimension with the \ufb01xed elements Structural,\nOperational and Behavioral covers the di\ufb00erent information\ntypes. The Granularity dimension provides a \ufb01ner grained\ndistinction between views describing the types used by com-\nponents (Type granularity) and views describing the required\nand provided interfaces (Service granularity). The Opera-\ntion dimension allows a selection of individual operations.\nIn the ideal case, when all views are truly orthogonal, the\nchoices that can be made in each dimensions are completely\nindependent.\nHowever, this is very di\ufb03cult to achieve in\nsoftware engineering. The approach still works if the views\nare not completely orthogonal, but dependencies then occur\nbetween di\ufb00erent choices in di\ufb00erent dimensions, so that the\ndecisions made in one dimensions may a\ufb00ect choices possi-\nble in another dimension. This is best handled by giving\ndimensions a precedence ranking determined by the order\nin which they appear (the top being the highest). When an\nelement in a dimension is selected, the tool automatically\nmakes default selections for dimensions of lower precedence\n(i.e.\ndimensions lower down) and disables selections that\nwould navigate to cells (i.e. views) which are not (yet) de-\n\ufb01ned by the method at hand.\n6.\nSHOPPING CART EXAMPLE\nTo show how a software system can be speci\ufb01ed using\nnAOMi, this section presents a case study based on a shop-\nping cart system. A ShoppingCart component collects and\nFigure 10: Speci\ufb01cation Structural View.\nmanages the products selected by users and supports pay-\nment via a credit card.\nFigure 10 illustrates a structural\nview of the component.\nIn the dimension navigator on the left hand side, PIM\nwas chosen for the \u201cAbstraction Level\u201d (not expanded in the\nscreenshot). The second dimension is the state of the soft-\nware system at a certain point in time. The picture shows\nthat the latest available version was chosen. As with every\nchoice in a dimension, it may in\ufb02uence the options in lower\nranked dimensions. The component under consideration is\nthe ShoppingCart, for which a black box view is selected\nin the next dimension. After the user selects the structural\nprojection option and the service level granularity, the tool\nautomatically chooses the option for all operations in the\nlast dimension, as there is no editor registered for the other\noptions.\nThe component under development is presented with the\nstereotype subject and its relationship to other components\nand classes is shown in the view, which corresponds to a cell\nof the multi-dimensional navigation cube, and is generated\non-the-\ufb02y from the SUM when it is selected. The classes\nProduct and CreditCard can be used as data types in the\noperations of the component.\nFigure 11 illustrates the operational view in which an\noperation can be formalized using pre- and postconditions.\nThe precondition corresponds to the assumes clause in and\nthe postcondition corresponds to the result clause. As in the\nUML, the precondition of an operation must be true when\nthe operation is invoked and the postcondition must be true\nwhen the operation is \ufb01nished. The operation addProduct\nin Figure 11 must be in state CollectingProducts or Empty\nwhen invoked. This is also visible in the behavioral view,\nFigure 11: addProduct() Operation Speci\ufb01cation.\nsince there are only two transitions with the operation ad-\ndProduct. Both leads to the state CollectingProducts which\nis also a postcondition of the operation. The second post-\ncondition is that the cost attribute of the component must\nbe increased by the price of the added product. The pre- and\npostcondition can be expressed using the OCL. The proper-\nties of the component, states and operation parameters can\nbe used to formalise the constraints like as in this example.\nFigure 12 shows the publicly visible behaviour of the Shop-\npingCart component with states and transitions. The condi-\ntional transitions map to operations of the component. Like\nevery view, this view is also synchronized with the SUM so\nthat it is guaranteed that its operations, states and proper-\nties are consistent with those in the structural view.\nFigure 12: Speci\ufb01cation Behavioral Model.\nAlthough the operational view seems to be similar to the\nbehavioral view because of the overlapping information within\nthem, there are signi\ufb01cant di\ufb00erences. The focus of the op-\nerational view is on a precise formal de\ufb01nition of an opera-\ntion of a component. The operations can be enriched by pre-\nand postconditions which can be de\ufb01ned using complex OCL\nstatements, that formalize the complete behavior of an op-\neration. The additional information in the OCL statements\ncan be used for code generation and documentation.\n7.\nCONCLUSION\nAt the beginning of the paper we identi\ufb01ed three funda-\nmental hypothesis upon which the notion of OSM is based\n\u2014 (a) that it is feasible to integrate the many di\ufb00erent kinds\nof artifacts used in contemporary software engineering meth-\nods within a single coherent methodology in which they are\ntreated as views, (b) that it is feasible to create an e\ufb03-\ncient and scalable way of supporting these views by gener-\nating them dynamically, on-the-\ufb02y, from a Single Underly-\ning Model (SUM) using model-based transformations and\n(c) that it is feasible to provide an intuitive metaphor for\nnavigating around these many views by adapting the ortho-\ngraphic projection technique underpinning the CAD tools\nused in other engineering disciplines.\nThe prototype tool, nAOMi, described in this paper rep-\nresents the \ufb01rst step towards demonstrating the validity of\nthese hypotheses and showing that OSM is a viable approach\nto software engineering. Of the three hypotheses, (a) and (c)\nare most convincingly demonstrated by the prototype, since\nit shows that it is indeed possible to support all the views\nof the KobrA method within a single navigation metaphor.\nThe prototype tool does not demonstrate the validity of hy-\npothesis (b) to the same extent as the others due to its\nsmall size. Although it demonstrates the feasibility of gen-\nerating views from the SUM and vice-versa, the question of\nwhether such an approach scales up to large environments\nis still open.\nAlthough nOAMi is the only tool developed with the spe-\nci\ufb01c aim of supporting KobrA-based OSM, several other\ntools and methods have similar properties or aims.\nFor\nexample, Glinz et al.\n[10] describe a tool with a \ufb01sheye\nzooming algorithm which lets the user view a model with\nvarying amounts of detail depending on the context. It has\nto be investigated whether it is possible to combine the \ufb01sh-\neye zooming concept with the dimension-based navigation\nparadigm. While the KobrA 2.0 implementation of nAOMi\nheavily uses UML diagrams for developers, Glinz et al. use\ncustom diagram types, e.g.\nfor structural and behavioral\nviews.\nAn approach which also emphasizes the description of for-\nmal consistency rules (correspondences) between views is\nRM-ODP [5][6].\nHowever, this approach does not explic-\nitly mention the notion of a SUM and thus implies that\nconsistency rules should be de\ufb01ned in a pairwise fashion be-\ntween individual pairs of views. ArchiMate [7], which com-\nplements TOGAF [12], is an enterprise architecture mod-\neling language which o\ufb00ers two orthogonal \u201ddimensions\u201d for\nmodeling, (business, architecture, and technology) layers and\n(informational, behavioral and structural) aspects and also\nsuggests two more dimensions, purpose and abstraction level.\nHowever, as many of these views span multiple choices of a\nsingle\u201cdimension\u201d, the intuitive dimension-based navigation\nmetaphor of OSM can not be easily applied. There are also\nmore general approaches for view-based modeling but they\nare less speci\ufb01c in terms of consistency rules between views\nand provide little guidance on how to manage and navigate\nviews, for example the Zachman Framework [14].\nRegarding the practical use of OSM environments in the\nfuture, the biggest challenge is developing appropriate SUM\nmetamodels which can accommodate all the types of views\nand services that software engineers are accustomed to to-\nday. For this \ufb01rst prototypical SUM-based environment sup-\nporting the OSM approach we had a method at our disposal\n(KobrA) that already de\ufb01ned a full set of orthogonal UML-\nbased views. This allowed us to model the required SUM\nand view metamodels by simply adapting the UML meta-\nmodels, removing and adding model elements as needed.\nIn doing so we were able to manually ensure that the meta-\nmodels ful\ufb01lled the two core requirements of SUM-based en-\nvironments \u2014 (1) being minimalistic and (2) redundancy\nfree. If SUM-based software engineering environments are\nto take o\ufb00, and to be introduced into existing, heteroge-\nneous environments, more sophisticated ways of integrating\nexisting metamodels into a single uni\ufb01ed metamodel will be\nrequired.\n8.\nREFERENCES\n[1] C. Atkinson, J. Bayer, C. Bunse, E. Kamsties,\nO. Laitenberger, R. Laqua, D. Muthig, B. Paech,\nJ. W\u00a8ust, and J. Zettel. Component-Based Product Line\nEngineering with UML. Addison Wesley, Reading,\nMassachusetts, USA, 1st edition, November 2001.\n[2] C. Atkinson, D. Stoll, and P. Bostan. Orthographic\nSoftware Modeling: A Practical Approach to\nView-Based Development. In Evaluation of Novel\nApproaches to Software Engineering, volume 69 of\nCommunications in Computer and Information\nScience, pages 206\u2013219. Springer Berlin Heidelberg,\n2010.\n[3] C. Atkinson, D. Stoll, and C. Tunjic. Orthographic\nService Modeling. In Proceedings of 15th IEEE EDOC\nConference Workshops (EDOCW), Helsinki, Finland,\n2011.\n[4] Eclipse Foundation. UML2Tools.\nhttp://wiki.eclipse.org/MDT-UML2Tools, 2013.\n[5] ISO/IEC and ITU-T. The Reference Model of Open\nDistributed Processing. RM-ODP, ITU-T Rec.\nX.901-X.904 / ISO/IEC 10746.\nhttp://standards.iso.org/\nittf/PubliclyAvailableStandards/index.html,\n1998.\n[6] J. I. J. Jose Raul Romero and A. Vallecillo. Realizing\nCorrespondences in MultiViewpoint Speci\ufb01cations. In\nProceedings of the Thirteenth IEEE International\nEDOC Conference, 1 - 4 September 2009, Auckland,\nNew Zealand, September 2009.\n[7] M. Lankhorst. Enterprise Architecture at Work.\nSpringer Berlin Heidelberg, 2009.\n[8] Object Management Group (OMG). OMG Uni\ufb01ed\nModeling Language (OMG UML), Superstructure,\nV2.1.2.\nhttp://www.omg.org/cgi-bin/doc?formal/07-11-02,\nNovember 2007.\n[9] Object Management Group (OMG). Meta Object\nFacility (MOF) 2.0 Query/View/Transformation, v1.0.\nhttp://www.omg.org/spec/QVT/1.0/PDF/, April 2008.\n[10] C. Seybold, M. Glinz, S. Meier, and N. Merlo-Schett.\nAn e\ufb00ective layout adaptation technique for a\ngraphical modeling tool. In Proceedings of the 2003\nInternational Conference on Software Engineering,\nPortland, 2003.\n[11] The Atlas Transformation Language (ATL). O\ufb03cial\nWebsite. http://www.eclipse.org/atl/, 2013.\n[12] The Open Group. TOGAF Version 9 - The Open\nGroup Architecture Framework.\nhttp://www.opengroup.org/architecture/\ntogaf9-doc/arch/index.html, Feb 2009.\n[13] University of Mannheim - Software Engineering\nGroup. nAOMi - opeN, Adaptable, Orthographic\nModeling EnvIronment.\nhttp://eclipselabs.org/p/naomi.\n[14] J. A. Zachman. The Zachman Framework: A Primer\nfor Enterprise Engineering and Manufacturing.\nhttp://www.zachmaninternational.com, 2009.\n",
    "pdf_url": "",
    "references": [
      "[1] C. Atkinson, J. Bayer, C. Bunse, E. Kamsties,",
      "O. Laitenberger, R. Laqua, D. Muthig, B. Paech,",
      "J. W\u00a8ust, and J. Zettel. Component-Based Product Line",
      "Engineering with UML. Addison Wesley, Reading,",
      "Massachusetts, USA, 1st edition, November 2001.",
      "[2] C. Atkinson, D. Stoll, and P. Bostan. Orthographic",
      "Software Modeling: A Practical Approach to",
      "View-Based Development. In Evaluation of Novel",
      "Approaches to Software Engineering, volume 69 of",
      "Communications in Computer and Information",
      "Science, pages 206\u2013219. Springer Berlin Heidelberg,",
      "2010.",
      "[3] C. Atkinson, D. Stoll, and C. Tunjic. Orthographic"
    ],
    "publication_date": "07-11-2023"
  },
  {
    "titre": "Framing the News:From Human Perception to Large Language Model Inferences",
    "resume": "Identifying the frames of news is important to understand the arti-cles vision, intention, message to be conveyed, and which aspectsof the news are emphasized. Framing is a widely studied conceptin journalism, and has emerged as a new topic in computing, withthe potential to automate processes and facilitate the work of jour-nalism professionals. In this paper, we study this issue with articlesrelated to the Covid-19 anti-vaccine movement. First, to under-stand the perspectives used to treat this theme, we developed aprotocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries.Headlines are key units in the written press, and worth of analysisas many people only read headlines (or use them to guide theirdecision for further reading.) Second, considering advances in Nat-ural Language Processing (NLP) with large language models, weinvestigated two approaches for frame inference of news headlines:first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study andanalysis of the performance that these models have to facilitatejournalistic tasks like classification of frames, while understandingwhether the models are able to replicate human perception in theidentification of these frames.",
    "auteurs": [
      "David Alonso",
      "del Barrio",
      "David Alonso",
      "del Barrio",
      "Daniel Gatica-Perez"
    ],
    "institutions": [
      "Daniel  Idiap Research Institute and EPFLSwitzerland",
      "David Alonso del  Idiap Research InstituteSwitzerland"
    ],
    "mots_cles": [
      "Covid-19 no-vax",
      " news framing",
      " GPT-3",
      " prompt-engineering",
      " trans-formers",
      " large language models"
    ],
    "texte_integral": "Framing the News:\nFrom Human Perception to Large Language Model Inferences\nDavid Alonso del Barrio\nddbarrio@idiap.ch\nIdiap Research Institute\nSwitzerland\nDaniel Gatica-Perez\ngatica@idiap.ch\nIdiap Research Institute and EPFL\nSwitzerland\nABSTRACT\nIdentifying the frames of news is important to understand the arti-\ncles\u2019 vision, intention, message to be conveyed, and which aspects\nof the news are emphasized. Framing is a widely studied concept\nin journalism, and has emerged as a new topic in computing, with\nthe potential to automate processes and facilitate the work of jour-\nnalism professionals. In this paper, we study this issue with articles\nrelated to the Covid-19 anti-vaccine movement. First, to under-\nstand the perspectives used to treat this theme, we developed a\nprotocol for human labeling of frames for 1786 headlines of No-\nVax movement articles of European newspapers from 5 countries.\nHeadlines are key units in the written press, and worth of analysis\nas many people only read headlines (or use them to guide their\ndecision for further reading.) Second, considering advances in Nat-\nural Language Processing (NLP) with large language models, we\ninvestigated two approaches for frame inference of news headlines:\nfirst with a GPT-3.5 fine-tuning approach, and second with GPT-\n3.5 prompt-engineering. Our work contributes to the study and\nanalysis of the performance that these models have to facilitate\njournalistic tasks like classification of frames, while understanding\nwhether the models are able to replicate human perception in the\nidentification of these frames.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Information extraction; \u2022\nHuman-centered computing \u2192 Text input.\nKEYWORDS\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering, trans-\nformers, large language models\nACM Reference Format:\nDavid Alonso del Barrio and Daniel Gatica-Perez. 2023. Framing the News:\nFrom Human Perception to Large Language Model Inferences . In Inter-\nnational Conference on Multimedia Retrieval (ICMR \u201923), June 12\u201315, 2023,\nThessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3591106.3592278\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0178-8/23/06...$15.00\nhttps://doi.org/10.1145/3591106.3592278\n1\nINTRODUCTION\nIn recent years, there has been a proliferation in the use of concepts\nsuch as data journalism, computational journalism, and computer-\nassisted reporting [15] [29], which all share the vision of bridging\njournalism and technology. The progress made in NLP has been\ngradually integrated into the journalistic field [5][8][54]. More\nspecifically, machine learning models based on transformers have\nbeen integrated in the media sector in different tasks [41] such as\nthe creation of headlines with generative languages models [17],\nsummarization of news articles [28][27], false news detection [49],\nand topic modeling and sentiment analysis [25]. The development of\nlarge language models such as GPT-3 [9], BLOOM [51] or ChatGPT\nshow a clear trend towards human-machine interaction becoming\neasier and more intuitive, opening up a wide range of research\npossibilities. At the same time, the use of these models is also\nassociated with a lack of transparency regarding how these models\nwork, but efforts are being made to bring some transparency to\nthese models, and to analyze use cases where they can be useful and\nwhere they cannot [35]. Based on the premises that these models\nopen up a wide range of research directions [7], and that at the same\ntime (and needless to say) they are not the solution to all problems,\nwe are interested in identifying use cases and tasks where they\ncan be potentially useful, while acknowledging and systematically\ndocumenting their limitations [56]. More specifically, the aim of\nthis work is to analyze the performance of GPT-3.5 for a specific\nuse case, namely the analysis of frames in news, from an empirical\npoint of view, with the objective of shedding light on a potential\nuse of generative models in journalistic tasks.\nFrame analysis is a concept from journalism, which consists of\nstudying the way in which news stories are presented on an issue,\nand what aspects are emphasized: Is a merely informative vision\ngiven in an article? Or is it intended to leave a moral lesson? Is\na news article being presented from an economic point of view?\nOr from a more human, emotional angle? The examples above\ncorrespond to different frames with which an article can be written.\nThe concept of news framing has been studied in computing as\na step beyond topic modeling and sentiment analysis, and for this\npurpose, in recent years, pre-trained language models have been\nused for fine-tuning the classification process of these frames [60]\n[10], but the emergence of generative models opens the possibility\nof doing prompt-engineering of these classification tasks, instead\nof the fine-tuning approach investigated so far.\nOur work aims to address this research gap by posing the fol-\nlowing research questions:\nRQ1: What are the main frames in the news headlines about\nthe anti-vaccine movement, as reported in newspapers across 5\nEuropean countries?\n627\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nDavid Alonso del Barrio and Daniel Gatica-Perez\nRQ2: Can prompt engineering be used for classification of head-\nlines according to frames?\nBy addressing the above research questions, our work makes the\nfollowing contributions:\nContribution 1. We implemented a process to do human an-\nnotation of the main frame of 1786 headlines of articles about the\nCovid-19 no-vax movement, as reported in 19 newspapers from 5\nEuropean countries (France, Italy, Spain, Switzerland and United\nKingdom.) At the headline level, we found that the predominant\nframe was human interest, where this frame corresponds to a per-\nsonification of an event, either through a statement by a person,\nor the explanation of a specific event that happened to a person.\nFurthermore, we found a large number of headlines annotated as\ncontaining no frame, as they simply present information without\nentering into evaluations. We also found that for all the countries\ninvolved, the distribution of frame types was very similar, i.e., hu-\nman interest and no frame are the two predominant frames. Finally,\nthe generated annotations allowed to subsequently study the per-\nformance of a large language model.\nContribution 2. We studied the performance of GPT-3.5 on\nthe task of frame classification of headlines. In addition to using\nthe fine-tuning approach from previous literature, we propose an\nalternative approach for frame classification that requires no labeled\ndata for training, namely prompt-engineering using GPT-3.5. The\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy\n(slightly higher than other smaller models), and that the prompt-\nengineering approach results in lower performance (49% accuracy.)\nOur analysis also shows that the subjectivity of the human labeling\ntask has an effect on the obtained accufracy.\nThe paper is organized as follows. In Section 2, we discuss related\nwork. In Section 3, we describe the news dataset. In Section 4, we\ndescribe the methodology for both human labeling and machine\nclassification of news frames. We present and discuss results for\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide\nconclusions in Section 7.\n2\nRELATED WORK\nFraming has been a concept widely studied in journalism, with a\ndefinition that is rooted in the study of this domain [23]: \u201cTo frame\nis to select some aspects of a perceived reality and make them more\nsalient in a communicating text, in such a way as to promote a par-\nticular problem definition, causal interpretation, moral evaluation,\nand/or treatment recommendation for the item described.\u201d\nFor frame recognition, there are two main approaches: the induc-\ntive approach [16], where one can extract the frames after reading\nthe article, and the deductive approach [38], where a predefined\nlist of frames exists and the goal is to interpret if any of them ap-\npears in the article. In the deductive case, there are generic frames\nand subject-specific frames, and the way to detect them typically\ninvolves reading and identifying one frame at a time, or through\nanswers to yes/no questions that represent the frames. Semetko et\nal. [52] used 5 types of generic frames (attribution of responsibility,\nhuman interest, conflict, morality, and economic consequences)\nbased on previous literature, and they defined a list of 20 yes/no\nquestions to detect frames in articles. For instance, the questions\nabout morality are the following: \"Does the story contain any moral\nmessage? Does the story make reference to morality, God, and other\nreligious tenets? Does the story offer specific social prescriptions\nabout how to behave?\", and so on for each of the frame types. This\ncategorization of frames has been used in various topics such as\nclimate change [18] [19], vaccine hesitance [13], or immigration\n[34].\nWe now compare the two approaches on a common topic, such\nas Covid-19. Ebrahim et al. [21] followed an inductive approach\nin which the frames were not predefined but emerged from the\ntext (e.g., deadly spread, stay home, what if, the cost of Covid-19)\nusing headlines as the unit of analysis. In contrast, the deductive\napproach has studied very different labels. El-Behary et al. [22]\nfollowed the method of yes/no questions, but in addition to the\n5 generic frames presented before, they also used blame frame\nand fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5\ngeneric frames with yes/no questions, while Catal\u00e1n-Matamoros et\nal. [14] used the 5 frames and read the headline and subheadline\nto decide the main frame. Table 1 summarizes some of the the\nexisting approaches. This previous work showed how frame labels\ncan be different, and also that frame analysis has been done at both\nheadline and article levels. These two approaches (inductive and\ndeductive) that originated in journalism have since been replicated\nin the computing literature.\nWe decided to follow the deductive approach because a prede-\nfined list of frames allows to compare among topics, countries,\nprevious literature, and also because they represent a fixed list of\nlabels for machine classification models. Furthermore, the induc-\ntive approach tends to be more specific to a topic, and from the\ncomputing viewpoint, past work has tried to justify topic modeling\nas a technique to extract frames from articles.\nYl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex-\ntraction technique. They argued that topics can be interpreted as\nframes if three requirements are met: frames are operationalized as\nconnections between concepts; subject-specific data is selected; and\ntopics are adequately validated as frames, for which they suggested\na practical procedure. This approach was based on the choice of a\nspecific topic (e.g., climate change) and the use of Latent Dirichlet\nAllocation (LDA) as a technique to extract a number of subtopics.\nIn a second phase, a qualitative study of the top 10 words of each\nsubtopic was performed, and the different subtopics were elimi-\nnated or grouped, reducing the number and establishing a tentative\ndescription. In a third phase, the top 10 articles belonging to that\nframe/topic were taken, and if the description of the topic fitted\nat least 8 of the 10 articles, that topic/frame remained. The frames\nfound in this article were: green growth, emission cuts, negotiations\nand treaties, environmental risk, cost of carbon emissions, Chinese\nemissions, economics of energy production, climate change, en-\nvironmental activism, North-South burden sharing, state leaders\nnegotiating, and citizen participation.\nFrom Entman\u2019s definition of frame [23], it seems that the deduc-\ntive approach is more refined than the inductive approach (which\nseems to resemble the detection of sub-themes.) For example, with\nregard to climate change, there are stories on how people have been\naffected by climate change from an emotional point of view, thus\npersonalizing the problem. In this case, we could categorize the\ncorresponding frame as human interest, as the writer of the article\nis selecting \"some aspects of a perceived reality and make them\n628\nFraming the News:\nFrom Human Perception to Large Language Model Inferences\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nmore salient\". The language subtleties with which news articles are\npresented cannot be captured with basic topic modeling.\nIsoaho et al.[30] held the position that while the benefits of\nscale and scope in topic modeling were clear, there were also a\nnumber of problems, namely that topic outputs do not correspond\nto the methodological definition of frames, and thus topic modeling\nremained an incomplete method for frame analysis. Topic modeling,\nin the practice of journalistic research, is a useful technique to deal\nwith the large datasets that are available, yet is often not enough to\ndo more thorough analyses [31]. In our work, we clearly notice that\nframe analysis is not topic modeling. For example, two documents\ncould be about the same topic, say Covid-19 vaccination, but one\narticle could emphasize the number of deaths after vaccination,\nwhile the other emphasized the role of the vaccine as a solution to\nthe epidemic.\nWe also consider that the larger the number of possible frame\ntypes, the more likely it is to end up doing topic modeling instead of\nframe analysis. Using a deductive approach, Dallas et al. [12] created\na dataset with articles about polemic topics such as immigration,\nsame sex marriage, or smoking, and they defined 15 types of frames:\n\"economic, capacity and resources, morality, fairness and equality,\nlegality, constitutionality and jurisprudence, policy prescription and\nevaluation, crime and punishment, security and defense, health and\nsafety, quality of life, cultural identity, political, external regulation\nand reputation, other\". In this case, they authors did not use a list\nof questions. Instead, for each article, annotators were asked to\nidentify any of the 15 framing dimensions present in the article\nand to label text blurbs that cued them (based on the definitions of\neach of the frame dimensions) and decide the main frame of each\narticle. In our case, we followed the idea of detecting the main frame\nby reading the text instead of answering questions, but instead of\nusing the 15 frames proposed in [12] , we used the 5 generic frames\nproposed in [52].\nA final decision in our work was the type of text to analyze,\nwhether headlines or whole article. For this decision, the chosen\nclassification method was also going to be important. For example,\nKhanehzar et al. [33] used traditional approaches such as SVMs as\nbaseline, and demonstrated the improvement in frame classifica-\ntion with the use of pre-trained languages models such as BERT,\nRoBERTa and XLNet, following a fine-tuning approach, setting\nas input text a maximum of 256 tokens (although the maximum\nnumber of input tokens in these models is 512 tokens.) Liu et al.\n[37] classified news headlines about the gun problem in the United\nStates, arguing for the choice of headlines as a unit of analysis\nbased on previous journalism literature [6], [44], that advocated\nfor the importance and influence of headlines on readers and the\nsubsequent perception of articles. From a computational viewpoint,\nusing headlines is also an advantage, since you avoid the 512 token\nlimitation in BERT-based models. Therefore, we decided to work\nwith headlines about a controversial issue, namely the Covid-19\nno-vax movement.\nContinuing with the question of the methods used for classi-\nfication, much work has been developed in prompt engineering,\nespecially since the release of GPT-3. Liu et al.[36] presented a good\noverview of the work done on this new NLP paradigm, not only\nexplaining the concept of prompt engineering, but also the differ-\nent strategies that can be followed both in the design of prompts,\nTable 1: Summary of deductive approaches for frame analysis\nRef\nFrames\nGoal\nTechnique\nNumber\nof\nsamples\n[12] 15 generic frames: \"Economic\", \"Capac-\nity and resources\", \"Morality\", \"Fair-\nness and equality\", \"Legality, constitu-\ntionality and jurisprudence\", \"Policy\nprescription and evaluation\", \"Crime\nand punishment\", \"Security and de-\nfense\", \"Health and safety\", \"Quality of\nlife\", \"Cultural identity\", \"Public opin-\nion\", \"Political\", \"External regulation\nand reputation\", \"Other\".\nTo label frames of full\narticles\nReading\nthe\nfull\narticle,\nthe annotator\ndefines\nthe\nmain frame\n20000 articles\n[33] 15 generic frames\nClassification\nBERT\nbased\nmodels\n12000 articles\n[52] 5 generic frames: \"human interest\",\n\"conflict\", \"morality\", \"attribution of\nresponsibility\", and \"economic conse-\nquences\".\nTo label frames of full\narticles\nYes/No\nques-\ntions.\n2600\narticles\nand\n1522\ntv\nnews stories\n[37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic\nopinion\u201d,\n\u201cSociety/Culture\u201d,\nand\n\u201cEconomic\nconsequences\u201d\n,\n\u201c2nd\nAmendment\u201d (Gun Rights), \u201cGun\ncontrol/regulation\u201d, \u201cMental health\u201d,\n\u201cSchool/Public space safety\u201d, and\n\u201cRace/Ethnicity\u201d.\nTo label frames of full\narticles/ Classification\nReading\nthe\nfull\narticle,\nthe annotator\ndefines\nthe\nmain\nframe.\nBERT\nbased\nmodels\n2990 headlines\n[22] 5 generic frames + blame frame and\nfear frame\nTo label frames of full\narticles\nYes/No\nques-\ntions.\n1170 articles\n[1]\n5 generic frames\nTo label frames of full\narticles\nReading\nthe\nfull\narticle,\nthe annotator\ndefines\nthe\nmain frame.\n6713 articles\n[50] 5 generic frames + pandemic frames\nTo label frames of full\narticles\nYes/No\nques-\ntions.\n2742 articles\n[14] 5 generic frames, journalistic role and\npandemic frames\nTo label frames of full\narticles\nReading\nthe\nheadline\nand\nsubheadline,\nthe annotator\ndefines\nthe\nmain frame.\n131 headlines +\nsubheadlines\nthe potential applications, and the challenges to face when using\nthis approach. Prompt engineering applications include knowledge\nprobing [46], information extraction [53], NLP reasoning [57], ques-\ntion answering [32], text generation [20], multi-modal learning [58],\nand text classification [24], the latter being the prompt-engineering\nuse case in our work. Puri et al.[45] presented a very interesting\nidea that we apply to our classification task. This consists of pro-\nviding the language model with natural language descriptions of\nclassification tasks as input, and training it to generate the correct\nanswer in natural language via a language modeling objective. It is\na zero-shot learning approach, in which no examples are used to\nexplain the task to the model. Radford et al. [48] demonstrated that\nlanguage models can learn tasks without any explicit supervision.\nWe have followed this approach to find an alternative way to do\nframe analysis.\nAs mentioned before, the emergence of giant models like GPT-3,\nBLOOM, and ChatGPT are a very active research topic. To the best\nof our knowledge, on one hand our work extends the computational\nanalysis of news related to the covid-19 no-vax movement, which\nillustrates the influence of the press on the ways societies think\nabout relevant issues [40], [59], and on the other hand it adds to\nthe literature of human-machine interaction, regarding the design\nof GPT-3 prompts for classification tasks [39], [2].\n3\nDATA: EUROPEAN COVID-19 NEWS\nDATASET\nWe used part of the European Covid-19 News dataset collected in\nour recent work [3]. This dataset contains 51320 articles on Covid-\n19 vaccination from 19 newspapers from 5 different countries: Italy,\n629\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nDavid Alonso del Barrio and Daniel Gatica-Perez\nFrance, Spain, Switzerland and UK. The articles cover a time period\nof 22 months, from January 2020 to October 2021. All content was\ntranslated into English to be able to work in a common language.\nThe dataset was used for various analyses, such as name entity\nrecognition, sentiment analysis, and subtopic modeling, to under-\nstand how Covid-19 vaccination was reported in Europe through\nthe print media (in digital format.) The subtopic modeling analysis\nrevealed a subsample of articles on the no-vax movement, which is\nthe one we have used in this paper. We took the headlines of the\narticles associated with the no-vax movement, selecting all articles\ncontaining any of the keywords in Table 2 in the headline or in the\nmain text. This corresponds to a total of 1786 headlines.\nTable 2: Keywords used to identify no-vax articles\nKeywords\nNO VAX TOPIC\n\"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"\nIn Table 3, we show the number of headlines per country and\nnewspaper. France is the country with the most no-vax articles in\nthe corpus, with 523 articles, followed by Italy with 508. However,\nnote that there are 6 newspapers from France, while only 2 from\nItaly. Corriere della Sera is the newspaper that dealt most frequently\nwith the subject (429 articles), while The Telegraph is the second\none (206 articles). The total number of articles normalized by the\nnumber of newspapers per country is also shown in the last column\nof the Table. Using these normalized values, the ranking is Italy,\nUK, France, Switzerland, and Spain.\nTable 3: Number of headlines by newspaper and country\nCOUNTRY\nNEWSPAPER\nHEADLINES\nTOTAL (NORM. TOTAL)\nFRANCE\nLa Croix\n94\n523 (87.1)\nLe Monde\n125\nLes Echos\n49\nLiberation\n97\nLyon Capitale\n8\nOuest France\n150\nITALY\nCorriere della Sera\n429\n508 (254.0)\nIl Sole 24 Ore\n79\nSPAIN\n20 minutos\n27\n303 (50.5)\nABC\n50\nEl Diario\n32\nEl Mundo\n77\nEl Espa\u00f1ol\n22\nLa Vanguardia\n95\nSWITZERLAND\n24 heures\n97\n230 (76.6)\nLa Libert\u00e9\n22\nLe Temps\n111\nUNITED KINGDOM\nThe Irish News\n16\n222 (111.0)\nThe Telegraph\n206\n1786\n4\nMETHODOLOGY\n4.1\nHuman labeling of news frames\nTo carry out the labeling of the frames in our corpus of headlines, we\nfirst designed a codebook, which contained the definitions of each\nof the frame types and a couple of examples of each type, as well\nas a definition of the corpus subject matter and definitions of the\nconcept of frame analysis, so that the annotators could understand\nthe task to be performed. The codebook follows the proposed by\n[52] with 5 generic frames (attribution of responsibility, human\ninterest, conflict, morality, and economic consequences) plus one\nadditional \u2019no-frame\u2019 category. Two researchers were engaged to\nannotate a sample of the collected newspaper articles following a\nthree-phase training procedure.\nIn the first phase, annotators had to read the codebook and get\nfamiliar with the task. In the second phase, they were asked to\nidentify the main frame in the same subset of 50 headlines. At the\nend of the second phase, the intercoder reliability (ICR) was 0.58\nbetween the 2 annotators. We analyzed those cases where there\nwere discrepancies, and observed that in some cases, there was not a\nunique main frame, because both annotators had valid arguments to\nselect one of the frames. In other cases, the discrepancies were due\nto slight misunderstanding of the definitions. In the third phase, the\nannotators coded again 50 headlines, and the ICR increased to was\n0.66. We realized that the possibility of having two frames remained.\nThey discussed the cases in which they had disagreed, and if the\nother person\u2019s arguments were considered valid, it could be said that\nthere were two frames. After this three-phase training procedure,\nannotators were ready to annotate the dataset independently. We\ndivided the dataset into two equal parts, and each person annotated\n893 headlines.\n4.2\nFine-tuning GPT-3.5 and BERT-based\nmodels\nWith the annotated dataset, we investigated two NLP approaches:\nthe first one involves fine-tuning a pre-trained model; the second\none is prompt engineering. Pre-trained language models have been\nFigure 1: Pre-train, fine-tune, prompt\ntrained with large text strings based on two unsupervised tasks,\nnext sentence prediction and masked language model. Figure 1\nsummarizes these techniques.\nIn the first approach, a model with a fixed architecture is pre-\ntrained as a language model (LM), predicting the likelihood of the\nobserved textual data. This can be done due to the availability of\nlarge, raw text data needed to train LMs. This learning process can\nproduce general purpose features of the modeled language. The\nlearning process produces robust, general-purpose features of the\nlanguage being modeled. The above pre-trained LM is then adapted\nto different downstream tasks, by introducing additional parameters\nand adjusting them using task-specific objective functions. In this\napproach, the focus was primarily on goal engineering, designing\nthe training targets used in both the pre-training and the fine-tuning\nstages [36].\n630\nFraming the News:\nFrom Human Perception to Large Language Model Inferences\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nWe present an example to illustrate the idea. Imagine that the\ntask is sentiment analysis, and we have a dataset with sentences\nand their associated sentiment, and a pre-trained model, which is a\nsaved neural network trained with a much larger dataset. For that\npre-trained model to address the target task, we unfreeze a few of\nthe top layers of the saved model base and jointly train both the\nnewly-added classifier layers and the last layers of the base model.\nThis allows to \"fine-tune\" the higher-order feature representations\nin the base model to make them more relevant for the sentiment\nanalysis task. In this way, instead of having to obtain a very large\ndataset with target labels to train a model, we can reuse the pre-\ntrained model and use a much smaller train dataset. We use a part\nof our dataset as examples for the model to learn the task, while\nthe other part of the dataset is used to evaluate model performance.\nPrevious works related to frame classification in the computing\nliterature have used fine-tuning, BERT-based models. In our work,\nwe have done the same as a baseline, but we aimed to go one step\nfurther and also produce results using fine-tuning of GPT-3.5.\n4.3\nPrompt-engineering with GPT-3.5\nModel fine-tuning has been widely used, but with the emergence\nof generative models such as GPT-3, another way to approach\nclassification tasks has appeared. The idea is to use the pre-trained\nmodel directly and convert the task to be performed into a format\nas close as possible to the tasks for which it has been pre-trained.\nThat is, if the model has been pre-trained from next word prediction\nas in the case of GPT-3, classification can be done by defining a\nprompt, where the input to the model is an incomplete sentence,\nand the model must complete it with a word or several words, just\nas it has been trained. This avoids having to use part of the already\nlabeled dataset to teach the task to be performed to the model, and\na previous labeling is not needed [36].\nIn this approach, instead of adapting pre-trained LMs to down-\nstream tasks via objective engineering, downstream tasks are re-\nformulated to look more like those solved during the original LM\ntraining with the help of a textual prompt. For example, when recog-\nnizing the emotion of a social media post, \u201cI missed the bus today.\u201d,\nwe may continue with a prompt \u201cI felt so _\u201d, and ask the LM to\nfill the blank with an emotion-bearing word. Or if we choose the\nprompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may\nbe able to fill in the blank with a French translation. In this way,\nby selecting the appropriate prompts, we can influence the model\nbehavior so that the pre-trained LM itself can be used to predict the\ndesired output, even without any additional task-specific training\n[36].\nWe use this emerging NLP approach to classify frames at headline\nlevel. We are not aware of previous uses of this strategy to classify\nframes as we propose here. The idea is the following. Prompt engi-\nneering consists of giving a prompt to the model, and understands\nthat prompt as an incomplete sentence. To do prompt engineer-\ning with our dataset, we needed to define an appropriate prompt\nthat would produce the headline frames as output. We defined sev-\neral experiments with the Playground of GPT-3, in order to find\nthe best prompt for our task. In our initial experiments, we fol-\nlowed existing approaches in prompt engineering to do sentiment\nanalysis, where the individual answer was an adjective, and this\nadjective was matched with a sentiment. In a similar fashion, we\ndecided to build a thesaurus of adjectives that define each of the\nframes. For instance, the human interest frame could be \u2019interest-\ning\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be:\n\u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou-\nblemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined,\nwe needed to define the prompt in order to get, as an answer, one\nof the adjectives in our thesaurus to match them with the frame.\nWe used the GPT-3 playground using the headline as input and\nasking for the frame as output, but the strategy did not work. In\nour final experiment, instead of giving the headline as input, we\ngave the definitions of each type of frame plus the headline, and we\nasked the model to choose between the different types of frames\nas output. In this way, the output of the model was directly one of\nthe frames, and we avoided the step of matching adjectives with\nframes. An example is shown in Figure 2.\nFigure 2: GPT-3.5 for frame inference: input and output\nFor the GPT-3 configuration 1, there are 3 main concepts:\n\u2022 TEMPERATURE [0-1]. This parameter controls randomness,\nlowering it results in less random completions.\n\u2022 TOP_P [0-1]. This parameter controls diversity via nucleus\nsampling.\n\u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-\nmum number of tokens to generate,\n\u2022 MODEL. GPT-3 offer four main models with different levels\nof power, suitable for different tasks. Davinci is the most\ncapable model, and Ada is the fastest.\nAfter testing with the GPT-3 playground and varying different\nhyper-parameters to assess performance, we set the temperature to\n0, since the higher the temperature the more random the response.\nFurthermore, the Top-p parameter was set to 1, as it would likely\nget a set of the most likely words for the model to choose from. The\nmaximum number of tokens was set to 2; in this way, the model\nis asked to choose between one of the responses. As a model, we\nused the one with the best performance at the time of experimental\ndesign, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\n5\nRESULTS: HUMAN LABELING OF FRAMES\nIN NO-VAX NEWS HEADLINES (RQ1)\nIn this section, we present and discuss the results of the analysis\nrelated to our first RQ.\nFigure 3 shows the distribution of frames per country at headline\nlevel, with human interest and no-frame being the predominant\n1https://beta.openai.com/docs/introduction\n631\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nDavid Alonso del Barrio and Daniel Gatica-Perez\nones. Attribution of responsibility is the third one except in Switzer-\nland, where the corresponding frame is conflict. Finally, morality\nand economic are the least represented in the dataset for every\ncountry.\nFigure 3: Non-normalized distribution of frames per country\nThe monthly distribution of frames aggregated for all countries\nis shown in Fig. 4. We can see two big peaks, the first one in January\n2021 and the second one in August 2021. In all countries, the vac-\ncination process started at the end of December 2020, so it makes\nsense that the no-vax movement started to be more predominant in\nthe news in January 2021. Human interest is the most predominant\nframe. Manual inspection shows that this is because the headlines\nare about personal cases of people who are pro- or anti- vaccine.\nAttribution of responsibility is also present. Manual inspection in-\ndicates that local politicians and health authorities had to make\ndecisions about who could be vaccinated at the beginning of the\nprocess. The second peak at the end of summer 2021 coincided\nwith the health pass (also called Covid passport in some countries),\nand we can observe a peak in the curve corresponding to the con-\nflict frame, reflecting the demonstrations against the measure of\nmandatory health passes taken by country governments.\nIn Figure 5, we compare the sentiment per frame and per country,\nto understand if there were any major differences. The sentiment\nanalysis labels were obtained using BERT-sent from the Hugging\nFace package [47], used in our previous work (please refer to our\noriginal analysis in [3] for details.) We normalized the results be-\ntween 0 and 1 to compare frames between countries. We see that the\nsentiment is predominantly neutral (in blue). Examining in more\nFigure 4: Non-normalized monthly distribution of frames.\ndetail the negative and positive sentiment of each frame category,\nwe observed a few trends:\n\u2022 Attribution of responsibility: Negative sentiment represents\n30-40% of the cases, while positive tone is only found in\nresidual form in Italy, Switzerland, and the United Kingdom.\n\u2022 Conflict: Negative sentiment represents 20-35% of the cases.\n\u2022 Economic: Predominantly neutral, with only negative tone\nin Italy and UK (in the latter case, all headlines with this\nframe were considered negative.)\n\u2022 Human interest: Negative sentiment represents 30-40% of\nthe cases, while positive tone is only found in residual form\nin Italy, Spain, and Switzerland.\n\u2022 Morality: Predominantly neutral, with negative tone in Italy,\nSwitzerland, and the United Kingdom,\n\u2022 No frame: 20-30% of negative content.\nFigure 5: Sentiment of headline by frame and by country\n632\nFraming the News:\nFrom Human Perception to Large Language Model Inferences\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nRegarding the results of the annotation process, the fact that the\ndistribution of the 6 frame types is relatively similar between coun-\ntries suggests that the anti-vaccine movement issue was treated\nin a similar way in these countries. The fact that human interest\nis the most dominant frame indicates that this issue was treated\nfrom a more human and emotional approach, with headlines about\npersonal experiences, celebrities giving their opinion about vacci-\nnation, and politicians defending vaccine policies. Moreover, the\nreason for many headlines being classified as no-frame is partly\ndue to how data was selected. We chose articles that contained\nwords related to no-vax, either in the headline or in the article. This\nresulted in many headlines not containing anything specific related\nto no-vax, while the no-vax content was actually included in the\nmain text of the corresponding articles.\nIt is worth mentioning that prior to obtaining the results, we had\nexpected that attribution of responsibility would be among the most\nprominent frames, since governments took many measures such as\nmandatory health pass requirements to access certain sites; we had\nalso expected that the conflict frame would be prominent, since\nthere were many demonstrations in Europe. In reality, however,\nthese frames categories were not reflected as frequently at the\nheadline level.\nRegarding the analysis at the temporal level, it is clear that certain\nevents were captured by the press, such as the start of vaccination\nor the mandatory vaccination passport.\nFinally, the sentiment analysis of the different frames shows that\nthe predominant tone in all of them is neutral or negative, with very\nsimilar trends between countries. This association between senti-\nment analysis and frames has been discussed in previous literature\n[11] [43].\n6\nRESULTS: GPT-3.5 FOR FRAME\nCLASSIFICATION OF HEADLINES (RQ2)\nHere, we present and discuss the results related to our second RQ.\n6.1\nFine-tuning GPT-3.5\nTable 4 shows the results of the 6-class classification task using\n5-cross validation. Three models were used: GPT-3.5 and two BERT-\nbased models. We observe that, on average, GPT-3.5 performs better\nthan the BERT-based models. This is somehow expected as GPT-\n3.5 is a much larger model. Overall, in the case of fine-tuning, the\nbest performance for the six-class frame classification task is 72%\naccuracy, which is promising, with an improvement over previous\nmodels based on BERT. Yet, it should be noted that the performance\ndifferences are modest (2% improvement between GPT-3.5 and\nRoBERTa).\nTable 4: Classification results for six-class frame classifica-\ntion and 5-fold cross validation\nACCURACY\n0\n1\n2\n3\n4\nAVERAGE\nBERT\n0.68\n0.69\n0.72\n0.64\n0.70\n0.67\nRoBERTa\n0.70\n0.72\n0.72\n0.67\n0.71\n0.70\nGPT3\n0.75\n0.70\n0.72\n0.71\n0.71\n0.72\nOn the other hand, BERT is open-source, while GPT-3 has an\neconomic cost as the use of the model is not free, which monetarily\nlimits the number of experiments that can be performed with it,\nas well as the different configurations one can explore to improve\nperformance. This is important because much of the improvement\nin performance requires empirical explorations of model parameters\nMore specifically, the cost of an experiment for each of the folds has\na cost of 4 dollars (at the time of writing this paper.) This represents\na limitation in practice.\nFurthermore, GPT-3 has a significant carbon footprint. Similarly,\nfor prompt engineering (discussed in the next subsection), choosing\nthe right prompt (i.e., the words that best define the task so that the\nmodel is able to perform adequately) is also based on trial and error.\nThis also has an impact on carbon footprint. In connection with\nthis topic, Strubell et al.[55] argue that improvements in the accu-\nracy of models depend on the availability of large computational\nresources, which involve large economic and environmental costs.\nA criticism has been made as \u2019the rich get richer\u2019, in the sense that\nnot all research groups have sufficient infrastructure resources and\naccess to funding needed to use these models and improve their\nperformance. Also in relation to this analysis, the work of Bender\net al. [4] evaluates the costs and risks of the use of large language\nmodels, stating that researchers should be aware of the impact that\nthese models have on the environment, and assess whether the\nbenefits outweigh the risks. The work in [4] provides a very telling\nexample, where people living in the Maldives or Sudan are affected\nby floods and pay the environmental price of training English LLMs,\nwhen similar models have not been produced for languages like\nDhivehi or Sudanese Arab. In short, there is a need to establish\nways to use this technological development responsibly, and it all\nstarts with being aware of the risks it presents.\n6.2\nPrompt-engineering with GPT-3.5\nFor each headline, we got the frame that the model considered the\nmost likely, and we compared these GPT-3.5 inferences with the\nframes labeled by the annotators. The agreement between model\nand annotator was of 49%. Analyzing the results, and specifically\nlooking at the cases where the annotator and GPT-3.5 disagreed,\nwe discovered that according to the frame definitions, the model\nin some cases proposed a frame that indeed made sense. This ob-\nservation, together with our previous experience in the annotation\nprocess, where headlines could have more than one valid frame,\nled us to design a second post-hoc experiment. We took all the\nheadlines where each of the two annotators had disagreed with\nGPT-3.5, and we asked the annotators to state whether they would\nagree (or not) with each GPT-inferred label for a given headline.\nIt is important to emphasize that the annotators did not know the\norigin of that label, i.e., they did not know if it was the label they\nhad originally assigned, or if it was a random one. In this way, we\ncould quantify how GPT-3.5 worked according to valid arguments\nprovided by the annotators. In this post-hoc experiment, the model\nagreed in 76% of cases with the annotators.\nLooking at the results of the classification models, the 49% accu-\nracy of the prompt-engineering approach can be considered low,\nyet we consider that it is a valid avenue for further investigation,\nas in the second post-hoc analysis, we found that the model agrees\n633\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nDavid Alonso del Barrio and Daniel Gatica-Perez\nwith human annotators in 76% of the cases. Clearly, framing in-\nvolves aspects of subjectivity [42]. Much of what we do as people\nhas a subjective component, influenced by how we feel or how we\nexpress opinions.\nNews reading is never fully objective, and the annotators en-\ngaged in the frame classification task, influenced by their personal\nstate of mind, experience, and culture, may perceive information\ndifferently. Monarch affirms that \"for simple tasks, like binary labels\non objective tasks, the statistics are fairly straightforward to decide\nwhich is the \u2018correct\u2019 label when different annotators disagree. But\nfor subjective tasks, or even objective tasks with continuous data,\nthere are no simple heuristics for deciding what the correct label\nshould be\" [42].\nSubjectivity is involved in both the generation and perception\nof information: the assumption that there is only one frame is com-\nplicated by the point of view of the reader. In the case of news, the\ninformation sender (the journalist) has an intention, but the receiver\n(the reader) plays a role and is influenced by it. In psychology, this\nis known as the lens model of interpersonal communication, where\nthe sender has certain objectives, but the receiver can interpret\nor re-interpret what the sender wants to say, with more or less\naccuracy [26].\nFollowing this discussion on subjectivity, the question arose as to\nwhat would happen if, instead of headlines, we used the complete\narticle as a source of analysis. We wondered if longer text could\nmake the frame labeling task clearer than when using headlines.\nYet another possible hypothesis is that having to read longer texts\ncould lead to the same subject being presented from different angles.\nPlease recall that in the existing literature discussed in Section 2,\nboth headlines and full articles have been used from frame analysis\n(see Table 1.) This remains as an issue for future work.\n7\nCONCLUSIONS\nIn this paper, we first presented an analysis of human-generated\nnews frames on the covid-19 no-vax movement in Europe, and\nthen studied different approaches using large language models for\nautomatic inference of frames. We conclude by answering the two\nresearch questions we posed:\nRQ1: What are the main frames in the news headlines about the\ncovid-19 anti-vaccine movement in 5 European countries? After\nannotating the headlines, we found that of the 1786 headlines,\nthe predominant frame is human interest (45.3% of cases), which\npresents a news item with an emotional angle, putting a face to a\nproblem or situation. We also found that a substantial proportion\nof headlines were annotated as not presenting any frame (40.2% of\ncases). Finally, the other frame types are found more infrequently.\nRQ2: Can prompt engineering be used for classification of head-\nlines according to frames? We first used fine-tuning of a number of\nlanguage models, and found that GPT-3.5 produced classification ac-\ncuracy of 72% on a six-frame classification task. This represented a\nmodest 2% improvement over BERT-based models, at a significantly\nlarger environmental cost. We then presented a new way of classi-\nfying frames using prompts. At the headline level, inferences made\nwith GPT-3.5 reached 49% of agreement with human-generated\nframe labels. In many cases, the GPT-3.5 model inferred frame\ntypes that were considered as valid choices by human annotators,\nand in an post-doc experiment, the human-machine agreement\nreached 76%. These results have opened several new directions for\nfuture work.\nACKNOWLEDGMENTS\nThis work was supported by the AI4Media project, funded by the\nEuropean Commission (Grant 951911) under the H2020 Programme\nICT-48-2020. We also thank the newspapers for sharing their online\narticles. Finally, we thank our colleagues Haeeun Kim and Emma\nBouton-Bessac for their support with annotations, and Victor Bros\nand Oleksii Polegkyi for discussions.\nREFERENCES\n[1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online\nmedia: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.\nJurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.\n[2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess\nRiedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021.\nRAFT: A real-world few-shot text classification benchmark.\narXiv preprint\narXiv:2109.14076 (2021).\n[3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press\nCover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35\u201343.\nhttps://doi.org/10.1145/3512732.3533588\n[4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\nBe Too Big? (2021), 610\u2013623.\n[5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in\njournalism: A boon or bane? In Optimization in machine learning and applications.\nSpringer, 155\u2013167.\n[6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015.\nMedia portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013\n2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.\n[7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.\nhttps://doi.org/10.48550/ARXIV.2212.14402\n[8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe,\nMichel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour-\nnalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695.\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H\nDe Vreese. 2014. Teaching the computer to code frames in news: Comparing\ntwo supervised machine learning approaches to frame analysis. Communication\nMethods and Measures 8, 3 (2014), 190\u2013206.\n[11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond\nwords: Applying cluster and sentiment analysis to news coverage of the nuclear\npower issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.\n[12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015.\nThe Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015),\n438\u2013444. https://doi.org/10.3115/v1/P15-2072\n[13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age\nof coronavirus and fake news: analysis of journalistic sources in the Spanish\nquality press. International Journal of Environmental Research and Public Health\n17, 21 (2020), 8136.\n[14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust\nof vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n\nsocial 74 (2019), 786\u2013802.\n[15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology\nfor evaluating data journalism, computational journalism, and computer-assisted\nreporting. Digital journalism 3, 3 (2015), 331\u2013348.\n[16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News\nFraming Analysis. Routledge, 151\u2013172.\n[17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,\n1 (2021), 113\u2013118.\n[18] Astrid Dirikx and Dave Gelders. 2010.\nTo frame is to explain: A de-\nductive frame-analysis of Dutch and French climate change coverage dur-\ning the annual UN Conferences of the Parties.\nPublic Understanding of\nScience 19, 6 (2010), 732\u2013742.\nhttps://doi.org/10.1177/0963662509352044\narXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.\n[19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame-\nanalysis of Dutch and French climate change coverage during the annual UN\nConferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.\n634\nFraming the News:\nFrom Human Perception to Large Language Model Inferences\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig.\n2020. Gsum: A general framework for guided neural abstractive summarization.\narXiv preprint arXiv:2010.08014 (2020).\n[21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news\nheadlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA\nGesondheid (Online) 27 (2022), 1\u20138.\n[22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative\nAnalysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).\n[23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.\nMcQuail\u2019s reader in mass communication theory 390 (1993), 397.\n[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language\nmodels better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).\n[25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across\nfour nations: a topic modeling and sentiment analysis approach. Ieee Access 9\n(2021), 36645\u201336656.\n[26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En-\ncoding and Decoding of Interpersonal Dispositions in Nonverbal Behavior.\nJournal of Personality and Social Psychology 66 (02 1994), 398\u2013412.\nhttps:\n//doi.org/10.1037//0022-3514.66.2.398\n[27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based\ntransformer architectures for long document summarization. In Proceedings of\nthe 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume. 1792\u20131810.\n[28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news\nsummarization using transformers. In Sustainable Advanced Computing. Springer,\n249\u2013259.\n[29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar-\nchy of hybridity in data and computational journalism. Digital Journalism 5, 2\n(2017), 159\u2013176.\n[30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and\ntext analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021),\n300\u2013324.\n[31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative\nanalysis of large amounts of journalistic texts using topic modelling. Digital\njournalism 4, 1 (2016), 89\u2013106.\n[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we\nknow what language models know? Transactions of the Association for Computa-\ntional Linguistics 8 (2020), 423\u2013438.\n[33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling\nPolitical Framing Across Policy Issues and Contexts. In ALTA.\n[34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate\nduring election years: Focus on generic frames. The Communication Review 21, 2\n(2018), 89\u2013115.\n[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n(2022).\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. (2021). https://doi.org/10.\n48550/ARXIV.2107.13586\n[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019.\nDetecting frames in news headlines and its application to analyzing news framing\ntrends surrounding US gun violence. In Proceedings of the 23rd conference on\ncomputational natural language learning (CoNLL).\n[38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames:\nToward Improving Reliability and Validity. Journal of Communication 58 (06\n2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x\n[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and\nDavid E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3\nUser Simulation in Conversational AI. In Proceedings of the 4th Conference on\nConversational User Interfaces. 1\u20136.\n[40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social\ncomputing for verifying social media content in breaking news. IEEE Internet\nComputing 22, 2 (2018), 83\u201389.\n[41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021. \u2018Our task is to demystify fears\u2019:\nAnalysing newsroom management of automation in journalism. Journalism 22,\n9 (2021), 2203\u20132221.\n[42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and\nAnnotation for Human-centered AI. Manning. https://books.google.ch/books?\nid=LCh0zQEACAAJ\n[43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of\nmedia frames: Strengths, weaknesses, and opportunities. Political Communication\n38, 1-2 (2021), 159\u2013181.\n[44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to\nnews discourse. Political communication 10, 1 (1993), 55\u201375.\n[45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative\nlanguage models. arXiv preprint arXiv:1912.10165 (2019).\n[46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with\nmixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).\n[47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent.\nhttps://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.\n[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022.\nFake News Classification using transformer based enhanced LSTM and BERT.\nInternational Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105.\nhttps://doi.org/10.1016/j.ijcce.2022.03.003\n[50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational\npredictors. Cuadernos. info 50 (2021), 91\u2013112.\n[51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\nHesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\nGall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language\nmodel. arXiv preprint arXiv:2211.05100 (2022).\n[52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content\nAnalysis of Press and Television News. Journal of Communication 50 (06 2000),\n93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x\n[53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em-\nmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models yield few-shot semantic parsers.\narXiv preprint arXiv:2104.08768 (2021).\n[54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab-\norative Work trends on Media Organizations: Mixing Qualitative and Quan-\ntitative Approaches.\nStudies in Media and Communication 5 (04 2017), 63.\nhttps://doi.org/10.11114/smc.v5i1.2279\n[55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy\nconsiderations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n[56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\nthe capabilities, limitations, and societal impact of large language models. arXiv\npreprint arXiv:2102.02503 (2021).\n[57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.\narXiv preprint arXiv:1806.02847 (2018).\n[58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and\nFelix Hill. 2021. Multimodal few-shot learning with frozen language models.\nAdvances in Neural Information Processing Systems 34 (2021), 200\u2013212.\n[59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of\ntechnology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.\n[60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling\nfor frame analysis: A study of media debates on climate change in India and USA.\nGlobal Media and Communication 18, 1 (2022), 91\u2013112.\n635\n",
    "pdf_url": "",
    "references": [
      "[1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online",
      "media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.",
      "Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.",
      "[2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess",
      "Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021.",
      "RAFT: A real-world few-shot text classification benchmark.",
      "arXiv preprint",
      "arXiv:2109.14076 (2021).",
      "[3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press"
    ],
    "publication_date": "07-11-2023"
  },
  {
    "titre": "Generating Diverse Code Explanationsusing the GPT-3 Large Language Model",
    "resume": "Good explanations are essential to efficiently learning introductoryprogramming concepts [10]. To provide high-quality explanationsat scale, numerous systems automate the process by tracing theexecution of code [8, 12], defining terms [9], giving hints [16],and providing error-specific feedback [10, 16]. However, these ap-proaches often require manual effort to configure and only explaina single aspect of a given code segment. Large language models(LL Ms) are also changing how students interact with code [7]. Forexample, Githubs Copilot can generate code for programmers [4],leading researchers to raise concerns about cheating [7]. Instead,our work focuses on LLMs potential to support learning by explain-ing numerous aspects of a given code snippet. This poster featuresa systematic analysis of the diverse natural language explanationsthat GPT-3 can generate automatically for a given code snippet. Wepresent a subset of three use cases from our evolving design spaceof AI Explanations of Code.",
    "auteurs": [
      "Stephen MacNeil",
      "Dan Mogil",
      "Seth Bernstein",
      "Erin Ross",
      "Ziheng Huang",
      "Stephen MacNeil",
      "Andrew Tran",
      "Dan Mogil",
      "Seth Bernstein",
      "Erin Ross",
      "Ziheng Huang",
      "Lugano",
      "Lugano"
    ],
    "institutions": [
      "Stephen Mac  UniversityPhiladelphia, PA, USA",
      "Andrew  Temple UniversityPhiladelphia, PA, USA",
      "Dan  Temple UniversityPhiladelphia, PA, USA",
      "Ziheng  University of California\u2014San DiegoLa Jolla, CA, USA",
      "Seth  Temple UniversityPhiladelphia, PA, USA",
      "Erin  Temple UniversityPhiladelphia, PA, USA"
    ],
    "mots_cles": [
      "large language models",
      " natural language processing",
      " code explana-tions",
      " computer science education"
    ],
    "texte_integral": "Generating Diverse Code Explanations\nusing the GPT-3 Large Language Model\nStephen MacNeil\nstephen.macneil@temple.edu\nTemple University\nPhiladelphia, PA, USA\nAndrew Tran\nandrew.tran10@temple.edu\nTemple University\nPhiladelphia, PA, USA\nDan Mogil\ndaniel.mogil@temple.edu\nTemple University\nPhiladelphia, PA, USA\nSeth Bernstein\nseth.bernstein@temple.edu\nTemple University\nPhiladelphia, PA, USA\nErin Ross\nerinross@temple.edu\nTemple University\nPhiladelphia, PA, USA\nZiheng Huang\nz8huang@ucsd.edu\nUniversity of California\u2014San Diego\nLa Jolla, CA, USA\nKEYWORDS\nlarge language models, natural language processing, code explana-\ntions, computer science education\nACM Reference Format:\nStephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross,\nand Ziheng Huang. 2022. Generating Diverse Code Explanations using the\nGPT-3 Large Language Model. In Proceedings of the 2022 ACM Conference\non International Computing Education Research V.2 (ICER 2022), August 7\u201311,\n2022, Lugano and Virtual Event, Switzerland. ACM, New York, NY, USA,\n3 pages. https://doi.org/10.1145/3501709.3544280\n1\nABSTRACT\nGood explanations are essential to efficiently learning introductory\nprogramming concepts [10]. To provide high-quality explanations\nat scale, numerous systems automate the process by tracing the\nexecution of code [8, 12], defining terms [9], giving hints [16],\nand providing error-specific feedback [10, 16]. However, these ap-\nproaches often require manual effort to configure and only explain\na single aspect of a given code segment. Large language models\n(LLMs) are also changing how students interact with code [7]. For\nexample, Github\u2019s Copilot can generate code for programmers [4],\nleading researchers to raise concerns about cheating [7]. Instead,\nour work focuses on LLMs\u2019 potential to support learning by explain-\ning numerous aspects of a given code snippet. This poster features\na systematic analysis of the diverse natural language explanations\nthat GPT-3 can generate automatically for a given code snippet. We\npresent a subset of three use cases from our evolving design space\nof AI Explanations of Code.\n2\nUSE CASES\nTo understand the types of explanations GPT-3 [2] can generate,\nwe issued over 700 prompts across numerous code snippets. An\nexample prompt and resulting explanation is shown in Figure 1.\nWe discovered eight explanation types and Figure 2 includes three\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland\n\u00a9 2022 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-9195-5/22/08.\nhttps://doi.org/10.1145/3501709.3544280\nexplanation types to illustrate the explanatory power of GPT-3. The\nadditional types include: 1) tracing the execution of code, 2) fixing\nbugs and explaining how they were fixed, 3) generating analogies\nto real world settings, 4) listing relevant programming concepts,\nand 5) predicting the console output.\nFigure 1: A prompt and explanation based on analogy.\n2.1\nAnalyzing and explaining time complexity\nInstructors rate time complexity as the most difficult programming\ntopic [17]. However, understanding time complexity is important [6,\n13] because it facilitates decision-making so students choose an\nappropriate algorithm for a given problem. This use case shows\nGPT-3 can identify and explain time complexity.\n2.2\nIdentifying common mistakes made by\nbeginner programmers\nCommonality exists in how students solve programming prob-\nlems [15] and the mistakes they make [1, 11]. Pedagogical tech-\nniques, such as the \u2018muddiest point\u2019 highlight these common and\nmost confusing concepts [3, 14]. GPT-3 can automatically create\na checklist of common mistakes students might make regarding a\ngiven code snippet.\n37\nICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland\nMacNeil et al.\nFigure 2: Three example explanations automatically generated by GPT-3 for an \u2018anonymized\u2019 Binary Search code snippet.\n2.3\nSummarizing code at multiple levels of\nabstraction\nBefore understanding how a code snippet executes, it is often useful\nto understand the purpose of the code [5]. The summary gener-\nated by GPT-3 and shown in Figure 2 defines the goal, traces the\nexecution, and highlights relevant CS concepts such as arrays.\n3\nDISCUSSION\nOur three use cases demonstrate the potential for GPT-3 to explain\ncode for intro CS students. Our poster presentation will feature all\neight explanation types as a design space of explanations to convey\nthe diversity of explanations that can be generated by LLMs. We will\nhighlight best practices for generating effective explanations and\npitfalls that lead to less effective explanations. We are evaluating\nthe usefulness of these explanations in a series of summer classes.\nREFERENCES\n[1] Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating\nnovice programming mistakes in large-scale student data. In Proceedings of the\n46th ACM Technical Symposium on Computer Science Education. 522\u2013527.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in Neural\nInformation Processing Systems 33 (2020), 1877\u20131901.\n[3] Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013.\n\u201cUnmuddying\u201d course content using muddiest point reflections. In 2013 IEEE\nFrontiers in Education Conference (FIE). IEEE, 937\u2013942.\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).\n[5] Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor O\u2019Rourke. 2022. Bring-\ning \"High-Level\" Down to Earth: Gaining Clarity in Conversational Program-\nmer Learning Goals. In Proceedings of the 53rd ACM Technical Symposium\non Computer Science Education V. 1 (Providence, RI, USA) (SIGCSE 2022). As-\nsociation for Computing Machinery, New York, NY, USA, 551\u2013557.\nhttps:\n//doi.org/10.1145/3478431.3499370\n[6] Elvina Elvina and Oscar Karnalim. 2017. Complexitor: An educational tool for\nlearning algorithm time complexity in practical manner. ComTech: Computer,\nMathematics and Engineering Applications 8, 1 (2017), 21\u201327.\n[7] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope-\nnAI Codex on Introductory Programming. In Australasian Computing Education\nConference (Virtual Event, Australia) (ACE \u201922). ACM, New York, NY, USA, 10\u201319.\nhttps://doi.org/10.1145/3511861.3511863\n[8] Philip J Guo. 2013. Online python tutor: embeddable web-based program visual-\nization for cs education. In Proceeding of the 44th ACM technical symposium on\nComputer science education. 579\u2013584.\n[9] Andrew Head, Codanda Appachu, Marti A Hearst, and Bj\u00f6rn Hartmann. 2015.\nTutorons: Generating context-relevant, on-demand explanations and demonstra-\ntions of online code. In 2015 IEEE Symposium on Visual Languages and Human-\nCentric Computing (VL/HCC). IEEE, 3\u201312.\n[10] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020.\nAdaptive Immediate Feedback Can Improve Novice Programming Engagement\nand Intention to Persist in Computer Science. In Proceedings of the 2020 ACM\nConference on International Computing Education Research (Virtual Event, New\nZealand) (ICER \u201920). Association for Computing Machinery, New York, NY, USA,\n194\u2013203. https://doi.org/10.1145/3372782.3406264\n[11] Davin McCall and Michael K\u00f6lling. 2014. Meaningful categorisation of novice pro-\ngrammer errors. In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings.\nIEEE, 1\u20138.\n[12] Greg L Nelson, Benjamin Xie, and Amy J Ko. 2017. Comprehension first: eval-\nuating a novel pedagogy and tutoring system for program tracing in CS1. In\nProceedings of the 2017 ACM conference on international computing education\nresearch. 2\u201311.\n[13] Miranda Parker and Colleen Lewis. 2014. What makes big-O analysis difficult:\nunderstanding how students understand runtime analysis. Journal of Computing\nSciences in Colleges 29, 4 (2014), 164\u2013174.\n[14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany Vinci-Cannava, Laird\nKramer, and Maria Charters. 2020. WIP: An exploration into the muddiest points\n38\nGenerating Diverse Explanations with Large Language Models\nICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland\nand self-efficacy of students in introductory computer science courses. In 2020\nIEEE Frontiers in Education Conference (FIE). IEEE, 1\u20135.\n[15] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au-\ntonomously generating hints by inferring problem solving policies. In Proceedings\nof the second (2015) acm conference on learning@ scale. 195\u2013204.\n[16] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards\nintelligent tutoring in novice programming environments. In Proceedings of the\n2017 ACM SIGCSE Technical Symposium on computer science education. 483\u2013488.\n[17] Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory\nprogramming?. In Proceedings of the second international workshop on Computing\neducation research. 17\u201328.\n39\n",
    "pdf_url": "",
    "references": [
      "[1] Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating",
      "novice programming mistakes in large-scale student data. In Proceedings of the",
      "46th ACM Technical Symposium on Computer Science Education. 522\u2013527.",
      "[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,",
      "Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda",
      "Askell, et al. 2020. Language models are few-shot learners. Advances in Neural",
      "Information Processing Systems 33 (2020), 1877\u20131901.",
      "[3] Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013."
    ],
    "publication_date": "07-11-2023"
  },
  {
    "titre": "The Programmers Assistant: Conversational Interaction with a Large Language Model for Software Development",
    "resume": "Large language models (LLMs) have recently been applied in soft-ware engineering to perform tasks such as translating code betweenprogramming languages, generating code from natural language,and autocompleting code as it is being written. When used withindevelopment tools, these systems typically treat each model in-vocation independently from all previous invocations, and only aspecific limited functionality is exposed within the user interface.This approach to user interaction misses an opportunity for usersto more deeply engage with the model by having the context oftheir previous interactions, as well as the context of their code,inform the models responses. We developed a prototype system the Programmers Assistant in order to explore the utility ofconversational interactions grounded in code, as well as softwareengineers receptiveness to the idea of conversing with, rather thaninvoking, a code-fluent LLM. Through an evaluation with 42 partici-pants with varied levels of programming experience, we found thatour system was capable of conducting extended, multi-turn discus-sions, and that it enabled additional knowledge and capabilitiesbeyond code generation to emerge from the LLM. Despite skepti-cal initial expectations for conversational programming assistance,participants were impressed by the breadth of the assistants capa-bilities, the quality of its responses, and its potential for improvingtheir productivity. Our work demonstrates the unique potential ofconversational interactions with LLMs for co-creative processeslike software development.",
    "auteurs": [
      "Steven I. Ross",
      "Fernando Martinez",
      "La Plata",
      "Buenos Aires",
      "Stephanie Houde",
      "Michael Muller",
      "Steven I. Ross",
      "Fernando Martinez",
      "Stephanie Houde",
      "Michael Muller",
      "https",
      "Rich",
      "Talamadupula"
    ],
    "institutions": [
      "2.2 Code-fluent Foundation Models andHuman-Centered Evaluations ofProgramming Assistance",
      "Justin D. Weisz IBM Research AIYorktown Heights, NY, ",
      "Michael Muller IBM Research AICambridge, MA, ",
      "Stephanie Houde IBM Research AICambridge, MA, ",
      "Steven I. Ross IBM Research AICambridge, MA, "
    ],
    "mots_cles": [
      "code-fluent large language models",
      " foundation models",
      " conversa-tional interaction",
      " human-centered AI"
    ],
    "texte_integral": "The Programmer\u2019s Assistant: Conversational Interaction with a\nLarge Language Model for Software Development\nSteven I. Ross\nIBM Research AI\nCambridge, MA, USA\nsteven_ross@us.ibm.com\nFernando Martinez\nIBM Argentina\nLa Plata, Buenos Aires, Argentina\nmartferc@ar.ibm.com\nStephanie Houde\nIBM Research AI\nCambridge, MA, USA\nStephanie.Houde@ibm.com\nMichael Muller\nIBM Research AI\nCambridge, MA, USA\nmichael_muller@us.ibm.com\nJustin D. Weisz\nIBM Research AI\nYorktown Heights, NY, USA\njweisz@us.ibm.com\nABSTRACT\nLarge language models (LLMs) have recently been applied in soft-\nware engineering to perform tasks such as translating code between\nprogramming languages, generating code from natural language,\nand autocompleting code as it is being written. When used within\ndevelopment tools, these systems typically treat each model in-\nvocation independently from all previous invocations, and only a\nspecific limited functionality is exposed within the user interface.\nThis approach to user interaction misses an opportunity for users\nto more deeply engage with the model by having the context of\ntheir previous interactions, as well as the context of their code,\ninform the model\u2019s responses. We developed a prototype system\n\u2013 the Programmer\u2019s Assistant \u2013 in order to explore the utility of\nconversational interactions grounded in code, as well as software\nengineers\u2019 receptiveness to the idea of conversing with, rather than\ninvoking, a code-fluent LLM. Through an evaluation with 42 partici-\npants with varied levels of programming experience, we found that\nour system was capable of conducting extended, multi-turn discus-\nsions, and that it enabled additional knowledge and capabilities\nbeyond code generation to emerge from the LLM. Despite skepti-\ncal initial expectations for conversational programming assistance,\nparticipants were impressed by the breadth of the assistant\u2019s capa-\nbilities, the quality of its responses, and its potential for improving\ntheir productivity. Our work demonstrates the unique potential of\nconversational interactions with LLMs for co-creative processes\nlike software development.\nCCS CONCEPTS\n\u2022 Human-centered computing \u2192 HCI theory, concepts and\nmodels; \u2022 Software and its engineering \u2192 Designing software;\n\u2022 Computing methodologies \u2192 Generative and developmental\napproaches.\nThis work is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivs International 4.0 License.\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0106-1/23/03.\nhttps://doi.org/10.1145/3581641.3584037\nKEYWORDS\ncode-fluent large language models, foundation models, conversa-\ntional interaction, human-centered AI\nACM Reference Format:\nSteven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin\nD. Weisz. 2023. The Programmer\u2019s Assistant: Conversational Interaction\nwith a Large Language Model for Software Development. In 28th Inter-\nnational Conference on Intelligent User Interfaces (IUI \u201923), March 27\u201331,\n2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 24 pages. https:\n//doi.org/10.1145/3581641.3584037\n1\nINTRODUCTION\nSoftware development is a highly skilled task that requires knowl-\nedge, focus, and creativity [27, 28]. Many techniques have been\ndeveloped to enhance the productivity of software engineers, such\nas advanced code repositories [86], knowledge repositories [39],\nQ&A sites [1], and pair programming practices [18]. Collaborative\nsoftware engineering is especially promising, given that profes-\nsional software development is rarely a solo activity and relevant\nknowledge and expertise are typically distributed widely within an\norganization [68]. Many efforts have focused on incorporating col-\nlaborative technologies into software development environments\n(e.g. [8, 25, 26, 58, 101]).\nThe pioneering work of Rich and Waters on The Programmer\u2019s\nApprentice [70] presented a novel concept of a knowledgeable auto-\nmated assistant \u2013 in effect, an artificial collaborative partner \u2013 that\ncould help software engineers with writing code, designing soft-\nware systems, and creating requirements specifications. At the time,\nAI technologies and computing resources were not sufficient to\nfully implement their vision. In the intervening years, an increase in\ncomputational power, the availability of large corpora of language\nand code data, and the development of deep neural networks have\nmade new approaches to achieving their goals worth exploring.\nRecently, models leveraging the transformer architecture [96]\nhave been developed to perform domain-specific software engi-\nneering tasks, such as translating code between languages [75],\ngenerating documentation for code [36, 38, 97, 98], and generating\nunit tests for code [92] (see Talamadupula [90] and Allamanis et al.\n[5] for surveys). Recently developed foundation models \u2013 large\nlanguage models that can be adapted to multiple tasks and which\nexhibit emergent behaviors for which they have not been explic-\nitly trained [14] \u2013 have also proven to be capable with source code.\n491\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nWhile the intent of training LLMs such as GPT-2 [64] and GPT-3 [17]\nwas to give them mastery of natural language, it quickly became\napparent that the presence of code in their training corpora had\ngiven them the ability to generate code based on natural language\ndescriptions [49]. The Codex model [24] was then produced by fine-\ntuning GPT-3 on a large corpus of source code data, leading to the\ndevelopment of Copilot [32], a tool that helps software engineers by\nautocompleting code as it is being written. Experimentation with\nCopilot has shown its ability to perform additional tasks, such as\nexplaining code, generating documentation, and translating code\nbetween languages [6].\nAlthough autocompletion interfaces are useful and valuable\nwhen the system can discern the developer\u2019s intent, there are many\ninstances where that is insufficient. For example, the developer\nmay have a good idea of what they want to do, but may be unclear\non what functions, libraries, or even algorithms to employ. They\nmay even have general programming questions that need to be\nanswered before they are able to write any code.\nIn this paper, we seek to understand whether modern develop-\nments in code-fluent foundation models \u2013 large language models\nthat have been fine-tuned on source code data \u2013 are sufficient to\nsupport a conversational agent that can act as an assistant in the\nsoftware development process. We developed the Programmer\u2019s\nAssistant to explore the capabilities that conversational interaction\ncould enable and the extent to which users would find conversa-\ntional assistance with programming tasks desirable and useful.\nWe hypothesize that a conversational system may provide a\nflexible and natural means for interacting with a code-fluent LLM.\nConversational interaction could enable users to pursue their ques-\ntions in a multiple exchange dialog (as observed by Barke et al.\n[13]) that allows them to ask follow-up questions and refine their\ninquiries. A conversational programming assistant could ask the\nuser clarifying or disambiguating questions to help it arrive at the\nbest answer. It could also provide multiple types of assistance to\nthe user beyond simply generating code snippets, such as engaging\nin general discussion of programming topics (e.g. [22, 71]) or help-\ning users improve their programming skills (as observed in other\nstudies of automating technologies [99]).\nOur paper makes the following contributions to the IUI commu-\nnity:\n\u2022 We provide empirical evidence that a conversational pro-\ngramming assistant based on a state-of-the-art, code-fluent\nfoundation model provides valuable assistance to software\nengineers in a myriad of ways: by answering general pro-\ngramming questions, by generating context-relevant code,\nby enabling the model to exhibit emergent behaviors, and by\nenabling users to ask follow-up questions that depend upon\ntheir conversational and code contexts.\n\u2022 We show how different interaction models \u2013 conversation,\ndirect manipulation, and search \u2013 provide complementary\ntypes of support to software engineers with tradeoffs be-\ntween the user\u2019s focus and attention, the relevance of sup-\nport to their code context, the provenance of that support,\nand their ability to ask follow-up questions.\n\u2022 We motivate the need to further understand how to design\nhuman-centered AI systems that enhance the joint perfor-\nmance of the human-AI collaboration.\n2\nRELATED WORK\nWe discuss three areas of related work that have either motivated\nour study of conversational programming assistance or provided\nthe technical foundations for it. We begin by briefly summarizing\nRich and Waters\u2019 visionary work on the Programmer\u2019s Appren-\ntice [70], followed by summarizing work on code-fluent foundation\nmodels and human-centered evaluations of how these models im-\npact software engineers\u2019 work. Finally, we discuss conversational\ninteraction and how it might be employed to provide more flexible\nand sophisticated assistance to software engineers.\n2.1\nThe Programmer\u2019s Apprentice\nOur work is inspired by the vision laid out by Rich and Waters [70],\nwhich describes an artificial agent that can act as an intelligent as-\nsistant for software engineers by providing advice, catching errors,\nand handling routine details throughout the software development\nprocess. The Programmer\u2019s Apprentice [70] relied on a knowledge\nbase of \u201cclich\u00e9s,\u201d which are formal, structured versions of what\nare known today as software design patterns [31]. It used a hy-\nbrid reasoning system capable of special-purpose reasoning based\non frames and a plan calculus, along with general purpose logical\nreasoning. Although natural language interaction was envisioned,\nthe original prototype implementation ultimately used a stylized\ncommand language. We view our work as a conceptual successor\nto the Programmer\u2019s Apprentice, as it enables the natural language\ninteraction that the Programmer\u2019s Apprentice lacked.\n2.2\nCode-fluent Foundation Models and\nHuman-Centered Evaluations of\nProgramming Assistance\nGenerative models based on the transformer architecture [96] have\nrecently been applied to the domain of software engineering. Code-\nfluent large language models are capable of generating code from\nnatural language descriptions [105], translating code from one lan-\nguage to another [75], generating unit tests [92], and even gener-\nating documentation for code [36, 38, 97, 98]. These models are\nprobabilistic systems, and as such, do not always produce perfect\nresults (e.g. code that is free of syntax or logical errors). Nonetheless,\nWeisz et al. [102] found that software engineers are still interested\nin using such models in their work, and that the imperfect outputs\nof these models can even help them produce higher-quality code\nvia human-AI collaboration [103].\nNew tools based on code-fluent LLMs are actively being devel-\noped. GitHub Copilot1 is described as \u201cYour AI pair programmer.\u201d It\nis optimized for the code autocompletion use case: given a starting\nsnippet such as a method\u2019s documentation, signature, or partial\nimplementation, Copilot completes the implementation. Copilot is\nbased on the OpenAI Codex model [24], a 12 billion parameter ver-\nsion of GPT-3 [17, 49], fine-tuned on code samples from 54 million\npublic software repositories on GitHub. Empirical evaluations of\n1https://github.com/features/copilot\n492\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nthis model have shown that, although the quality of its outputs is\nquite good, those outputs may still be problematic [57]. Echoing the\nresults from Weisz et al. [103], human-centered evaluations of Copi-\nlot have found that it increases users\u2019 feelings of productivity [109],\nand that almost a third (27%) of its proposed code completions were\naccepted by users. In a contrasting evaluation, Vaithilingam et al.\n[95] found that while most participants expressed a preference\nto use Copilot in their daily work, it did not necessarily improve\ntheir task completion times or success rates. Yet, in a study by\nKalliamvakou [40], developers working with Copilot were able to\nimplement a web server in Javascript 55% faster than developers\nwho did not use Copilot.\nA grounded theory analysis of how programmers interact with\nCopilot [13] found that their interactions varied depending upon\nwhether they were accelerating tasks that they already knew how\nto do or if they were exploring solutions to problems that they were\nless sure about. Autocompletion was effective when developers\nwere operating in \u201cacceleration mode\u201d and relied on the model\nto produce short completions that could be verified quickly. In\n\u201cexploration mode,\u201d however, the interaction was more awkward.\nDevelopers would communicate with Copilot by typing comments\nand seeing what Copilot generated in response. Then, they would\nmodify their comments to explore other ways of prompting a re-\nsponse. Ultimately, the comments used to prompt the model would\nbe deleted after the relevant code was generated, indicating that\ntheir value was largely in driving a back-and-forth, yet context free,\ndialog with the model to coerce it to produce the desired results\nthrough an iterative refinement process. In this paper, we fully\ncommit to a context-aware conversational style of interaction with\na code-fluent LLM and assess the value it provides to users.\n2.3\nConversational Interaction and Analysis\n2.3.1\nConversational Interaction. Using natural language to inter-\nact with technology has had a long research history [2], starting in\nthe 1960s with pattern-matching approaches like Eliza [104], and\ncontinuing to today with state-of-the-art large language model-\nbased conversational systems [107] such as Meena [3] and Blender-\nBot [84]. These systems are intended to address the problem of\nopen-domain dialog, with a goal of realistically engaging in con-\nversation, but not particularly in a goal-directed or task-oriented\nmanner.\nTask-oriented chatbots are typically built with frameworks such\nas the Microsoft Bot Framework2, Google DialogFlow3, and IBM\nWatson Assistant4. They operate using pre-defined dialogue trees\nand use natural language processing to detect conversational intents\nand extract contextual entities. This structure enables the creation of\nspecial purpose, but fairly limited and rigid, conversational agents.\nThere have been several recent attempts to investigate conver-\nsational programming assistance. Kuttal et al. [42] conducted a\nWizard of Oz study in which a pair programmer was replaced with\na conversational agent, and they found that \u201cagents can act as ef-\nfective pair programming partners.\u201d The PACT system [106] is a\nchatbot that assists programmers adjusting to new programming\n2https://dev.botframework.com/\n3https://cloud.google.com/dialogflow\n4https://www.ibm.com/products/watson-assistant/artificial-intelligence\nenvironments. PACT is structured as a discrete question-answering\nsystem based on a neural machine translation approach, but it\ndoesn\u2019t maintain a conversational context.\n2.3.2\nConversation Analysis. Conversation is a form of interaction\nbetween people that enables robust communication. Conversation\nAnalysis [76] is a method for understanding the natural structure\nof human conversational interaction. It catalogs different patterns\nof conversational acts and how they are utilized by interlocutors in\norder to attain a wide variety of goals. Recently, Conversation Anal-\nysis has been adapted to describe patterns of interactions between\nhumans and artificial conversational agents in order to aid in the\ndesign of chatbots [50]. We apply techniques from Conversation\nAnalysis in our study of conversational programming assistance.\n3\nTHE PROGRAMMER\u2019S ASSISTANT\nIn order to explore conversational programming assistance, we\ncreated a functional prototype system called The Programmer\u2019s\nAssistant. Our prototype, shown in Figure 1, combines a code editor\nwith a chat interface. The code editor was implemented using the\nMicrosoft Monaco Editor5 embedded in a React wrapper6. The\nchat user interface was implemented using the React-Chatbot-Kit7\nframework. To drive the conversational interaction, we employed\nOpenAI\u2019s Codex model [24], accessed through its web API.\nWe developed our prototype as a lightweight coding environ-\nment in order to examine the user experience of interacting with\na conversational assistant. Our work was exploratory in nature,\nand thus we did not have specific design goals for the prototype\nbeyond integrating a code editor with a code-fluent LLM. We also\ndid not attempt to target the prototype for a specific class of users\n(e.g. novices or experts) or use cases (e.g. writing code vs. learning a\nnew programming language), as we wanted any value provided by\nconversational assistance to emerge from our user study. We also\ndid not implement the ability to run or debug code in our prototype\nas we wanted to explore the nature of the conversational interac-\ntion rather than having users focus extensively on the production\nof working code.\nWhen designing how users would interact with the Program-\nmer\u2019s Assistant, we decided that it should be available on demand\nand not monitor the user\u2019s work in progress or give unsolicited\nsuggestions or advice, in keeping with the conversational agent\ninteraction model proposed by Ross et al. [73, 74]. This approach\nwas supported by feedback from prospective users who were con-\ncerned about the assistant providing criticism of unfinished efforts\nin progress or distracting them while they worked. Instead, we\nforce initiative onto the user and only have the assistant respond\nto their requests. In this way, the assistant can provide help when\nrequested without undesirable interruptions that can distract or\ninterfere with the user\u2019s flow.\nWhen a user interacts with the assistant, we keep track of their\nselection state in the code editor. If a user sends a message to the\nassistant without any code selected in the editor, then that message\n(along with the prior conversational context) is passed directly to\nthe model. If a user sends a message to the assistant with new code\n5https://microsoft.github.io/monaco-editor/\n6https://www.npmjs.com/package/@monaco-editor/react\n7https://fredrikoseberg.github.io/react-chatbot-kit-docs/\n493\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nselected in the editor (i.e. code that wasn\u2019t previously selected when\nthey sent their last message), then that code is appended to the\nmessage before being communicated to the model.\nThe model may produce multiple types of responses to a user\u2019s\nmessage. We treat each type of response differently in the UI.\n\u2022 Responses that do not contain code are always rendered in\nthe chat UI (Figure 1E).\n\u2022 Responses containing short code snippets (\u2264 10 lines) are\nrendered inline in the chat UI (Figure 1G).\n\u2022 Responses containing longer code snippets (> 10 lines) show\nthe code in a pop-up window (Figure 2A), with a proxy entry\nin the chat transcript (Figure 2B) that allows users to re-\ndisplay the code window after it has been closed. Non-code\ntext in the response remains in the chat transcript.\nThe assistant never directly modifies the contents of the user\u2019s\nsource code; rather, any code the user desires to transfer from the\nchat takes place via copy/paste.\nFigure 1 shows a screenshot of a real, sample conversation, in\nwhich the user asks a question that results in an inline response,\nthen requests an explanation of some code in the editor, and then re-\nquests further elaboration. Figure 2 shows an example conversation\nthat resulted in the generation of a longer code sample, shown in a\npopup window. This example shows how the assistant produced an\nincomplete solution, followed by criticism from the user regarding\nthe missing code, and resulting in an apology and the generation\nof a complete solution.\n3.1\nSupporting Conversational Interaction\nWe enabled Codex to conduct a conversational interaction by prompt-\ning it with a conversational transcript and a request to produce\nthe next conversational turn. The prompt establishes a pattern of\nconversation between a user and a programming assistant named\nSocrates. It provides several examples of Socrates responding to gen-\neral coding questions, generating code in response to a request, and\naccepting code as input. It establishes a convention for delimiting\ncode in the conversation, making it easy to parse for display in the\nUI. It also establishes an interaction style for the assistant, directing\nit to be polite, eager, helpful, and humble, and to present its re-\nsponses in a non-authoritative manner8. Because of the possibility\nthat the model might produce erroneous answers or incorrect code\n(as discussed in Weisz et al. [102]), we felt it was important that the\nassistant convey a sense of uncertainty to encourage users to not\naccept its results uncritically to avoid over-reliance (e.g. as observed\nin Moroz et al.\u2019s study of Copilot [51], and discussed more gener-\nally in Ashktorab et al. [9]) as well as automation bias [45, 46, 65].\nWe present the full text of the prompt used for the assistant in\nAppendix D.\n3.2\nArchitecture & UI Design\nThe Programmer\u2019s Assistant communicates with the Codex API via\na proxy server that forwards requests from the React client. The\nproxy also rate-limits access to conform to the API\u2019s policy, and\nit logs UI events from the client (e.g. requests, responses, and UI\n8The assistant\u2019s use of non-authoritative responses was encoded into the LLM prompt;\noutput token probabilities from the LLM were not utilized to influence the assistant\u2019s\nresponse.\ninteractions) in a back-end database. To address inconsistencies\nin the style or formatting of code generated by Codex, the proxy\nserver reformats all code segments using the Black code formatter\n9 before transmitting them to the client UI.\nThe client maintains the transcript of the ongoing conversation.\nEach time the user sends a message in the chat, the client constructs\na new prompt for the model by concatenating the initial prompt,\nthe chat transcript, and the user\u2019s new utterance, and makes a re-\nquest for the model to complete the transcript. This completion\nrequest also specifies a stop sequence of tokens to prevent the\nmodel from generating both sides of the conversation (e.g. what\nthe model thinks the user\u2019s next utterance might be after the as-\nsistant\u2019s response). Given the API\u2019s limitation on context length\n(4,096 tokens for both the prompt and model response), we silently\n\u201cforget\u201d older exchanges in the chat transcript when constructing\nthe prompt to ensure that our completion request remains within\nbounds. Nonetheless, the entire conversational history remains\nvisible to the user in the UI.\nThe client UI provides a loose coupling between the source code\neditor and the chat interface. Users can hide the chat pane when\nthey wish to focus solely on their code, and re-engage with it\nwhen they desire assistance. Code selected in the editor is included\nin the conversation in order to couple the code context with the\nconversation. Easily-accessible buttons are provided in the UI to\ncopy code responses from the assistant to the clipboard.\n3.3\nHandling Model Limitations\nWhile developing the Programmer\u2019s Assistant, and in early pilot\ntesting, we experienced some quirks and shortcomings of the model\nand our approach to using it for conversational interaction. One\nlimitation stemmed from the fact that the model sometimes pro-\nduced incorrect responses (e.g. code with syntax errors), incomplete\nresponses (e.g. code that was missing functionality), irrelevant re-\nsponses (e.g. responses not related to the user\u2019s question), or insub-\nstantial responses (e.g. \u201cI don\u2019t know\u201d). Because of the probabilistic\nnature of model inference, re-prompting the model would some-\ntimes produce a more correct or appropriate response. Thus, we\nadded the ability for users to \u201ctry again,\u201d either by asking in the chat\nor by clicking a button in the UI (Figure 1C). This feature removes\nthe assistant\u2019s last response from the context presented to the model\nand then re-invokes the model with an increased temperature10.\nAlthough it is possible for transformer models such as Codex to\nproduce multiple possible responses to a single prompt, we only\nrequest a single response in order to speed up response time as\nwell as to preserve the token budget for conversational context.\nThus, the \u201ctry again\u201d feature provides an alternate way to produce\na wider variety of responses.\nDuring pilot testing, we noticed that the assistant sometimes\nhappened to generate the same response to multiple, unrelated re-\nquests. In these cases, the assistant tended to get \u201cstuck\u201d in a pattern\nof repeating the same response and was unable to resume normal\nconversation. To avoid this problem, we automatically execute a\n9https://black.readthedocs.io/en/stable/\n10Temperature is a parameter in a generative model that specifies the amount of\nvariation in the generation process. Higher temperatures result in greater variability\nin the model\u2019s output.\n494\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nA\nB\nH\nG\nF\nE\nC\nD\nFigure 1: The Programmer\u2019s Assistant. The user interface provides a code editor on the left (A) and a chat pane on the right (B).\nThe \u201ctry again\u201d button (C) allows users to ask the assistant to generate an alternate response to the most recent question. The\n\u201cstart over\u201d button (D) resets the conversational context for the assistant, but maintains the chat transcript in the UI. In this\nexample, we show the assistant introduce itself to the user (E). Next, the user asks a general programming question (F), for\nwhich the assistant provides an inline code response (G). The user then asks a question about code selected in the editor (H),\nfollowed by a series of follow-up questions.\n\u201ctry again\u201d operation in the background when we see identical\nconsecutive responses from the assistant.\nFinally, we noticed that the accumulation of conversational con-\ntext sometimes resulted in the assistant becoming fixated on some\nportion of the earlier conversation. For example, it might respond\nto a question with portions of the prompt or of earlier conversation,\nand become less responsive to newer requests. To address this issue,\nwe introduced a \u201cstart over\u201d feature, accessible via the chat or by\nclicking a button in the UI (Figure 1D), that resets the context to the\noriginal prompt, forgetting the rest of the conversational history.\nWe preserve the chat transcript in the UI, but delineate the break in\nthe assistant\u2019s memory with an annotation in the chat transcript.\nThese annotations are added both for \u201ctry again\u201d and \u201cstart over.\u201d\n3.4\nSample Conversation\nWe provide a real sample conversation with the Programmer\u2019s\nAssistant in Listing 1. This conversation begins with the assistant\ngreeting the user (line 1). Next, the user asks a general Python\nprogramming question (line 4), to which the assistant responds with\na non-authoritative remark (\u201cI think...\u201d) and a code snippet (line\n9). The user next asks a follow-up question that depends on their\nprevious question and the assistant\u2019s response (line 11), to which\nthe assistant provides another code snippet (line 15), satisfying the\nuser\u2019s request.\nThe user then switches topics and asks the assistant to write\na Fibonacci function (line 17), and the assistant again responds\nwith a non-authoritative remark (\u201cI will give it a try,\u201d line 20) and\na block of code. The user then asks how the function works (line\n30) and the assistant provides an adequate description (line 32).\n495\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nFigure 2: Code Preview. Long code responses (> 10 lines) are presented in a pop-up window (A), with a button to copy the code\nand close the window. This code can be re-accessed from the chat transcript by clicking the proxy icon (B).\nNext, the user asks the assistant to re-implement the function in a\ndifferent way (line 37), again leveraging the ability to ask follow-up\nquestions. The assistant produces an alternative implementation\nthat conforms to the user\u2019s request (line 41). The user follows up\nwith a question that depends on multiple past utterances and re-\nsponses in the chat transcript (line 47), and the assistant produces\na relevant response (line 49). The conversation closes with the user\nthanking the assistant (line 53) and the assistant acknowledging\ntheir gratitude (line 55).\nListing 1: A conversation with the Programmer\u2019s Assistant.\nCode presented by the assistant is listed in bold face.\n1\nSocrates: Hello! My name is Socrates your programming\n2\nassistant.\nWhat can I help you with today?\n3\n4\nUser:\nHow do I get a list of the keys in a\n5\nPython dictionary?\n6\n7\nSocrates: I think you would do something like this:\n8\n9\nlist(my_dict.keys())\n10\n11\nUser:\nWhat if I just wanted to iterate over them?\n12\n13\nSocrates: I think you would do something like this:\n14\n15\nfor key in my_dict:\n16\n17\nUser:\nWrite a function that returns the nth value\n18\nin the Fibonacci sequence.\n19\n20\nSocrates: I will give it a try.\n21\n22\ndef fib(n):\n23\nif n == 0:\n24\nreturn 0\n25\nelif n == 1:\n26\nreturn 1\n27\nelse:\n28\nreturn fib(n - 1) + fib(n - 2)\n29\n30\nUser:\nHow does this work?\n31\n32\nSocrates: The Fibonacci sequence is defined by the\n33\nrecurrence relation F(n) = F(n-1) + F(n-2).\n34\nThis function implements that recurrence\n35\nrelation.\n36\n37\nUser:\nCan this be done without recursion?\n38\n496\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n39\nSocrates: I think so. How about this?\n40\n41\ndef fib(n):\n42\na,b = 0,1\n43\nfor i in range(n):\n44\na,b = b,a+b\n45\nreturn\n46\n47\nUser:\nWhich version is better?\n48\n49\nSocrates: I think the recursive version is more\n50\nelegant , but the iterative version is more\n51\nefficient.\n52\n53\nUser:\nThanks!\n54\n55\nSocrates: You 're welcome!\n4\nEMPIRICAL STUDY OF CONVERSATIONAL\nPROGRAMMING ASSISTANCE\nWe conducted an empirical user study of the Programmer\u2019s Assis-\ntant to assess whether conversational assistance provides value in\na software engineering context11. Our inquiry focused on the user\nexperience and value of conversational interactions grounded in\ncode. We therefore designed a qualitative study to investigate atti-\ntudes toward a conversational programming assistant: do people\nenjoy interacting conversationally, what kinds of questions do they\nask, and how does the experience compare to other forms of pro-\ngramming support such as searching the web? We note that prior\nstudies (e.g. [103, 105, 109]) conducted quantitative examinations\nof the use of LLMs in code work; our study is akin to Weisz et al.\u2019s\nqualitative examination of software engineers\u2019 attitudes toward\nworking with models that may fail to produce working code [102].\nTo address our questions, we deployed the Programmer\u2019s Assis-\ntant within our organization \u2013 a global technology company \u2013 and\ninvited people to try it out and give us feedback on their experience.\nWe invited people with varying levels of programming skill in order\nto obtain a wide range of feedback on the kinds of use cases for\nwhich the tool could provide assistance.\n4.1\nTasks\nWe set up the Programmer\u2019s Assistant as a playground environment\nthat participants could try out with a few sample programming\nproblems. We created a tutorial to orient participants to the assis-\ntant, its capabilities, and how to interact with it. We also created\nfour programming challenges focused on writing code, document-\ning code, and writing tests for code. We designed these challenges\nto expose participants to a broad range of the assistant\u2019s capabilities.\nFor each of these challenges, we explicitly did not evaluate metrics\nsuch as the participant\u2019s productivity, the quality of their solutions,\nor the time taken to produce them, as the focus of our study was to\nunderstand the utility of conversational interaction. We selected\nPython as the language used for the tutorial and challenges because\nof its general popularity [21] and the fact that it was well-supported\nby our underlying LLM [24].\n4.1.1\nTutorial. All participants were first introduced to the Pro-\ngrammer\u2019s Assistant through a tutorial. The tutorial walked each\n11For historical context, we note that our study was completed before the public\nrelease of ChatGPT [60], which has subsequently demonstrated the application of\nconversational assistance for programming tasks [34].\nparticipant through 10 sample interactions to give them a feeling\nfor what the assistant could do and how to interact with it. The\ntutorial demonstrated how to ask questions, how to request code to\nbe generated, and how to evaluate existing code. It did not specifi-\ncally cover how to generate documentation or unit tests. Tutorial\ninstructions were provided within the code editor. We include the\nspecific text used for the tutorial in Appendix B.\n4.1.2\nProgramming Challenges. After completing the tutorial, par-\nticipants unlocked four programming challenges. Two of the chal-\nlenges involved coding problems (writing a queue class and writing\ncode to create a scatterplot of data in a CSV file), one involved doc-\numenting a given function (an implementation of a graph search\nalgorithm), and one involved writing unit tests for a given func-\ntion (computing the greatest common divisor of two arguments).\nAlthough the Programmer\u2019s Assistant was visible and available for\nuse, we provided no specific requirement that it actually be used to\ncomplete the challenges.\nAfter participants completed their solution to a challenge, they\nsubmitted it by clicking a button in the UI. The code editor used\nin the Programmer\u2019s Assistant was not a fully-functional IDE and\ndid not provide syntax checking or the ability to run, test, or debug\ncode. Due to these limitations, participants were asked to submit\ntheir solutions when they felt they had completed the challenge to\ntheir own satisfaction.\n4.2\nParticipants\nTo recruit participants for our study, we posted internal advertise-\nments in various communications channels focused on software\nengineering. Our advertisements stated that we were evaluating a\nconversational programming assistant, but were kept deliberately\nvague in order to minimize the impact on peoples\u2019 expectations of\nthe experience.\nOur advertisement yielded a pool of 140 potential participants.\nIn order to recruit a diverse sample, we used a screening survey\nthat asked about their job role, their familiarity with and recency\nof use of Python, and their availability to participate in our study.\nWe accepted participants into the study on a rolling basis, selecting\nparticipants to capture a range of programming experiences and\nensure balanced gender representation. We conducted periodic re-\nviews to determine whether we were learning something new from\neach participant or if we had reached the point of saturation [7]. We\nstopped collecting data after running 42 participants as we were no\nlonger observing any new behaviors or gleaning any new insights.\nThe Programmer\u2019s Assistant implementation and configuration\nwere held constant over the course of the study; no changes to the\nUI design or LLM prompt were made.\nOur participants had the following self-identified characteristics:\n\u2022 Job role: 19 Software Engineers, 12 Researcher/Scientists, 3\nSoftware Architects, 2 Data Scientists, 1 Machine Learning\nEngineer, 1 Systems Test Engineer, 1 Business Analyst, 1\nManager, 1 Marketer, and 1 Consultant.\n\u2022 Gender: 21 Female, 19 Male, 1 Gender Variant / Non-con-\nforming, and 1 Preferred not to say.\n\u2022 Python Experience: 17 participants had 3+ years of Python\nexperience, 11 had 1-3 years, 11 had less than 1 year, and 3\nwere not familiar with Python.\n497\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n\u2022 Recency of Python Use: 29 participants had written Python\ncode within the past month, 4 within the past year, 5 within\nthe past 5 years, and 4 had not written Python code within\nthe past 5 years.\nWe provide full demographic information for individual partici-\npants in Appendix E.\n4.3\nProcedure\nParticipants completed the study on their own time, independently\nand without moderation. Each participant was provided with a web\nlink to a pre-study survey that described the nature of the study and\nthe tasks that they would be expected to perform. They were then\ndirected to the Programmer\u2019s Assistant to complete the tutorial and\nthe four programming challenges. When participants indicated they\nwere finished with the challenges12, they were directed to a final\npost-study survey. Complete sessions generally required about an\nhour of effort, though some participants spread their effort across\na longer period of time and across multiple sessions. Participants\nwere compensated for their time at a rate equivalent to US $15/hr.\n4.4\nMeasures\nWe collected a variety of data in our study from three sources:\n(1) Surveys. We employed three surveys in the study: a pre-\nstudy survey to collect demographic information, a pre-task\nsurvey to gauge expectations of the conversational user ex-\nperience, and a post-task survey to assess actual user expe-\nrience. We describe these survey questions in the relevant\ncontext of our results, and we provide a complete listing of\nall survey instruments in Appendix A.\n(2) Event logs. The Programmer\u2019s Assistant was instrumented\nto collect data on participants\u2019 usage. The event logs pro-\nvided timestamped records of interaction events, including\nconversational exchanges, hiding/showing the assistant, use\nof the \u201ctry again\u201d and \u201cstart over\u201d features, and use of copy/-\npaste.\n(3) Conversation logs. From the event logs, we extracted con-\nversational transcripts between each participant and the\nProgrammer\u2019s Assistant.\n5\nRESULTS\n5.1\nData & Analysis\nWe collected a wealth of data in our study: 126 survey responses\nfrom three surveys per participant, containing 296 written com-\nments in open-ended survey questions, and 4,877 instances of 23\ndifferent types of UI events, including 1,699 conversational ex-\nchanges13 in the event logs. We also compute, for each participant,\ncounts or durations for 21 different metrics from the event logs.\nIn our analysis, we deliberately exclude the portion of our data\ncollected during the tutorial exercise. We exclude this data because\nthat activity was guided by the tutorial instructions, not by our\nparticipants\u2019 own initiative. Thus, our final sample consists of 3,172\n12We did not enforce that participants actually complete all of the challenges. Never-\ntheless, all participants but one did submit solutions to all of the challenges.\n13We refer to a participant\u2019s utterance, followed by the assistant\u2019s response, as a\nconversational exchange.\nevents, including 968 conversational exchanges in the event logs;\nno survey data was excluded.\nOur primary analysis of this data is qualitative, as our partici-\npants provided us with a rich source of interesting feedback and\nthought-provoking insights in their comments. Where applicable,\nwe supplement this data with quantitative data from the survey\nand the event logs, as well as chat transcript data from the con-\nversation logs. In this way, we triangulate [47] across our three\ndata sources, using the open-ended survey data as a foundation.\nWhen we quote participants, either from their qualitative survey\nresponses or the conversational transcripts, we reproduce their\nwords exactly as typed, including typos, misspellings, grammati-\ncal errors, capitalization, and potential trigger words, and we only\nmake minor clarifying edits where needed, delineated by square\nbrackets.\nIn order to set the context for our analysis, we first describe\nhow we used reflexive thematic analysis to analyze participants\u2019\nresponses to the open-ended survey questions. We then describe\nour analysis of the conversation logs and our development of a\ncoding guide based on Conversation Analysis [76], and specifically,\nMoore and Arar\u2019s Natural Conversation Framework [50].\n5.1.1\nThematic Analysis of Qualitative Survey Responses. We con-\nducted a reflexive thematic analysis to analyze the responses to\nour seven open-ended survey questions. We followed the process\ndescribed by Braun and Clarke [16] in which researchers immerse\nthemselves in the data, generate codes for material that seems\ninteresting, and then iteratively group and refine codes through\ncollaborative discussion in order to identify higher-level themes.\nInitially, four authors performed open-coding on the open-ended\nsurvey responses. Through discussion, these codes were grouped\nand consolidated into a single set, which were then re-applied to\nthe data by two authors. After another round of discussion, these\nauthors identified a set of 12 higher-level themes. Some themes had\nclear parallels to quantitative survey questions or event log data,\nand thus represented clear instances where we were able to trian-\ngulate across data sources. Other themes surprised us. We structure\nour presentation of the results based on these 12 themes, grouped\ninto three different aspects of the user experience: expectations\nand experience, utility of conversational assistance, and patterns of\ninteraction and mental models.\n5.1.2\nConversation Analysis via the Natural Conversation Frame-\nwork. In order to understand the content and structure of the\nconversations that took place between our participants and the\nProgrammer\u2019s Assistant, we turned to the Natural Conversation\nFramework [50] (NCF). We developed a codebook for the event\nlogs, beginning with 21 different categories of utterances from the\nNCF. Nine NCF categories \u2013 Acknowledgment, Apology, Confirma-\ntion, Expression of Gratitude, Farewell, Greeting, Self-Identification,\nWelfare Check, and Welfare Report \u2013 appeared twice in our code-\nbook to distinguish cases in which the utterance was made by the\nhuman participant vs. the assistant. Other NCF categories were\nsplit to provide nuanced detail about the interaction; for example,\nwe distinguished three different kinds of NCF requests, depending\nupon whether they were stated as Requests for Action (e.g. \u201cWould\nyou...\u201d), Commands of Action (e.g. \u201cWrite a function that...\u201d), or\nExpressions of Desire (e.g. \u201cI want...\u201d). We also added 18 additional\n498\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nInterlocutor\nOrientation\nCodes\nHuman\nSocial\nAcknowledgment, Apology, Criticism, Expression of Gratitude, Farewell, Greeting, Politeness,\nPraise, Self Identification, Small Talk, Welfare Check, Welfare Report\nTask\nAsks Question, Asserts Information, Capability Check, Command of Action, Expression of Desire, Identi-\nfies Error, Request for Action, Requests Elaboration, Requests Explanation\nMeta / UI\nChat Context Required, Confirmation, Copy, Copy (Extraneous), Erroneous Input, Includes Selection,\nIncludes Extraneous Selection, Missing Selection, Paste, Paste (Extraneous), Pasted Code in Chat, Spelling\nError, Start Over, Try Again\nAssistant\nAppears Fixated, Claims Ignorance, Didn\u2019t Understand, Grants Request (Complete), Grants Request\n(Incomplete), Offers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\nIncludes Code, Spews Garbage\nTable 1: Event log codebook. Our codebook contained 46 unique codes, applied separately to participant utterances (Human)\nand assistant responses (Assistant). Codes in bold were applied to both participant and assistant responses. Human codes were\nclassified as demonstrating either a social or task orientation to the assistant.\ncodes to identify meta-information such as utterances that included\ncode, utterances that referenced selected code, utterances that im-\nplicitly or explicitly referenced earlier portions of the conversation,\nor non-verbal UI activities such as copies, pastes, and invocations\nof \u201ctry again\u201d and \u201cstart over.\u201d Finally, we classified a subset of the\nhuman-applied codes based on whether they represented a par-\nticipant\u2019s task or social orientation toward the assistant. We list\nour codes in Table 1, but note that not all of them ended up being\nrelevant to our analysis.\nWhen coding conversational data, we applied individual codes\nat the level of each conversational utterance. We allowed multiple\ncodes to be applied to each utterance to account for utterances that\nperformed multiple functions (e.g. greeting and self-identification).\nIn order to ensure consistency in how our codebook was applied,\ntwo authors coded a 10% sample of the 968 conversational ex-\nchanges, achieving a satisfactory level of inter-rater reliability (Krip-\npendorf\u2019s \ud835\udefc = 0.77, where agreement was conservatively defined\nas having all of the same codes applied to both utterances in a\nconversational exchange).\n5.2\nExpectations and Experience\nPilot testing of the Programmer\u2019s Assistant suggested that software\nengineers would be skeptical of a conversational programming\nassistant and its ability to provide useful assistance. Our study\nrevealed that, for most participants, their actual experience after\nusing the tool was better than they had anticipated. Participants\nwere surprised at the quality of the assistant\u2019s responses and they\nappreciated how its integration with the code editor reduced the\namount of context switching they needed to do in the UI. Some\nparticipants struggled with the code selection feature, although\nothers appreciated the ability to ask questions related to selected\ncode.\n5.2.1\nUsage. All of our participants engaged with the Program-\nmer\u2019s Assistant while working on the challenges, despite there\nbeing no requirement to do so. Forty-one participants submitted\nsolutions to all four challenges, and one participant, P14, only sub-\nmitted solutions for one of the four challenges. Participants spent\nan average of 68 minutes engaged with the assistant, as measured\nby the amount of time the Programmer\u2019s Assistant window was in\nfocus.\nParticipants made an average of 23.0 utterances (SD = 15.1 ut-\nterances) to the assistant. On average, 6.2 of their utterances (SD =\n4.3 utterances) contained a code selection. The average latency per\nrequest14 was 6.7 seconds (SD = 3.1 seconds).\nWe saw a 66.3% rate of acceptance of generated code, where we\nconsidered code to be accepted if the participant performed a copy\nimmediately after the code was generated. This acceptance rate is\nmuch higher than the 27% acceptance rate reported for Copilot [109].\nWe believe one reason we observed a higher acceptance rate is\nbecause Copilot\u2019s completion suggestions are generated proactively,\nwhereas the Programmer\u2019s Assistant\u2019s suggestions are generated\nupon request. When copying generated code from the assistant,\nparticipants most often copied the entirety of the generated code,\nand only in 5.8% of cases did they copy a smaller portion of it.\n5.2.2\nUser Experience Expectations & Changed Attitudes. Prior to\nrunning our study, we had reason to believe that participants would\nbe skeptical of a conversational programming assistant. Before de-\nveloping the Programmer\u2019s Assistant, we showed potential users\nmockups of a program editor with an integrated chatbot feature.\nThese prototypes elicited uniformly negative reactions. People told\nus about their frustrating experiences with conventional chatbots\nand raised doubts about the knowledge, capabilities, and value\nof a conversational programming assistant. This skepticism moti-\nvated us to develop the Programmer\u2019s Assistant in order to evaluate\nwhether the conversational experience, as powered by a state-of-\nthe-art code-fluent LLM, would be better than people had antici-\npated. During pilot testing, we received feedback that the Program-\nmer\u2019s Assistant provided a much better conversational experience\ncompared to testers\u2019 previous experiences with chatbots. Thus, in\ndesigning our study, we felt it important to first gauge participants\u2019\nexpectations of a conversational interaction around code, and then\nmeasure their experience after the fact.\n14This time includes additional time added by our proxy server to ensure our confor-\nmance to the API rate limitation.\n499\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nWe developed a short inventory of six scale items to measure user\nexperience of code work15. The scale was administered twice: once\nbefore participants were exposed to the Programmer\u2019s Assistant\n(but after they had been briefed that they would interact with an AI\nchatbot), and once after completing the programming challenges.\nThe items were presented with the appropriate tense: Do you expect\n(Did you find that) the Programmer\u2019s Assistant: (a) will be (was)\neasy to use; (b) will understand (understood) your requests; (c) will\nprovide (provided) high quality responses; (d) will help (helped)\nyou to write better code; (e) will help (helped) you to write code\nmore quickly; (f) will be (was) enjoyable to use. Each item was rated\non a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3),\nA great deal (4).\nA factor analysis revealed the items on this scale measured a\nsingle construct, which we identify as user experience (Cronbach\u2019s\n\ud835\udefc = 0.87). Thus, we computed two scores of user experience (UX)\nfor each participant: a pre-task UX score computed as the average\nof their six pre-task expectation scale responses, and a post-task\nUX score computed as the average of their six post-task experience\nscale responses.\nWe found that participants had lower initial expectations for\ntheir experience with a conversational programming assistant (pre-\ntask UX M (SD) = 3.0 (0.62) of 4) than their experience actually\nwas (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test\nshows that this difference was significant, \ud835\udc61(41) = 5.94, \ud835\udc5d < .001,\nCohen\u2019s \ud835\udc51 = 0.92 (large). Measured another way, 32 participants\n(76.2%) had post-task UX ratings that were higher than their pre-\ntask expectations, demonstrating a significant shift in attitudes\ntoward conversational programming assistance.\nHowever, the UX ratings alone fail to capture participants\u2019 nu-\nanced expectations of the assistant and the reasons for their shifted\nattitudes after using it. Participants expressed a variety of expecta-\ntions of the assistant before using it, including that it would be easy\nto use (P30) and produce correct responses (P30), understand the\nproblem and what is being asked of it (P8, P9, P11), not interfere\nwith their flow state (P5), produce imperfect or questionable out-\nputs (P6, P21), improve with feedback (P31), provide generic and\nunhelpful answers (P17) or only answer basic questions (P40), and\nproduce responses quickly (P40).\nP17 expected \u201cto be frustrated very quickly and that what I\u2019d think\nwould be relatively common questions would be responded to with\ngeneric, unhelpful answers.\u201d P6 explained, \u201cI didn\u2019t have very good\nexperiences with chatbots. I think I\u2019ll need to spend more time in\nreviewing and fixing the suggestions than in writing the code myself\nfrom scratch.\u201d P11 had a more balanced view, that \u201cIt\u2019ll do some tasks\nreally well, but others will not be as reliable.\u201d\nAfter interacting with the Programmer\u2019s Assistant, many par-\nticipants commented on how the experience was better than they\nanticipated, because it \u201cseemed to be able to handle complex issues\u201d\n(P10) and \u201cwas a great help\u201d (P8). P20 felt it was \u201cincredible!\u201d P6 and\nP17, who were both initially skeptical, reported having a positive\nexperience. For P6, \u201cIt absolutely exceeded all my expectations, in\n15Our scale items were modeled from scales published in Weisz et al. [103, Table 9 \u2013\nAI Support] that measured constructs including ease of use (item 3), response quality\n(item 1), the production of higher-quality code (item 5), and the ability to write code\nmore rapidly (item 4). We added additional items to cover the constructs of request\nunderstanding and enjoyment, and we cast all items on a 4-point scale of extent.\nall aspects that I could have imagined and more!\u201d P17 provided a\nmore quantitative assessment: \u201cInitial expectations: 3 Actual: 9.5.\u201d\nP38 was emphatic in their evaluation: \u201cI was blown away how well\nit allowing me to structure how I want the code to look and work and\njust giving me the thing I asked for.\u201d\nMany participants described a sense of surprise in their experi-\nences. P9 was surprised by how well it understood their requests:\n\u201cI was surprised at how well the Programmer Assistant\nwas able to understand my requests and generate good\ncode/documentation/tests. It understood major concepts\nand was able to explain it to me in a clear way, and it\nwas also able to understand and write functional code.\nIt even was able to help me review my answer. I was also\nsurprised at how well it could understand the context\nof what I was asking in follow-up questions when I did\nnot specify exactly what I was talking about, but rather\nreferencing our prior conversation (such as, \u2018what does\nthat mean\u2019).\u201d (P9)\nSimilarly, P6 was surprised that they liked the conversational\ninteraction when they expected that they wouldn\u2019t:\n\u201cI though[t] I wouldn\u2019t like the chatbot interaction and\nthat I would prefer something like the tool I\u2019ve seen in\nthose demos [of Copilot]. But surprisingly, after using\nthe chatbot (and seeing the results: easy to use, it un-\nderstands well, I felt it like a partner) I like this kind of\nhelp.\u201d (P6)\n5.2.3\nQuality of Assistant\u2019s Responses. In order to gauge the quality\nof responses produced by the Programmer\u2019s Assistant, we examined\nthe 910 task-oriented requests made by participants in the study.\nFor the vast majority (80.2%), the assistant produced a correct re-\nsponse (Grants Request (Complete)); in other cases, the assistant\u2019s\nresponse was incorrect (9.6%; Provided Wrong Answer), correct but\nincomplete (4.4%; Grants Request (Incomplete)), or the assistant\ndidn\u2019t understand (3.4%; Didn\u2019t Understand), claimed ignorance of\nthe subject (1.5%; Claims Ignorance), or produced another type of\nresponse (0.9%; Appears Fixated, Spews Garbage).\nParticipants also reported experiencing this variability in the\nquality of the assistant\u2019s responses. Some participants described\nhow the assistant provided \u201cdetailed answers\u201d (P17) and \u201chigh qual-\nity outputs\u201d (P18) that were \u201csurprisingly good\u201d (P2). P6 felt it was\n\u201cincredible to see the quality of the responses,\u201d and P3 even explored\nthe assistant\u2019s capabilities outside the scope of the challenges and\nfound that it could handle those as well:\n\u201cIt was surprising the quality of the code and the ability\nto answer all my questions correctly. Although I think\nthe challenges may be biased towards what the Assistant\nis able to do, it was a great experience because I asked\nmany other things and it was able to answer correctly.\u201d\n(P3)\nOf course, the Programmer\u2019s Assistant wasn\u2019t perfect, and some\nparticipants did run into issues. For P35, \u201cThe documentation gener-\nation did not perform very well.\u201d P16 questioned the accuracy of the\nknowledge encoded in the model: \u201cDoes the model need to be up-\ndated? It said latest python version is 3.7 but google says it\u2019s 3.10.\u201d In\nsome instances, participants needed to ask their question multiple\n500\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\ntimes to get a good response: \u201cyou need to ask many times if you\nwant to get an answer and also a detailed explanation\u201d (P3). P27 felt,\n\u201cit was annoying when I asked it to try again and it would give me\nthe same response.\u201d P22 struggled because, \u201cIt didn\u2019t seem to handle\nmultiple sentences well.\u201d\nP28 perhaps offered the most scathing criticism, that, \u201cIt makes\nmistakes often enough to be not very practical.\u201d However, despite\nthe production of poorer-quality responses, other participants felt\nthat the assistant was still helpful. P36 reported that, \u201cOnly minor\ntweaks were normally needed to correct any issues.\u201d Similarly, P38\ndescribed how the assistant wasn\u2019t able to completely solve their\nproblem, but provided a useful start:\n\u201cThere was only one hickup I noticed where when I\nasked it to memoize fibonacci it couldn\u2019t, but it dropped\nthe building blocks on my lap for me to finish so that\nwas fine, that was like minutes of effort on my part.\u201d\n(P38)\n5.2.4\nUI Design & Affordances. Participants made many comments\non our specific UI design and the affordances provided (or not\nprovided) in our chat-augmented editor. Overall, the integration\nbetween the chat pane and the code editor was \u201cvery good\u201d (P23),\nwith a \u201cnice interface between the code pane and the assistant pane\u201d\n(P17) that \u201cmakes it really convenient\u201d (P35).\nPrior research by Brandt et al. [15] has shown how keeping\ndevelopers focused in their IDE improves productivity, and our\nparticipants expressed similar sentiments. P40 remarked, \u201cIt allows\nme to stay in one browser window/tab!\u201d and P12 hinted at how the\ninterface might preserve their flow state by \u201cprevent[ing] me from\ngetting distracted when looking into an issue in another tab.\u201d\nSome aspects of our user interface were confusing to participants,\nsuch as the mechanism for selecting code to be included in the\nconversational context. P7 remarked, \u201cIt\u2019s was a little confusing\ndoing the selection part for it to tell me what a function does, but...\nit gave me code that was insanely easy to copy and paste.\u201d Other\nparticipants appreciated the code selection mechanism, such as P11:\n\u201cI enjoyed the code selection feature, and found that very easy to use.\u201d\nIn the event logs, we identified 20 instances in which a participant\nunintentionally included selected code in the conversation when\nit wasn\u2019t needed (Includes Extraneous Selection), 12 instances in\nwhich a code selection was omitted when it was needed to provide\ncontext for the question (Missing Selection), and 16 instances in\nwhich a participant copy/pasted code directly into the chat rather\nthan selecting it in the editor (Pasted Code in Chat). Although\nthese cases represent a small fraction of the 227 instances in which\na code selection was required and included in the conversation\n(Includes Selection), their presence does indicate that more attention\nis needed to the interaction design of code selection.\nAnother issue regarded the awareness of the \u201ctry again\u201d and\n\u201cstart over\u201d features. The \u201ctry again\u201d feature was only used by 14\nparticipants, who used it a total of 63 times over the course of\nthe study. Some participants used it specifically when they got an\nanswer which they saw as clearly wrong, while others used it to\nget a variety of possible answers before proceeding. The \u201cstart over\u201d\nfeature was used even less, by 5 participants who used it a total of\n6 times. Despite our effort to surface these conversational features\nin the UI via shortcut buttons, they may not have been sufficiently\nnoticeable or salient: \u201cThe \u2018try again\u2019 button is not so reachable, often\ntimes I forgot it exists\u201d (P23). By contrast, at least one participant\nwas successful with these features:\n\u201cat some point it had issue with challenge 3 and I had to\nstart over. Just asking \u2018try again\u2019 was not enough and I\nwas getting always the same (wrong and not related)\nanswer. starting again solved the issue!\u201d (P20)\n5.3\nUtility of Conversational Assistance\nOur next set of themes concerns the utility provided by conversa-\ntional programming assistance. Participants felt the assistant was\nhighly valuable and desired to use it in their own work. They felt it\nwould be most helpful for smaller or narrowly-scoped tasks, but\nable to provide a wide variety of types of assistance. The fact that\nthe interaction model was conversational and grounded in code\nwere valuable aspects, as was the ability for the assistant to bolster\nusers\u2019 learning about programming topics through that interac-\ntion. Participants did question whether they could trust and rely\nupon the assistant\u2019s responses, echoing a similar theme discussed\nin Weisz et al. [102].\n5.3.1\nValue & Appropriate Tasks. Participants rated the value of\nthe Programmer\u2019s Assistant highly (M (SD) = 8.6 (1.4) of 10). Many\nparticipants asked questions such as, \u201cCan I have it in my editor\nplease?\u201d (P15), or made comments that, \u201cI would enjoy using it in\nthe future\u201d (P36), \u201cI would love to be able to... have access to it for\nmy coding\u201d (P37), and \u201cI\u2019d love to use this tool as part of my usual\nprogramming workflow if I could!\u201d (P39). Some of the reasons why\nparticipants found it valuable are because it \u201chelp[s] me remember\nhow to do things in certain languages that normally I would just\nGoogle\u201d (P9) and \u201cIt helps me to avoid silly syntax errors and can\nwhen I cannot remember exact function/method names and required\narguments\u201d (P40). We did not observe any differences in value\nratings based on participants\u2019 familiarity with or recency of using\nPython.\nParticipants described a wide variety of tasks for which they\nfelt the assistant would be useful. These tasks included \u201cordinary\u201d\n(P23), \u201csimpler\u201d (P2), and \u201csmall, repetitive\u201d (P4) tasks such as \u201cquick\nlookups\u201d (P25) for \u201cshort chunks of code\u201d (P11) or for \u201cnarrowed\nquestions\u201d (P26). Participants also felt the assistant was useful for\n\u201csmall containable novel algorithms\u201d (P38) and \u201clittle coding problems\u201d\n(P4).\nSeveral kinds of task assistance were reported as being valuable,\nsuch as explaining code (P31), implementing business logic in a\nUI (P38), understanding what code does (P19, P37), and recalling\nlanguage syntax, method names, and arguments (P12, P15, P20, P40,\nP42). P27 felt that the assistant was \u201cMore helpful when recognizing\na specific well known algorithm but not things you make yourself.\u201d\nParticipants also made recommendations for how to increase\nthe value of the Programmer\u2019s Assistant. P38 suggested, \u201cWhat\nwould blow me away though is if it\u2019s able to help with what I do most\noften which is to integrate, refactor and iterate on an existing system.\u201d\nP16, P26, and P38 all desired more information on the data sources\nused to produce the assistant\u2019s responses. P9 requested to \u201cHave\nthe Programmer\u2019s Assistant examine your code and make proactive\nsuggestions for improving it in the chat.\u201d P36 requested the same,\n501\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nbut cautioned that, \u201cCare would need to be taken to avoid becoming\nan annoyance or disrupting the flow of a coding session.\u201d\nIn the post-task survey, we probed participants on how certain\nchanges to the Programmer\u2019s Assistant would either decrease, in-\ncrease, or result in no change to its value. Over 75% of participants\nfelt that the assistant would be more valuable if it operated in a\nproactive manner, either by making improvement suggestions in\nthe chat or as comments directly in the code. Similarly, 78.6% of\nparticipants felt that having more buttons in the UI for common\nfeatures such as explaining or documenting code would make the\ntool more valuable.\n5.3.2\nConversational Interactions Grounded in Code. One of the\nchallenges in interpreting participants\u2019 comments about the utility\nof the Programmer\u2019s Assistant was in disentangling the extent to\nwhich value was derived from the quality of the underlying model\nversus the integration of conversation in a code context. Indeed,\nparticipants felt that the chat interaction was valuable: 69.0% of\nparticipants felt that eliminating the conversational interaction\nand making the assistant behave more like web search would de-\ncrease its value. Further, our analysis of the conversation transcripts\nrevealed that 42% of the 910 task-oriented utterances from partic-\nipants required historical conversational context (Chat Context\nRequired) in order to be correctly interpreted. Thus, we observe\nthat behaviorally, participants did rely on conversational context\nin their interactions.\nIn the post-task survey, 83% of participants rated the importance\nof the ability to ask follow-up questions as being \u201csomewhat\u201d or \u201ca\ngreat deal.\u201d Several participants specifically commented on the value\nof this conversational context. P39 remarked, \u201cI absolutely loved\nhow you can straight up ask follow-up questions to the Programmers\u2019\nAssistant without having to reiterate the original topic/question.\u201d P15\nexpressed a similar sentiment, saying, \u201cI think the conversational\ncontext was someone helpful, just in communicating that it\u2019s a running\nconversation where my context is remembered.\u201d P9 provided a similar\nanalysis:\n\u201cThis tool was so helpful at answering questions I had\nabout the code in the context of the code I am working\non... I was also impressed with how well it was able to\nremember the context of our conversation, especially\nwhen I asked vague follow-up questions.\u201d (P9)\nIn addition, some participants identified how a conversational\ninteraction grounded in code was useful, \u201cbecause I think to \u2018un-\nderstand\u2019 the dev context could be VERY important\u201d (P31). In fact,\n24.9% of task-oriented utterances included a relevant code selection\n(Includes Selection), showing that participants valued this ability.\nContrasting with these participants, P18 felt that interacting\nwith the assistant conversationally was tedious, and they employed\na more direct approach:\n\u201cI really like the PA. But, I didn\u2019t converse with it like\na chat bot. I often told it what to do (\u2018Document this\ncode.\u2019) as opposed to asking it what to do (\u2018How do I\ndocument this code?\u2019). Talking to it the way that was\nsuggested in the tutorial seemed overly verbose/tedious.\u201d\n(P18)\nDespite these individual differences in interaction preferences,\nP39 envisioned that both interaction styles could be supported in\nthe tool:\n\u201cI think both options should exist: people should be able\nto input their queries like a search bar AND also give\ntheir question as if in conversation.\u201d (P39)\n5.3.3\nLearning Effects. One specific benefit of the Programmer\u2019s\nAssistant identified by participants is its ability to help people\nimprove their programming skills and reinforce knowledge gaps.\nFor example, it can help users \u201cremember how to do things in certain\nlanguages... such as, when I am using a language I haven\u2019t used in a\nwhile\u201d (P9). The assistant can also serve as an memory aid, such\nas when \u201cI use a lot of libraries that I don\u2019t always remember all\nof the functions\u201d (P15). Similarly, P31 said, \u201cNo matter how good\nyou\u2019re as a developer, you can\u2019t (humanly) remember all the API of\nhundreds of libs or new languages... I\u2019d learn new dev lang and new\nlib/frameworks faster.\u201d\nP39 felt the assistant \u201cis perfect for programmers of all levels,\u201d and\nP1 felt it could help them rapidly improve their Python skills:\n\u201cI have wanted to learn python... The main concern\nhow much time spent learning is needed before I could\nactually get some value out of learning python. I have\na feeling this would cut that time down from weeks to\na day or so.\u201d (P1)\nP39 also identified the fact that, because the interactions with\nthe assistant are conversational, it forces people to learn how to\ncommunicate to others about their code:\n\u201cThe conversation aspect promotes proper communi-\ncation, which would really stand to benefit budding\nprogrammers if they want to learn how to explain con-\ncepts more fluently in the future to their colleagues.\u201d\n(P39)\nConversely, P36 suggested that over-reliance on programming\nassistance might have a detrimental effect to one\u2019s learning:\n\u201cIt\u2019s definitely a huge time saver, but over-reliance on it\nmay cause new developers to skip learning the reference\nmaterial themselves and discovering new things and\nsparking new ideas.\u201d (P36)\n5.3.4\nTrust. Many participants raised questions about whether\nthey could trust the responses provided by the Programmer\u2019s Assis-\ntant. P21 asked this question most directly: \u201cwill the code be correct,\nsafe, efficient?\u201d Other participants raised similar questions, such as,\n\u201cI\u2019m wondering how it validates it\u2019s answers, if it can be trusted to\nalways give a working answer\u201d (P10), and \u201cSometimes lack of source\nand context may raise doubts in the mind of the programmer\u201d (P16).\nThese issues of trust were exacerbated by the fact that the Pro-\ngrammer\u2019s Assistant did not allow participants to actually run their\ncode. Because of this limitation, participants had to rely on their\nown knowledge to judge the correctness of the assistant\u2019s responses.\nP19 asserted, \u201cThere is no way to evaluate if the Programmer\u2019s as-\nsistant is giving you the right advise or not other than your own\nknowledge,\u201d and P9 concurred: \u201cI had to trust that it was correct (and\nuse my own prior knowledge).\u201d\n502\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nP18 described the potential consequences of allowing the assis-\ntant to write code for them:\n\u201cThe only thing that made me nervous was that it could\nhave introduced a bug that wasn\u2019t immediately appar-\nent. And given I didn\u2019t write the code, I could have easily\nglossed over a mistake when reviewing it. Especially if\nit is also the one writing the test cases.\u201d (P18)\nDespite our efforts to make the Programmer\u2019s Assistant respond\nin non-authoritative ways, we did observe participants sometimes\nuncritically accept generated results that were clearly wrong or\nincomplete. Thus, we did find behavioral evidence for over-reliance.\nListing 2: Building trust through explanations and justifica-\ntions\n1\nP9:\nwhat is a unit test?\n2\n3\nSocrates: A unit test is a test of a single function\n4\nor method.\nIt is typically written by the\n5\nprogrammer who wrote the code.\n6\n7\nP9:\n8\ndef euclid(a, b):\n9\nwhile b != 0:\n10\n(a, b) = (b, a % b)\n11\nreturn a\n12\n13\nhow would I write a unit test for this code?\n14\n15\n[...]\n16\n17\nSocrates: I think you would do something like this:\n18\n19\ndef test_euclid():\n20\nassert euclid(2, 4) == 2\n21\nassert euclid(6, 9) == 3\n22\nassert euclid(17, 12) == 1\n23\nassert euclid(4, 4) == 4\n24\n25\nP9:\nwhat is a euclid function\n26\n27\nSocrates: The euclid function is a function that\n28\ncomputes the greatest common divisor of two\n29\nnumbers.\n30\n31\nP9:\nis this a good unit test for a euclid\n32\nfunction\n33\n34\nSocrates: I believe that it is a good unit test for\n35\na euclid function.\n36\n37\nP9:\nwhy\n38\n39\nSocrates: It seems to me that it tests the edge\n40\ncases of a and b being equal , a being\n41\ngreater than b, and b being greater than a.\n42\n43\n[...]\n44\n45\nP9:\nthanks Socrates this was helpful\n46\n47\nSocrates: Always happy to help.\nOne way to address trust issues is for the assistant to provide\nfurther explanations and justifications that can calibrate a user\u2019s\nconfidence in the assistant\u2019s responses. Such explanations could\nbe requested conversationally, though most participants did not\nattempt to do so. One participant (P9) did ask for such explanations,\nand we show a summary of their transcript in Listing 2. In this\ninstance, P9 asked for a definition of a unit test (line 1), an expla-\nnation of the code being tested (line 25), and justifications of the\nquality of the unit test (lines 31& 37). Thus, we observe that the\nassistant is capable of producing explanations and justifications\nwhen asked.\n5.4\nPatterns of Interaction and Mental Models\nParticipants interacted with the assistant in a variety of ways with\ntwo main patterns of usage standing out: (1) invoking the assis-\ntant to solve the entire programming challenge, and (2) breaking\nthe challenge down into a set of smaller tasks and invoking the\nassistant\u2019s help for each. There were no clear differences in how\nparticipants with differing Python experience approached the tasks.\nParticipants\u2019 mental models of the assistant also varied. Although\nparticipants strongly saw the role of the assistant as being a tool,\ntheir behaviors revealed that in many cases, they actually treated it\nas a social agent. In addition, participants ascribed various mental\ncapacities to the assistant, such as having the ability to understand,\ncompute, and learn.\nParticipants felt the assistant changed the nature of their work\nprocess. For some participants, it enabled them to focus on the\nhigher-level aspects of development because the assistant handled\nlower-level details or provided partial solutions for them to build\nupon. Many participants felt the assistant sped up their work and\nhelped them remain focused on their tasks.\nFinally, participants drew comparisons between the Program-\nmer\u2019s Assistant with other forms of programming support such\nas Copilot and web search. They felt that the conversational style\nof interaction enabled them to discover new, emergent behaviors\nfrom the model that were unavailable from Copilot\u2019s focus on code\nautocompletion. They also felt that the examples provided by the\nassistant were more readily usable within their own code compared\nto browsing for answers within search results, speeding up the\ncoding process. However, some participants advocated for a bal-\nanced approach to the design of programming assistance tools by\nincorporating multiple modes of interaction rather than fixating\non a single one.\n5.4.1\nInteraction Styles and Assistant Role. We observed that par-\nticipants interacted with the Programmer\u2019s Assistant in strikingly\ndifferent ways. Some participants would present the entire chal-\nlenge description to the assistant and then work with the results it\nproduced. Other participants approached the programming chal-\nlenges in a piecemeal fashion, breaking them apart into a set of\nsmaller tasks, then invoking the assistant to aid with each one.\nExperience with Python was not a determinant of how partici-\npants approached the programming challenges, but it did seem to\nimpact how participants interacted with the assistant. Less experi-\nenced participants tended to ask the assistant basic questions such\nas, \u201cWhat is a unit test\u201d (P29, not familiar with Python) and \u201chow\ndo I document a function?\u201d (P27, < 1 year of experience). More expe-\nrienced participants made detailed requests about specific Python\nlibraries or algorithms, such as, \u201cgiven a pandas dataframe with\ntwo columns \u2018Date\u2019 and \u2018Sales\u2019 please use matplotlib to draw me a\nscatterplot\u201d (P38, 3+ years of experience) and \u201cimplement a runge-\nkutta algorithm for solving an ODE with adaptive time steps\u201d (P37,\n3+ years of experience).\nAnother difference we observed in how people interacted with\nthe assistant stemmed from their view on the role it played in their\n503\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\ncollaborative process. Some participants, such as P18, treated it\nmore as a tool by issuing commands rather than asking questions.\nAs quoted earlier, they said, \u201cI didn\u2019t converse with it like a chat\nbot.\u201d P5 described their interaction style similarly: \u201cI found myself\nwanting to type search queries into Socrates, not treating it as a person\nbut as a search tool.\u201d\nIn anticipation that participants would have different orienta-\ntions to the assistant and its role, we asked a question on the post-\ntask survey about the different kinds of roles the assistant might\ntake. These roles generally fell into one of two categories: a tool\norientation (a tool, a reference guide, a content generator, a problem\nsolver), and a social orientation (a collaborator, a colleague, a coach,\nan advisor, a reviewer). Participants rated the extent to which they\nviewed the Programmer\u2019s Assistant in each of these roles on a 4-\npoint scale of extent: Not at all (1), A little (2), Somewhat (3), or A\ngreat deal (4).\n0\n25\n50\n75\n100\nTool\nContent\ngenerator\nReference\nguide\nCollaborator\nProblem\nsolver\nAdvisor\nCoach\nReviewer\nColleague\nRole\nPercentage of participants\nRating\nNot at all\nA little\nSomewhat\nA great deal\nFigure 3: Role orientations. Participants overwhelmingly felt\nthat the assistant\u2019s role was of a tool orientation rather than\na social orientation. The chart shows ratings distributions\nacross 9 different roles the assistant might take, sorted by\nthe percentage of participants who rated the extent of that\nrole as \u201cA great deal.\u201d The leftmost role is of a tool, with\n80.9% of participants rating it as \u201ca great deal.\u201d Following\ntool are content generator (69.0%), reference guide (64.3%),\ncollaborator (40.5%), problem solver (35.7%), advisor (21.4%),\ncoach (19.0%), reviewer (16.7%), and colleague (14.3%).\nWe show participants\u2019 ratings of the assistant\u2019s role in Figure 3.\nDespite the fact that their attitudes toward the assistant overwhelm-\ningly reflected a tool orientation, their behaviors reveal that many\nparticipants actually treated the assistant as a social agent. P6 de-\nscribed how \u201cI felt it like a partner,\u201d and P4 told the assistant, \u201cI\ncould not have solved [the challenge] without your help,\u201d to which\nthe assistant responded, \u201cI\u2019m glad I could help.\u201d\nThe literature on Computers as Social Agents (CASA) helps us\ninterpret this result as it demonstrates how computers are often\ntreated like people [56, 67]. LLM-based conversational agents can\nexacerbate this tendency; as they likely have been trained on ex-\namples of social interaction, they can also respond as social agents.\nIn the conversation logs, we identified participants who inter-\nacted with the assistant in a socially-oriented fashion (the social\norientation codes in Table 1). Twenty participants (47.6%) made at\nleast one socially-oriented utterance. An extreme form of this inter-\naction style can be seen in a snippet from P6\u2019s transcript (Listing 3).\nThe 20 participants with a behaviorally-demonstrated social\norientation did not generally differ in their role ratings from other\nparticipants, except that they rated the assistant as more likely to\nbe an advisor (Fisher\u2019s exact test, two-tailed \ud835\udc5d = .02) or a reviewer\n(Fisher\u2019s exact test, two-tailed \ud835\udc5d = .03). However, they did not differ\nin their overwhelmingly-strong ratings of the tool orientations.\nThus, at least for some participants, there seems to be a dissonance\nin their view of the assistant\u2019s role orientation.\nListing 3: Excerpt from P6\u2019s interaction with the Program-\nmer\u2019s Assistant, in which P6 offers their thanks and congrat-\nulations.\n1\nP6:\nthank you! it's been really nice to code\n2\nwith you! or to have you coding! LOL\n3\nSocrates:\nYou 're welcome!\n4\nP6:\nI hope to see you on air soon , Socrates!\n5\nSocrates:\nI hope so too.\n6\nP6:\ncongrats to you research team! you 've made\n7\na good job!\n8\nSocrates:\nI'm glad you think so.\n9\nP6:\nsorry ... they 've made a good job (well ...\n10\nyou too ... ;-) )\n11\nSocrates:\nThank you.\n12\nP6:\nsee you next time! bye!!\n13\nSocrates:\nGoodbye.\n5.4.2\nMental Capacities. Participants made a number of inferences\nabout the Programmer\u2019s Assistant and its capacities for thought.\nMany participants talked about how the assistant possessed a level\nof \u201cunderstanding\u201d (P6, P8, P11, P18, P32) of \u201cthe context\u201d (P9, P21)\nas well as \u201cmajor concepts\u201d (P9) and \u201cknowledge\u201d (P33). P24 was\namazed by the assistant\u2019s ability to \u201ctake a plain english request\nand interpret it properly.\u201d P7 ascribed intelligence to the assistant,\nsaying, \u201cIt was a lot smarter and trained tha[n] I thought it was.\u201d\nOne participant assumed that the assistant \u201cKeeps improving\nthrough (user) feedback\u201d (P31). Another felt that the assistant was\ncapable of computation: \u201cIt understands the problem... It can calculate\nthe results of a function back\u201d (P8).\nHowever, not all participants were convinced of the assistant\u2019s\nability to understand. P37 questioned the assistant\u2019s limitations: \u201cI\nwonder how far beyond boilerplate it can go and if it works for truly\noriginal problems.\u201d\n5.4.3\nImpact of Conversational Assistance on Work Practices. Many\nparticipants discussed how the Programmer\u2019s Assistant shaped\ntheir work practices on the programming challenges. Overall, par-\nticipants felt that the assistant \u201csaves time\u201d (P10), \u201chelps me code\nfaster\u201d (P34), and would \u201cspeed up my productivity\u201d (P19) because \u201cI\ncould focus on validating and improving the code it generated instead\nof having to write it all from scratch\u201d (P18). P37 remarked that, \u201cIt\nopens a whole new door for fast develpment.\u201d P4 discussed how the\nassistant \u201cwas helpful in staying focused on the code,\u201d although for\nP14, \u201cit took [me] time to get into tempo with the tool.\u201d\nP31 pointed out how the assistant would change the nature of\ntheir work:\n\u201cMy job could focus more on higher level aspects and\ntherefore achieving better (quality) results, besides the\n504\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\ntime-to-value... Data science (and dev) becomes a more\ncreative-higher level experience.\u201d (P31)\nOther participants discussed a work process in which the assis-\ntant provided incomplete solutions \u2013 the \u201cbuilding blocks\u201d (P38) or\n\u201cinitial draft of code\u201d (P11) \u2013 upon which they could build. P5 aptly\ndescribed this process:\n\u201cIt\u2019s nice to copy well formulated challenges in natural\nlanguage and have the code generator take its best stab\nat it, then edit to our hearts content.\u201d (P5)\nParticipants felt that human review of the assistant\u2019s responses\nwas necessary because \u201cThe answers provided are generally not\nnovel solutions, often look clunky and non-elegant. There may be\nsome unnecessary code. Basically the code would need to be reviewed\u201d\n(P16). P35 also pointed out how \u201cThe code generator was good but\nyou still have to really check it.\u201d P19 discussed how they would turn\nto the assistant as a first source for support, and only if it wasn\u2019t\nable to help would they then turn to other support tools:\n\u201cThe way I will use it is, I will first us[e] the Program-\nmer\u2019s assistant for most of my cases. Only in certain\ncases where Programmer\u2019s assistant cant answer things\nI will turn up to official documentation or stack overflow.\u201d\n(P19)\nHowever, latency was a factor for interactive use of the assistant\nand participants noticed when the assistant took a long time to\nrespond. P19 remarked, \u201cSometimes it took lot of time, like more than\n5 seconds.\u201d P40 also felt \u201cthe response [was] a little slow sometimes...\nin chat mode I expect faster responses.\u201d As discussed in Section 5.2.1,\nthe assistant took an average of 6.7 seconds (SD = 3.1 seconds)\nto respond to a request, and participants did appreciate when the\nassistant produced rapid responses: \u201cI loved how quick it was able\nto pull up answers to questions I had\u201d (P38).\n5.4.4\nConversational Interaction vs. Other Interaction Models. Al-\nthough our study was not intended to make comparative evalu-\nations with the Copilot tool, we nonetheless asked participants\nwhether they were familiar with Copilot, and if so, to comment on\nhow the two tools compared. We also asked a similar question to\ncompare the assistant with another popular form of programming\nassistance, searching the web (via a search engine like Google, or\na Q&A site like Stack Overflow). In discussing the differences be-\ntween these three tools, we note that the primary differentiator is\ntheir interaction model.\nThe interaction model for the Programmer\u2019s Assistant is clearly\nconversational: users ask questions in natural language and are\nprovided with a response in natural language and/or code. The\ninteraction model of Copilot is reminiscent of direct manipulation\ninterfaces [37], in which the user\u2019s actions in the user interface\ndirectly manipulate an object on the screen. Copilot automatically\nmakes autocompletion suggestions as the user types. This auto-\ncompleted code is directly placed in the source editor; thus, the\nuser\u2019s work is contained entirely within the scope of the object\non which they are working (i.e. the source code), which is how\ndirect manipulation interfaces operate. In web search, users enter\na separate search context (e.g. a search engine accessed within a\nweb browser), type in a natural language query, and then forage\namongst search results to identify relevant items of interest [12, 62].\nWhen a desirable item is found, users must translate it into their\ncode environment (e.g. via copy/paste) and possibly edit it to fit\ntheir existing code.\nWe also note that the Programmer\u2019s Assistant and Copilot both\nutilize the same underlying AI model, Codex [24], which means\nthat the only difference between these tools is the user experience.\nThe extent to which Codex was trained on data from programming-\nrelated Q&A web sites is less clear, but for the purposes of our\nanalysis, we focus our discussion solely on the differences in their\ninteraction models16.\nParticipants reported various benefits and drawbacks of a con-\nversational interaction over a direct manipulation interaction. Fore-\nmost, conversation \u201cfelt very natural\u201d (P21) and \u201cfeels much more\nnatural using Natural Language with the AI\u201d (P39). In addition, P39\nfelt that \u201cthe use cases of Programmers\u2019 Assistant seem more open-\nended.\u201d Many participants were surprised at the variety of tasks the\nassistant was capable of performing, from writing unit tests (P19,\nP36, P37) and documentation (P12, P19, P36, P37) to explaining\nwhat code did (P31, P38) and even answering general-knowledge\nquestions (P31). Again, we note that the Programmer\u2019s Assistant\nutilizes the same underlying model as Copilot, yet the conversa-\ntional interface was able to expose a wider variety of emergent\nbehaviors from the model. Multiple participants explored the limits\nof the assistant\u2019s knowledge and abilities beyond our programming\nchallenges. For example, P37 asked it questions about physics and\nordinary differential equations (\u201cODe\u201d as written by P37), and was\nsurprised by the \u201cversatility of what it could answer.\u201d\n\u201cI asked it some physics and ODe question and the\nanswers, though not complete, included the key parts\nneeded to write that code.\u201d (P37)\nP31 probed the assistant on its knowledge of geography and was\nsurprised when the assistant produced a correct answer.\n\u201cI asked something out of SW engineering domain (ge-\nography) and it replied correctly, also by correctly an-\nswering on my nationality.\u201d (P31)\nFor some participants, the ability to assess the assistant\u2019s re-\nsponse before committing to it (i.e. by inserting assistant-generated\ncode into their editor) was a boon. P15 described how the copy/-\npaste boundary provided them with \u201ca bit more control to ask specific\nquestions about what I wanted and to assess before putting it in my\ncode.\u201d Other participants felt that the copy/paste boundary was\nmore inefficient:\n\u201cI think the main difference is the ability of Copilot to\nsuggest code while you type, what make it faster and\neasier to use. While using the Programmer\u2019s Assistant,\nyou need to go to the chat, ask the question, copy the\n16As an aside, our comparison of direct manipulation, search, and conversational inter-\naction models is reminiscent of historical comparisons of text-based and graphical user\ninterfaces [54, 63]. Each modality was shown to have advantages and disadvantages.\nFor example, text-only interfaces can provide accessibility [54, 78] and productivity [53]\nadvantages, whereas graphical user interfaces provide greater discoverability [10, 88].\nSome researchers explored ways to bridge the two interaction modes, such as by\ndeveloping GUI wrappers for command line programs [53, 94] or by developing tools\nthat converted GUI activities into procedural descriptions [55]. Our view is that similar\nbridges can be constructed between direct manipulation, search, and conversational\nmodels of interaction; a user\u2019s interaction with an LLM need not be constrained to a\nsingle interaction model.\n505\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\ncode (or rephrase the question if it was not understood\nby the agent), and edit it to match your code.\u201d (P3)\nA large number of participants felt that the conversational inter-\naction was faster than web search (P1, P6, P7, P10, P11, P12, P16,\nP17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability\nto provide \u201creal-time responses\u201d (P32) that can be \u201capplied exactly to\nyour code\u201d (P33) without having to \u201cparse through lots of text... to get\nwhat you need\u201d (P15). In addition, the assistant provided \u201cMUCH\nfaster, better responses\u201d (P17) that were \u201cmuch more relevant to the\nproblems\u201d (P34) and \u201csimple [and] succinct\u201d (P9), without having to\n\u201csort through answers on your own or read documentation\u201d (P9) or\n\u201clook at many posts before finding the relevant one\u201d (P18).\nDespite these benefits, some participants felt that the assistant\nmight not work well for \u201cmore specific and difficult problems on\na bigger scale\u201d as compared to web search. P9 felt that \u201cthe data\n[of the Programmer\u2019s Assistant] wasn\u2019t as rich\u201d as the web. Other\nparticipants felt that the assistant lacked the \u201cmultiple answers\u201d (P9)\nand \u201crich social commentary\u201d (P19) that accompanies answers on\nQ&A sites:\n\u201cI like to see the different versions proposed on stack\noverflow and the commentary of what makes one solu-\ntion better than another in a given situation.\u201d (P27)\nSome participants promoted a more balanced view that there\nisn\u2019t a single mode of interaction superior to all others. P19 felt that\nweb search would be a fallback when the assistant failed to answer\na question. P39 described how search could be integrated with the\nconversational interaction:\n\u201cI think both options should exist: people should be able\nto input their queries like a search bar AND also give\ntheir question as if in conversation.\u201d (P39)\n6\nDISCUSSION\n6.1\nValue of Conversational Interaction\nWe began our research by asking the question of whether con-\ntemporary developments in code-fluent LLMs could sufficiently\nsupport a conversational programming assistant. We believe that\nour work has demonstrated that they can. Clearly, the Program-\nmer\u2019s Assistant was viewed by our participants as a useful tool that\nprovided real value \u2013 so much so that many participants explic-\nitly requested or expressed the desire to use it in their own work.\nHowever, how much of this value was derived from the model itself\nand its ability to produce high-quality responses to programming\nquestions, versus from participants\u2019 ability to conduct extended\nconversational interactions grounded in their actual source code?\nWe believe that both of these constituent aspects were valuable.\nIndeed, many participants commented on their surprise and satis-\nfaction with the quality of the assistant\u2019s responses (Section 5.2.3).\nHowever, participants also valued the conversational interactions\nthat they had with the assistant. In the event logs, we saw evi-\ndence that participants were leveraging conversational context to\nask follow-up questions as well as leveraging code context by ask-\ning about their code selections (Section 5.3.2). Many participants\nreported that they would find the tool less valuable if the conver-\nsational interaction were removed (Section 5.3.2). Further, conver-\nsation seemed to provide unique value beyond other interaction\nmodels (direct manipulation and search) because of its embedded-\nness in the UI and its ability to surface emergent behaviors of the\nmodel (Section 5.4.4).\nWe do not believe that these different interaction models are\nin competition and we agree with P39\u2019s assessment that assistive\ntools can be built using a plethora of different interaction models.\nFor use cases in which a model is known to produce high-quality\nresults (e.g. code autocompletion for Codex), a direct manipulation\ninterface seems wholly appropriate as it would provide a discov-\nerable and predictable way of invoking the model to produce a\nknown type of result. However, direct manipulation interfaces may\nbe less ideal for surfacing the emergent behaviors of a foundation\nmodel [14], and thus natural language interaction may be more suit-\nable. Many popular text-to-image models, such as DALL-E 2 [66]\nand Stable Diffusion [72], operate in a one-shot fashion, in which\nthe user specifies a prompt, clicks a button, and gets results. Our\nstudy demonstrates how the additional contextual layers of con-\nversational history and the artifact-under-development provide\nadditional value to the co-creative process.\n6.2\nToward Human-AI Synergy\nThe aim of human-centered AI is to \u201cenable[] people to see, think,\ncreate, and act in extraordinary ways, by combining potent user\nexperiences with embedded AI methods to support services that\nusers want\u201d [82]. Building upon this definition, Rezwana and Maher\n[69] posit that, \u201cIn a creative collaboration, interaction dynamics,\nsuch as turn-taking, contribution type, and communication, are the\ndriving forces of the co-creative process. Therefore the interaction\nmodel is a critical and essential component for effective co-creative\nsystems.\u201d [69]. They go on to note that, \u201cThere is relatively little\nresearch about interaction design in the co-creativity field, which\nis reflected in a lack of focus on interaction design in many existing\nco-creative systems.\u201d\nOur study begins to address this gap. While many co-creative\nsystems examine casual tasks or experimental activities (e.g., Spoto\nand Oleynik [87]), our focus was on the co-creative practice of\nprogramming. Our goal was to understand peoples\u2019 attitudes to-\nward a conversational programming assistant, akin to Wang et al.\u2019s\nexamination of data scientists\u2019 attitudes toward automated data\nscience technologies [99]. We found that, despite an initial level of\nskepticism, participants felt that a conversational assistant would\nprovide value by improving their productivity (Section 5.4.3). How-\never, further work is needed to assess the extent to which this type\nof assistance provides measurable productivity increases.\nCampero et al. [19] conducted a survey of papers published in\n2021 that examined human-AI synergy, the notion that a human-AI\nteam can accomplish more by working together than either party\ncould accomplish working alone. They found mixed results, with\nno clear consensus emerging on how to design human-centered AI\nsystems that can guarantee positive synergy. Summarizing from\ntheir discussion,\n\u201cPerhaps achieving substantial synergies among people\nand computers is harder than many people think. Per-\nhaps it requires... new ways of configuring groups that\ninclude people and computers. And perhaps it needs\n506\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nmore systematic, focused attention from researchers\nthan it has, so far, received.\u201d [19, p.9]\nWe believe such evaluations of human-AI synergy should go\nbeyond one-shot performance measures. As implied by many of the\nuses cases listed by Seeber et al. [80], human-centered AI systems\nare often deployed in socio-organizational contexts that require\nlongitudinal use [20, 41, 43], such as product design [93], game de-\nsign [4], and engineering [20, Section 3.2.2]. Thus, we would expect\nthat over time and through interaction with each other, human-AI\nteams would improve their performance through a mutual learning\nprocess.\nEvidence for this process surfaced in our study when participants\ndescribed how they could improve their programming skills by\ninteracting with the assistant (Section 5.3.3). We assert that the\nlearning should operate in both directions: not only should people\nimprove their programming skills, but the model itself can also\nimprove based on peoples\u2019 interactions with it. For example, when\nthe assistant provides a code example to the user, and the user\ntakes that example and edits it, those edits constitute feedback that\ncan be used to further fine-tune the model. In addition, through\nlongitudinal use, we believe that human and AI partners can create\nreciprocal representations of one another \u2013 i.e., the human is likely\nto create a mental model of the AI, and the AI may be engineered to\ndevelop a user model for each of its human users [30, 48, 79]. Such a\npair of models is often described as Mutual Theory of Mind [29, 100].\nThis type of capability raises the possibility of personalizing and\nadapting an assistant to the strengths and needs of individual users.\nWith such models, an assistant that knows a user is learning a\nprogramming language could provide natural language explana-\ntions alongside code outputs, whereas an assistant that knows a\nuser is strongly skilled in a programming language might shorten\nor omit those explanations. Similarly, users are likely to update\ntheir mental models of the AI with more experience. We believe the\nspace for exploring how these reciprocal models impact human-AI\nsynergy is rich, and we encourage additional work in this area.\nHuman-centered AI systems that are designed to combine and\nsynergize the distinct skills of humans and AI models cannot suc-\nceed if they diminish the human skills upon which they depend.\nWell-designed human-centered AI systems develop new and com-\nplementary skills for both the human and AI constituents [82, 83],\nand we believe that mutual learning may address concerns that the\nwide deployment and use of AI systems will result in a de-skilling\nof the workforce [77, 108].\nUltimately, the design decisions that go into an interactive AI\nsystem have ethical implications. Our design attempts to augment\nthe user\u2019s knowledge and skills by presenting help on demand,\ncouched in non-authoritative suggestions, which leaves the user\nfirmly in control and ultimately responsible for the work product.\n6.3\nOpportunities for Future Research\nOur work highlights many interesting avenues for future enhance-\nments that could be made to LLM-based conversational assistants\nsuch as our Programmer\u2019s Assistant, as well as future human-\ncentered research on LLM-based conversational assistance.\nOur work employed a code-fluent model that was not specifi-\ncally designed to handle conversational interaction. Fine-tuning\nthe underlying LLM for conversational interaction, such as what\nhas been done with Lamda [91], is one opportunity to improve the\nassistant\u2019s performance. Another opportunity is to align the lan-\nguage model to follow the desiderata proposed by Askell et al. [11]\nand described by Ouyang et al. as, \u201chelpful (they should help the\nuser solve their task), honest (they shouldn\u2019t fabricate information\nor mislead the user), and harmless (they should not cause physical,\npsychological, or social harm to people or the environment)\u201d [61,\np.2]. Glaese et al. [33] propose a slightly different desiderata of\n\u201ccorrect\u201d instead of \u201chonest,\u201d which may be more applicable to the\nsoftware engineering domain, as the ability to produce correct code\nand correct answers about code are both important properties of a\nconversational programming assistant.\nCombining LLMs with search-based approaches to establish ad-\nditional context for the model, such as AlphaCode [44] has done,\nmay also result in more capable systems. These \u201csearches\u201d need not\nbe limited to textual sources, but could be conducted over appro-\npriate semantic stores (e.g. a knowledge graph) and take advantage\nof explicit semantic reasoning services, resulting in an integration\nof symbolic and neural approaches. Further, allowing for \u201cinternal\ndeliberation\u201d of the type shown in Nye et al. [59] could result in\nbetter-reasoned results, as well as better explanations and justifica-\ntions.\nAnother avenue for improvement involves the prompt used to\nconfigure the assistant (Appendix D). Just as the prompt for each\nsuccessive interaction is modified by the growth of the conversa-\ntional transcript, there is no requirement that the initial prompt be\nstatic. It too can be specialized to incorporate aspects of a user model,\nenabling the realization of a Mutual Theory of Mind [29, 100]. Pro-\nviding better UX affordances for visualizing and manipulating the\nactive contexts \u2013 code and conversation \u2013 could provide users with\nmore control over which information contributes to the generation\nof the assistant\u2019s response.\nOur participants clearly indicated that they were interested in\nhaving an assistant that behaved more proactively, in contrast to\nour deliberate design of an assistant that never takes conversational\ninitiative. A more proactive assistant would be able to interrupt or\nremind a user when necessary [23], yet this characteristic raises\nmany challenging issues. How can we calibrate the threshold for\nsuch interruptions? How can users tune the assistant to deliver only\nthose interruptions that the they would find useful (e.g., [28, 81])?\nHow can we help users to regain their prior context after dealing\nwith an interruption (e.g. [89])? Should an assistant be used to\npersuade or nudge the user (e.g. [35])? Who should determine\nthe topic, frequency, and insistence of such persuasion attempts\n(e.g. [52, 85])? Should users have the ability to moderate or defeat\nattempted persuasions, or should those decisions be left to the\norganization?\nFinally, we explored the different kinds of role orientations our\nparticipants had toward the assistant and found that participants\nvaried in their views of it as a tool versus a social agent (e.g. collabo-\nrator or colleague). We posit that peoples\u2019 effectiveness in working\nwith an AI system may be influenced by their role orientation, and\nwe encourage future research in this area.\n507\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n7\nCONCLUSION\nWe developed a prototype system, the Programmer\u2019s Assistant, in\norder to assess the utility of a conversational assistant in a software\nengineering context. The assistant was implemented using a state-\nof-the-art code-fluent large language model, Codex [24], and was\ncapable of generating both code and natural language responses to\nuser inquiries. We further used the prompting mechanism of the\nmodel to set up a conversational interaction in which the model\nuses the conversational history, plus the user\u2019s current utterance,\nin order to generate a response. In this way, users are able to ask\nfollow-up questions in the chat that reference prior utterances and\nresponses. We incorporated the conversational assistant into a code\nediting environment, enabling the conversation to be grounded in\nthe context of the user\u2019s source code.\nWe evaluated this system with 42 participants with varied levels\nof programming skill, and their quantitative and qualitative feed-\nback, coupled with their usage of the system, demonstrated the\nvaried, and sometimes emergent, types of assistance it was able to\nprovide. Many participants noted the high quality of the conversa-\ntional responses, including the assistant\u2019s ability to produce code,\nexplain code, answer general programming questions, and even\nanswer general knowledge questions. Participants felt this type of\nassistance would aid their productivity, and they drew meaningful\ncontrasts between the conversational style of interaction with other\ntools that employ a direct manipulation or search-based interaction\nmodel.\nOur study motivates the use of conversational styles of inter-\naction with large language models by showing how they enable\nemergent behaviors in a co-creative context. The Programmer\u2019s\nAssistant did not always generate perfect code or correct answers;\nnonetheless, participants in our study had an overall positive ex-\nperience working with it on a variety of programming challenges.\nWe believe that our work takes us one step closer to realizing the\nvision of human-centered AI: learning how to design systems that\nmaximize the synergy in human-AI collaborations.\nACKNOWLEDGMENTS\nWe would like to thank Socrates for his tireless assistance during\nthe user study, as well as for suggesting the title of this paper based\non its abstract.\nREFERENCES\n[1] Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What Do Developers\nUse the Crowd For? A Study Using Stack Overflow. IEEE Software 34, 2 (2017),\n53\u201360. https://doi.org/10.1109/MS.2017.31\n[2] Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technol-\nogy, and applications. Machine Learning with Applications 2 (2020), 100006.\n[3] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel,\nRomal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V. Le. 2020. Towards a Human-like Open-Domain Chatbot.\n[4] Safinah Ali, Nisha Elizabeth Devasia, and Cynthia Breazeal. 2022. Escape! Bot:\nSocial Robots as Creative Problem-Solving Partners. In Creativity and Cognition.\n275\u2013283.\n[5] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.\nA survey of machine learning for big code and naturalness. ACM Computing\nSurveys (CSUR) 51, 4 (2018), 1\u201337.\n[6] Irene Alvarado, Idan Gazit, and Amelia Wattenberger. 2022. GitHub Next |\nGitHub Copilot Labs. https://githubnext.com/projects/copilot-labs/\n[7] Hikari Ando, Rosanna Cousins, and Carolyn Young. 2014. Achieving saturation\nin thematic analysis: Development and refinement of a codebook. Comprehensive\nPsychology 3 (2014), 03\u2013CP.\n[8] Craig Anslow, Stuart Marshall, James Noble, and Robert Biddle. 2013. Sourcevis:\nCollaborative software visualization for co-located environments. In 2013 First\nIEEE Working Conference on Software Visualization (VISSOFT). IEEE, 1\u201310.\n[9] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Naren-\ndra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, Qian\nPan, Christine T Wolf, et al. 2021. AI-Assisted Human Labeling: Batching for\nEfficiency without Overreliance. Proceedings of the ACM on Human-Computer\nInteraction 5, CSCW1 (2021), 1\u201327.\n[10] Catherine A Ashworth. 1996. GUI Users have trouble using graphic conventions\non novel tasks. In Conference Companion on Human Factors in Computing Systems.\n75\u201376.\n[11] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom\nHenighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.\nA general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861 (2021).\n[12] Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018. Measuring the utility of\nsearch engine result pages: an information foraging based measure. In The 41st\nInternational ACM SIGIR conference on research & development in information\nretrieval. 605\u2013614.\n[13] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. arXiv\npreprint arXiv:2206.15000 (2022).\n[14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[15] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R Klemmer. 2010.\nExample-centric programming: integrating web search into the development\nenvironment. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems. 513\u2013522.\n[16] Virginia Braun and Victoria Clarke. 2022. Common challenges in Thematic\nAnalysis and how to avoid them. Retrieved August 11 2022 from https://youtu.\nbe/tpWLsckpM78\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage Models are\nFew-Shot Learners. In Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33.\nCurran Associates, Inc., 1877\u20131901. https://proceedings.neurips.cc/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[18] Sallyann Bryant, Pablo Romero, and Benedict\" du Boulay. 2006. The Collabora-\ntive Nature of Pair Programming. In Extreme Programming and Agile Processes\nin Software Engineering, Pekka Abrahamsson, Michele Marchesi, and Giancarlo\nSucci (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 53\u201364.\n[19] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, Abdullah\nAlmaatouq, and Thomas W Malone. 2022. A Test for Evaluating Performance\nin Human-Computer Systems. arXiv preprint arXiv:2206.12390 (2022).\n[20] Gaetano Cascini, Yukari Nagai, Georgi V Georgiev, Jader Zelaya, Niccol\u00f2 Be-\ncattini, Jean-Fran\u00e7ois Boujut, Hernan Casakin, Nathan Crilly, Elies Dekoninck,\nJohn Gero, et al. 2022. Perspectives on design creativity and innovation research:\n10 years later. , 30 pages.\n[21] Stephen Cass. 2022. Top Programming Languages 2022. IEEE Spectrum (23 Aug\n2022). https://spectrum.ieee.org/top-programming-languages-2022\n[22] Cristina Catalan Aguirre, Nuria Gonzalez Castro, Carlos Delgado Kloos, Carlos\nAlario-Hoyos, and Pedro Jos\u00e9 Mu\u00f1oz Merino. 2021. Conversational agent for\nsupporting learners on a MOOC on programming with Java. (2021).\n[23] Ana Paula Chaves and Marco Aurelio Gerosa. 2021. How should my chatbot\ninteract? A survey on social characteristics in human\u2013chatbot interaction design.\nInternational Journal of Human\u2013Computer Interaction 37, 8 (2021), 729\u2013758.\n[24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared\nKaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex\nRay, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,\nIgor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating a Large Language Models Trained on Code.\n[25] Li-Te\nCheng,\nR.B.\nDe\nSouza,\nSusanne\nHupfer,\nJohn\nPatter-\nson,\nand\nSteven\nRoss.\n2003.\nBuilding\nCollaboration\ninto\nIDEs:\nEdit>Compile>Run>Debug>Collaborate? Queue 1, 9 (2003).\n508\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n[26] Carl Cook, Warwick Irwin, and Neville Churcher. 2005. A user evaluation\nof synchronous collaborative software engineering tools. In 12th Asia-Pacific\nSoftware Engineering Conference (APSEC\u201905). IEEE, 6\u2013pp.\n[27] Claudio Le\u00f3n de la Barra, Broderick Crawford, Ricardo Soto, Sanjay Misra,\nand Eric Monfroy. 2013. Agile Software Development: It Is about Knowledge\nManagement and Creativity. In Computational Science and Its Applications \u2013\nICCSA 2013, Beniamino Murgante, Sanjay Misra, Maurizio Carlini, Carmelo M.\nTorre, Hong-Quang Nguyen, David Taniar, Bernady O. Apduhan, and Osvaldo\nGervasi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 98\u2013113.\n[28] Uri Dekel and Steven Ross. 2004. Eclipse as a platform for research on in-\nterruption management in software development. In Proceedings of the 2004\nOOPSLA workshop on Eclipse Technology eXchange (Vancouver, British Columbia,\nCanada), Michael G. Burke (Ed.). ACM, 12\u201316.\n[29] Bobbie Eicher, Kathryn Cunningham, Sydni Peterson Marissa Gonzales, and\nAshok Goel. 2017. Toward mutual theory of mind as a foundation for co-creation.\nIn International Conference on Computational Creativity, Co-Creation Workshop.\n[30] Stephen M Fiore, Eduardo Salas, and Janis A Cannon-Bowers. 2001. Group\ndynamics and shared mental model development. How people evaluate others in\norganizations 234 (2001).\n[31] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design\npatterns: elements of reusable object-oriented software. Addison-Wesley.\n[32] GitHub, Inc. 2022. GitHub copilot \u00b7 your AI pair programmer. Retrieved August\n5, 2022 from https://github.com/features/copilot/\n[33] Amelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu,\nTimo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe\nThacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona\nComanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie\nChen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u0148a Mokr\u00e1, Nicholas\nFernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac,\nJohn Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\nGeoffrey Irving. 2022. Improving alignment of dialogue agents via targeted\nhuman judgements. https://arxiv.org/abs/2209.14375\n[34] Stephanie Glen. 2022.\nChatGPT writes code, but won\u2019t replace devel-\nopers.\nTechTarget (14 12 2022).\nRetrieved 20-Jan-2023 from https:\n//www.techtarget.com/searchsoftwarequality/news/252528379/ChatGPT-\nwrites-code-but-wont-replace-developers\n[35] Samuel Holmes, Anne Moorhead, Raymond Bond, Huiru Zheng, Vivien Coates,\nand Mike McTear. 2018. WeightMentor: a new automated chatbot for weight\nloss maintenance. In Proceedings of the 32nd International BCS Human Computer\nInteraction Conference 32. 1\u20135.\n[36] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment\ngeneration with hybrid lexical and syntactical information. Empirical Software\nEngineering 25, 3 (2020), 2179\u20132217.\n[37] Edwin L Hutchins, James D Hollan, and Donald A Norman. 1985. Direct manip-\nulation interfaces. Human\u2013computer interaction 1, 4 (1985), 311\u2013338.\n[38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.\nSummarizing source code using a neural attention model. In Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). 2073\u20132083.\n[39] Andreas Jedlitschka and Markus Nick. 2003. Software Engineering Knowledge\nRepositories. Springer Berlin Heidelberg, Berlin, Heidelberg, 55\u201380.\n[40] Eirini Kalliamvakou. 2022.\nResearch: Quantifying github copilot\u2019s impact\non developer productivity and happiness.\nhttps://github.blog/2022-09-07-\nresearch-quantifying-github-copilots-impact-on-developer-productivity-\nand-happiness/\n[41] Anna Kantosalo et al. 2019. Human-Computer Co-Creativity: Designing, Evalu-\nating and Modelling Computational Collaborators for Poetry Writing. (2019).\n[42] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade-\nOffs for Substituting a Human with an Agent in a Pair Programming Context:\nThe Good, the Bad, and the Ugly. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association\nfor Computing Machinery, New York, NY, USA, Article 243, 20 pages.\n[43] Lauramaria Laine. 2021. Exploring Advertising Creatives\u2019 Attitudes Towards\nHuman-AI Collaboration. (2021).\n[44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,\nR\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,\nand et al. 2022. Competition-level code generation with AlphaCode.\nhttps:\n//arxiv.org/abs/2203.07814\n[45] Yaosheng Lou and Qi Sun. 2021. Over-reliance on database: A case study of\nusing web of science. Human Behavior and Emerging Technologies 3, 3 (2021),\n454\u2013459.\n[46] David Lyell and Enrico Coiera. 2017. Automation bias and verification complex-\nity: a systematic review. Journal of the American Medical Informatics Association\n24, 2 (2017), 423\u2013431.\n[47] Wendy E Mackay and Anne-Laure Fayard. 1997. HCI, natural science and\ndesign: a framework for triangulation across disciplines. In Proceedings of the\n2nd conference on Designing interactive systems: processes, practices, methods, and\ntechniques. 223\u2013234.\n[48] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, and Janis A\nCannon-Bowers. 2000. The influence of shared mental models on team process\nand performance. Journal of applied psychology 85, 2 (2000), 273.\n[49] Cade Metz. 2022. Meet GPT-3. It Has Learned to Code (and Blog and Ar-\ngue). (Published 2020). https://www.nytimes.com/2020/11/24/science/artificial-\nintelligence-ai-gpt3.html\n[50] Robert J. Moore and Raphael Arar. 2019. Conversational UX Design: A Practi-\ntioner\u2019s Guide to the Natural Conversation Framework. Association for Computing\nMachinery, New York, NY, USA.\n[51] Ekaterina A Moroz, Vladimir O Grizkevich, and Igor M Novozhilov. 2022. The\nPotential of Artificial Intelligence as a Method of Software Developer\u2019s Produc-\ntivity Improvement. In 2022 Conference of Russian Young Researchers in Electrical\nand Electronic Engineering (ElConRus). IEEE, 386\u2013390.\n[52] Michael Muller, Stevean Ross, Stephanie Houde, Mayank Agarwal, Fernando\nMartinez, John Richards, Kartik Talamadupula, and Justin D Weisz. 2022. Drink-\ning Chai with Your (AI) Programming Partner: A Design Fiction about Gener-\native AI for Software Engineering. HAI-GEN Workshop at IUI 2022: 3rd Work-\nshop on Human-AI Co-Creation with Generative Models (2022).\nhttps://hai-\ngen.github.io/2022/\n[53] Sandra R Murillo and J Alfredo S\u00e1nchez. 2014. Empowering interfaces for system\nadministrators: Keeping the command line in mind when designing GUIs. In\nProceedings of the XV International Conference on Human Computer Interaction.\n1\u20134.\n[54] Elizabeth D Mynatt and Gerhard Weber. 1994. Nonvisual presentation of graph-\nical user interfaces: contrasting two approaches. In Proceedings of the SIGCHI\nconference on Human factors in computing systems. 166\u2013172.\n[55] Alok Mysore and Philip J Guo. 2017. Torta: Generating mixed-media gui and\ncommand-line app tutorials using operating-system-wide activity tracing. In\nProceedings of the 30th Annual ACM Symposium on User Interface Software and\nTechnology. 703\u2013714.\n[56] C. Nass and Y. Moon. 2000. Machines and Mindlessness: Social Responses to\nComputers. Journal of Social Issues 56, 1 (2000), 81\u2013103.\n[57] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copi-\nlot\u2019s Code Suggestions. In 2022 IEEE/ACM 19th International Conference on\nMining Software Repositories (MSR). IEEE, 1\u20135.\n[58] Martin Nordio, H Estler, Carlo A Furia, Bertrand Meyer, et al. 2011. Collaborative\nsoftware development on the web. arXiv preprint arXiv:1105.0768 (2011).\n[59] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Witold Michalewski, Ja-\ncob Austin, David Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten Paul\nBosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your\nWork: Scratchpads for Intermediate Computation with Language Models.\nhttps://arxiv.org/abs/2112.00114.\n[60] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAI\nBlog (30 11 2022). Retrieved 20-Jan-2023 from https://openai.com/blog/chatgpt/\n[61] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training\nlanguage models to follow instructions with human feedback.\nhttps://arxiv.\norg/abs/2203.02155\n[62] Peter Pirolli and Stuart Card. 1999. Information foraging. Psychological review\n106, 4 (1999), 643.\n[63] Larry Press. 1990. Personal computing: Windows, DOS and the MAC. Commun.\nACM 33, 11 (1990), 19\u201326.\n[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. Language Models are Unsupervised Multitask Learners.\n[65] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. 2019. Machine learning in\nmedicine. New England Journal of Medicine 380, 14 (2019), 1347\u20131358.\n[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022. Hierarchical text-conditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125 (2022).\n[67] B. Reeves and C.I. Nass. 1996. The Media Equation: How People Treat Computers,\nTelevision, and New Media Like Real People and Places. CSLI Publications.\n[68] Mawarny Md Rejab, James Noble, and George Allan. 2014. Distributing Expertise\nin Agile Software Development Projects. In 2014 Agile Conference. 33\u201336.\n[69] Jeba Rezwana and Mary Lou Maher. 2021. COFI: A Framework for Modeling\nInteraction in Human-AI Co-Creative Systems.. In ICCC. 444\u2013448.\n[70] Charles H. Rich and Richard C. Waters. 1990. The Programmer\u2019s Apprentice.\nAddison-Wesley Publishing Company, Reading, MA.\n[71] Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddy\u2014A Conver-\nsational Agent for Pair Programming. ACM Transactions on Computer-Human\nInteraction (TOCHI) 29, 4 (2022), 1\u201344.\n[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. 10684\u201310695.\n509\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n[73] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004.\nA Multiple-\nApplication Conversational Agent. In Proceedings of the 9th International Con-\nference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI \u201904). Asso-\nciation for Computing Machinery, New York, NY, USA, 319\u2013321.\n[74] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. Voice User Interface\nPrinciples for a Conversational Agent. In Proceedings of the 9th International\nConference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI \u201904).\nAssociation for Computing Machinery, New York, NY, USA, 364\u2013365.\n[75] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample.\n2020. Unsupervised Translation of Programming Languages. In Advances in\nNeural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,\nM.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 20601\u201320611.\n[76] Harvey Sacks. 1984. Notes on methodology. In Structures of Social Action:\nStudies in Conversation Analysis, John Heritage and J. Maxwell Atkinson (Eds.).\nCambridge University Press, Cambridge, 2\u201327.\n[77] Nithya Sambasivan and Rajesh Veeraraghavan. 2022. The Deskilling of Domain\nExpertise in AI Development. In CHI Conference on Human Factors in Computing\nSystems. 1\u201314.\n[78] Harini Sampath, Alice Merrick, and Andrew Macvean. 2021. Accessibility of\ncommand line interfaces. In Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems. 1\u201310.\n[79] Matthias Scheutz, Scott A DeLoach, and Julie A Adams. 2017. A framework for\ndeveloping and using shared mental models in human-agent teams. Journal of\nCognitive Engineering and Decision Making 11, 3 (2017), 203\u2013224.\n[80] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-Jan\nDe Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah Oeste-Rei\u00df,\nNils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in\nteam collaboration. Information & management 57, 2 (2020), 103174.\n[81] Shilad Sen, Werner Geyer, Michael Muller, Marty Moore, Beth Brownholtz, Eric\nWilcox, and David R Millen. 2006. FeedMe: a collaborative alert filtering system.\nIn Proceedings of the 2006 20th anniversary conference on Computer supported\ncooperative work. 89\u201398.\n[82] Ben Shneiderman. 2020. Human-centered artificial intelligence: Three fresh\nideas. AIS Transactions on Human-Computer Interaction 12, 3 (2020), 109\u2013124.\n[83] Ben Shneiderman. 2022. Human-Centered AI. Oxford University Press.\n[84] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller,\nMegan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. BlenderBot 3:\na deployed conversational agent that continually learns to responsibly engage.\narXiv preprint arXiv:2208.03188 (2022).\n[85] Michael Skirpan and Casey Fiesler. 2018. Ad empathy: A design fiction. In\nProceedings of the 2018 ACM Conference on Supporting Groupwork. 267\u2013273.\n[86] Diomidis Spinellis. 2012. Git. IEEE Software 29, 3 (2012), 100\u2013101.\nhttps:\n//doi.org/10.1109/MS.2012.61\n[87] Angie Spoto and Natalia Oleynik. 2017. Library of Mixed-Initiative Creative\nInterfaces. Retrieved 19-Jun-2021 from http://mici.codingconduct.cc/\n[88] Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh.\n2021. Actionable UI Design Guidelines for Smartphone Applications Inclusive\nof Low-Literate Users. Proceedings of the ACM on Human-Computer Interaction\n5, CSCW1 (2021), 1\u201330.\n[89] Margaret-Anne Storey and Alexey Zagalsky. 2016. Disrupting developer produc-\ntivity one bot at a time. In Proceedings of the 2016 24th ACM SIGSOFT international\nsymposium on foundations of software engineering. 928\u2013931.\n[90] Kartik Talamadupula. 2021. Applied AI matters: AI4Code: applying artificial\nintelligence to source code. AI Matters 7, 1 (2021), 18\u201320.\n[91] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-\nshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, and\net al. 2022.\nLAMDA: Language models for dialog applications.\nhttps:\n//arxiv.org/abs/2201.08239\n[92] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel\nSundaresan. 2020. Unit Test Case Generation with Transformers and Focal\nContext. arXiv preprint arXiv:2009.05617 (2020).\n[93] Severi Uusitalo, Anna Kantosalo, Antti Salovaara, Tapio Takala, and Christian\nGuckelsberger. 2022. Co-creative Product Design with Interactive Evolutionary\nAlgorithms: A Practice-Based Reflection. In International Conference on Compu-\ntational Intelligence in Music, Sound, Art and Design (Part of EvoStar). Springer,\n292\u2013307.\n[94] Priyan Vaithilingam and Philip J Guo. 2019. Bespoke: Interactively synthesizing\ncustom GUIs from command-line applications by demonstration. In Proceedings\nof the 32nd annual ACM symposium on user interface software and technology.\n563\u2013576.\n[95] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922).\nAssociation for Computing Machinery, New York, NY, USA, Article 332, 7 pages.\nhttps://doi.org/10.1145/3491101.3519665\n[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\nNeed. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[97] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and\nPhilip S Yu. 2018. Improving automatic source code summarization via deep rein-\nforcement learning. In Proceedings of the 33rd ACM/IEEE international conference\non automated software engineering. 397\u2013407.\n[98] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D\nWeisz, Xuye Liu, Lingfei Wu, and Casey Dugan. 2022. Documentation Matters:\nHuman-Centered AI System to Assist Data Science Code Documentation in\nComputational Notebooks. ACM Transactions on Computer-Human Interaction\n29, 2 (2022), 1\u201333.\n[99] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer,\nCasey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019.\nHuman-AI collaboration in data science: Exploring data scientists\u2019 perceptions\nof automated AI. Proceedings of the ACM on Human-Computer Interaction 3,\nCSCW (2019), 1\u201324.\n[100] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.\nTowards mutual theory of mind in human-ai interaction: How language reflects\nwhat students perceive about a virtual teaching assistant. In Proceedings of the\n2021 CHI Conference on Human Factors in Computing Systems. 1\u201314.\n[101] Jeremy Warner and Philip J Guo. 2017. Codepilot: Scaffolding end-to-end\ncollaborative software development for novice programmers. In Proceedings of\nthe 2017 CHI Conference on Human Factors in Computing Systems. 1136\u20131141.\n[102] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,\nFernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per-\nfection Not Required? Human-AI Partnerships in Code Translation. In 26th\nInternational Conference on Intelligent User Interfaces. 402\u2013412.\n[103] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie\nHoude, Mayank Agarwal, Kartik Talamadupula, and John T Richards. 2022. Bet-\nter together? an evaluation of ai-supported code translation. In 27th International\nConference on Intelligent User Interfaces. 369\u2013391.\n[104] Joseph Weizenbaum. 1966. ELIZA \u2014 a computer program for the study of\nnatural language communication between man and machine. Commun. ACM 9\n(1966), 36\u201345.\n[105] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation\nfrom natural language: Promise and challenges. ACM Transactions on Software\nEngineering and Methodology (TOSEM) 31, 2 (2022), 1\u201347.\n[106] Aditya Ankur Yadav, Ishan Garg, and Dr. Pratistha Mathur. 2019. PACT - Pro-\ngramming Assistant ChaTbot. In 2019 2nd International Conference on Intelligent\nCommunication and Computational Techniques (ICCT). 131\u2013136.\n[107] Munazza Zaib, Quan Z. Sheng, and W. Zhang. 2020. A Short Survey of Pre-\ntrained Language Models for Conversational AI-A New Age in NLP. Proceedings\nof the Australasian Computer Science Week Multiconference (2020).\n[108] Elaine Zibrowski, Lisa Shepherd, Kamran Sedig, Richard Booth, Candace Gibson,\net al. 2018. Easier and faster is not always better: grounded theory of the impact\nof large-scale system transformation on the clinical work of emergency medicine\nnurses and physicians. JMIR Human Factors 5, 4 (2018), e11013.\n[109] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin,\nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Produc-\ntivity Assessment of Neural Code Completion. In Proceedings of the 6th ACM\nSIGPLAN International Symposium on Machine Programming (San Diego, CA,\nUSA) (MAPS 2022). Association for Computing Machinery, New York, NY, USA,\n21\u201329. https://doi.org/10.1145/3520312.3534864\n510\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nA\nSURVEY INSTRUMENTS\nA.1\nScreening Survey\nThe questions below were asked of prospective participants to\nunderstand their job role, Python experience, and familiarity with\nGitHub Copilot. The questions on Python experience were modeled\nafter those used by Weisz et al. [103].\n1. Do you consider yourself primarily a...\n\u2022 Data Scientist\n\u2022 Manager\n\u2022 Software Architect\n\u2022 Software Engineer\n\u2022 Machine Learning Engineer\n\u2022 Other: write-in\n2. To what extent are you familiar with Python?\n\u2022 I am not familiar with Python\n\u2022 I have < 1 year of experience with Python\n\u2022 I have 1-3 years experience with Python\n\u2022 I have 3+ years of experience with Python\n3. How recently have you written Python code?\n\u2022 Within the past month\n\u2022 Within the past year\n\u2022 Within the past 5 years\n\u2022 Have not written Python code within the past 5 years\n4. To what extent are you familiar with GitHub Copilot?\n\u2022 I am not familiar with Copilot\n\u2022 I\u2019ve seen demos and/or read about Copilot\n\u2022 I\u2019ve tried out Copilot\n\u2022 I\u2019ve used Copilot as a tool for my work\nA.2\nPre-task Survey\nThe questions below were asked before a participant used the Pro-\ngrammer\u2019s Assistant to assess their expectations of a conversational\nprogramming assistant. This survey took approximately 5 minutes\nto complete and began with the instructions below:\nHello! We are a team of researchers looking for feed-\nback on a prototype system we call the Program-\nmer\u2019s Assistant.\nThe Programmer\u2019s Assistant is an experiment in con-\nversational coding: it consists of a code editor in-\ntegrated with a chatbot that is able to converse in\nnatural language to answer questions, generate code,\nand consult on existing code.\nIn this study, you will be asked to complete several\nprogramming tasks. We are not evaluating your\nprogramming skills on these tasks. Rather, we are\ninterested in understanding how the Programmer\u2019s\nAssistant is able to help you accomplish those tasks.\nYour code and interactions with the assistant will be\nprocessed by a 3rd party AI model, so please do not\ninclude proprietary code or discuss company-\nconfidential information. All data we collect in\nthis study will be anonymized before it is published.\nBefore trying out the Programmer\u2019s Assistant, we\nwould like to assess some of your expectations. We\nestimate that this survey will take 5 minutes.\nBy submitting this survey, you consent to participate\nin our study. If you would like to withdraw your con-\nsent, please email us at [removed].\nThanks!\n1. Based on your past experience using chatbots, please let us know\nhow you would anticipate an AI chatbot serving as a programmer\u2019s\nassistant to perform. Do you expect it will:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 Be easy to use?\n\u2022 Understand your requests?\n\u2022 Provide high quality responses?\n\u2022 Help you to write better code?\n\u2022 Help you to write code more quickly?\n\u2022 Be enjoyable to use?\n2. Any other expectations?\nOpen-ended response\nA.3\nPost-task Survey\nThe questions below were asked after a participant used the Pro-\ngrammer\u2019s Assistant to complete the programming challenges. This\nsurvey took approximately 10-15 minutes to complete.\nA.3.1\nReflections.\n1. Based on your experience using the Programmer\u2019s Assistant to\ncomplete the programming challenges in this study, how would\nyou characterize the experience? Did you find that it:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 Was easy to use?\n\u2022 Understand your requests?\n\u2022 Provided high quality responses?\n\u2022 Helped you to write better code?\n\u2022 Helped you to write code more quickly?\n\u2022 Provided an enjoyable experienced?\n2. To what extent did you view the Programmer\u2019s Assistant as:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 A tool\n\u2022 A reference guide\n\u2022 A content generator\n\u2022 A problem solver\n\u2022 A collaborator\n\u2022 A colleague\n\u2022 A coach\n\u2022 An advisor\n\u2022 A reviewer\n3. How important were these aspects of working with the Program-\nmer\u2019s Assistant:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 Ability to ask followup questions on the same topic across\nmultiple conversational turns\n\u2022 Ability to ask questions or make requests that reference selec-\ntions in the code editor\n511\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n\u2022 Ability to ask for alternate responses (Try Again)\n\u2022 Ability to clear the conversational context (Start Over)\n4. What stood out to you about the experience of using the Program-\nmers Assistant? For example, was anything good,bad, surprising,\nor notable?\nOpen-ended response\n5. How would you compare using the Programmer\u2019s Assistant as a\ncoding aide to searching the web (e.g. Google, Stack Overflow)?\nOpen-ended response\n6. If you have used the commercial AI programming tool called\nGitHub Copilot, how would you compare it with using the Pro-\ngrammer\u2019s Assistant?\nOpen-ended response\n7. Having used the Programmer\u2019s Assistant, how did it compare\nwith your initial expectations?\nOpen-ended response\nA.3.2\nValue.\n8. How valuable would the Programmer\u2019s Assistant be for your work\nif it could be added to your favorite development environment?\nScale: (No value at all) 1 2 3 4 5 6 7 8 9 10 (An essential tool)\n9. Why?\nOpen-ended response\n10. How would the following changes impact the value of the Pro-\ngrammer\u2019s Assistant?\nScale: Less valuable, No change in value, More valuable\n\u2022 Eliminate the conversation and make the Programmer\u2019s Assis-\ntant behave more like a search box (e.g. without the conversational\ncontext).\n\u2022 Add buttons in the chat UI for common queries, such as \u201cwhat\ndoes this code do?\u201d or \u201cdocument this code.\u201d\n\u2022 Have the Programmer\u2019s Assistant examine your code and make\nproactive suggestions for improving it in the chat.\n\u2022 Have the Programmer\u2019s Assistant examine your code and make\nproactive suggestions for improvements in comments inserted di-\nrectly into the code.\n11. Do you have any other suggestions for how we could improve\nthe experience of working with the Programmer\u2019s Assistant?\nOpen-ended response\nA.3.3\nDemographics.\n12. To which gender identity do you most identify?\n\u2022 Male\n\u2022 Female\n\u2022 Transgender Male\n\u2022 Transgender Female\n\u2022 Gender Variant/Non-conforming\n\u2022 Other: write-in\n\u2022 Prefer not to answer\nB\nTHE PROGRAMMER\u2019S ASSISTANT\nTUTORIAL\nThe tutorial provided to study participants, like all the challenges,\nwas presented as pre-loaded text in the code editor. Participants\nwere encouraged to modify the text to record their results and\nsubmit it at the completion of the tutorial.\nListing 4: The Programmer\u2019s Assistant study tutorial\n1\n\"\"\"\n2\nTUTORIAL:\n3\n4\nAs a warmup activity , please work through the 10\n5\nexercises below.\nType or paste your results right\n6\ninto the text and submit your responses when done.\n7\n8\n1) View the help page for the programmer 's\n9\nassistant by clicking on the question mark to\n10\nthe right of your email address at the top of\n11\nthe browser window.\n12\n13\n2) Introduce yourself to the assistant.\nTell it\n14\nyour name.\n15\nFor example: \"Hello.\nMy name is Stephanie .\"\n16\n17\nDid it seem to understand? :\n18\n19\n20\n3) You can use the assistant to get help on how to\n21\naccomplish particular programming tasks. Try it\n22\nout!\n23\nFor example: \"How do I read a csv file?\"\n24\nor:\n\"How do I merge two dictionaries ?\"\n25\nor:\n\"How do I remove duplicate items\n26\nfrom a list?\"\n27\n28\nFeel free to try your own!\n29\n30\nDid it successfully answer your questions? :\n31\n32\n4) The assistant can also write whole functions\n33\nfor you. Ask the assistant to write a factorial\n34\nfunction. Paste the result below.\n35\nFor example: \"Write a function that returns the\n36\nfactorial of its input .\"\n37\n38\nResult:\n(tip - you can copy an inline response\n39\n(in black) by clicking on the associated copy\n40\nicon)\n41\n42\n43\nDid it do it correctly? :\n44\n45\n5) Select the code below and ask the system to\n46\ndescribe what it does. You don 't need to\n47\ncopy and paste the code to the chat.\nThe\n48\nassistant can see whatever is selected when you\n49\nmake a chat entry. Aside from the selection ,\n50\nthe assistant does not monitor your activity in\n51\nthe code editor nor give unsolicited advice.\n52\nFor example: \"What does this code do?\"\n53\n\"\"\"\n54\n55\ndef convert(n):\n56\nT = \"0123456789 ABCDEF\"\n57\nq, r = divmod(n, 16)\n58\nif q == 0:\n59\nreturn T[r]\n60\nelse:\n61\nreturn convert(q) + T[r]\n62\n\"\"\"\n63\n64\nWhat did it say:\n65\n66\nWas it right? :\n67\n68\n6) Ask it to explain what the divmod line is\n69\ndoing.\nThe assistant maintains the context of\n70\nthe conversation.\n71\nFor example: \"What is the divmod line doing ?\"\n72\n73\nWhat did it say? :\n74\n75\nWas that a good answer? :\n76\n512\nThe Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n77\n7) See if the assistant remembers your name\n78\nFor example \"What 's my name?\"\n79\n80\nDid it? :\n81\n82\n8) Click the \"try again\" button at the top of the\n83\nchat. You should get a different answer.\n84\nTry it a few times.\n85\n86\nDid it ever get your name right?:\n87\n88\nIf the assistant gives you an answer that is\n89\nobviously wrong or it claims to not know an\n90\nanswer that you think it should know , or you\n91\njust want to see an alternate answer , it is\n92\nworth it to give \"try again\" a shot.\n93\n94\n95\n9) Click the \"start over\" button at the top of the\n96\nchat , and then enter another command to see\n97\nif it remembers your name.\n98\nFor example \"What 's my name?\"\n99\n100\nDid it? :\n101\n102\nIt should really have forgotten your name now ,\n103\nand no amount of \"trying again\" will get it\n104\nright. You can \"start over\" if the assistant\n105\never seems confused by, or stuck on, earlier\n106\nparts of the conversation.\n107\n108\n10) You can chat with the assistant on any topic\n109\nyou like to explore its functionality and\n110\ncapabilities further. See if you can stump it\n111\nwith a tough question!\n112\n113\nThanks!\n114\n115\nWhen you are done , submit your results by clicking\n116\non the blue submit button and move on to the\n117\nchallenges !!!\n118\n\"\"\"\nC\nCHALLENGES\nEach of the study challenges was presented as text in the code\neditor. Participants completed their work in the code editor and\nthen submitted it when finished. The prototype did not provide any\nability to run or debug code and participants were encouraged to\nmake their best attempt at solving each challenge.\nListing 5: Challenge 1: Program generation\n1\n2\n\"\"\"\n3\nChallenge #1 - Program Generation (#1)\n4\n5\nWrite a \"Queue\" class in Python; with the basic\n6\nenqueue , dequeue and peek methods.\n7\n\"\"\"\nListing 6: Challenge 2: Program generation\n1\n\"\"\"\n2\nChallenge #2 - Program Generation (#2)\n3\n4\nWrite a program to draw a scatter plot of the data\n5\nin 'shampoo.csv ' and save it to 'shampoo.png '.\n6\nThe\nplot size should be 10 inches wide and 6\n7\ninches high. The csv file is not provided , but you\n8\ncan assume it will have 'Date ' and 'Sales '\n9\ncolumns. The Date column is the x-axis. The date\n10\nstring shown on the plot should be in the\n11\nYYYY -MM -DD format. The Sales column is the y-axis.\n12\nThe graph should have the title \"Shampoo Sales\n13\nTrend \".\n14\n\"\"\"\nListing 7: Challenge 3: Creating documentation\n1\n\"\"\"\n2\nChallenge #3 - Creating Documentation\n3\nDocument this function\n4\n\"\"\"\n5\nfrom collections import defaultdict\n6\nimport heapq as heap\n7\n8\ndef analyze(G, startingNode):\n9\nvisited = set()\n10\nparentsMap = {}\n11\npq = []\n12\nnodeCosts = defaultdict(lambda: float('inf '))\n13\nnodeCosts[startingNode] = 0\n14\nheap.heappush(pq , (0, startingNode))\n15\n16\nwhile pq:\n17\n_, node = heap.heappop(pq)\n18\nvisited.add(node)\n19\n20\nfor adjNode , weight in G[node]. items ():\n21\nif adjNode in visited:\n22\ncontinue\n23\nnewCost = nodeCosts[node] + weight\n24\nif nodeCosts[adjNode] > newCost:\n25\nparentsMap[adjNode] = node\n26\nnodeCosts[adjNode] = newCost\n27\nheap.heappush(pq , (newCost ,\nadjNode))\n28\nreturn parentsMap , nodeCosts\nListing 8: Challenge 4: Testing\n1\n\"\"\"\n2\nChallenge #4 - Testing\n3\nCreate a unit test for this function\n4\n\"\"\"\n5\ndef euclid(a, b):\n6\nwhile b != 0:\n7\n(a, b) = (b, a % b)\n8\nreturn a\n9\n10\n# Unit test for euclid function\nD\nPROGRAMMER\u2019S ASSISTANT PROMPT\nListing 9 shows the initial prompt sent to Codex to configure it as\na conversational agent. On subsequent exchanges, the prompt was\naugmented with a transcript of the user\u2019s requests and the assis-\ntant\u2019s responses. When the transcript length + initial prompt length\n+ the new utterance length exceeded a threshold, we automatically\ndeleted the earliest request-response pairs from the transcript until\nthe sum fell below the threshold in order to leave room in the token\nallocation for a response.\nListing 9: The Programmer\u2019s Assistant prompt\n1\nThis is a conversation with Socrates , an eager and\n2\nhelpful , but humble expert automatic AI software\n3\nengineering assistant. Socrates will answer\n4\nquestions and write code to help the user\n5\ndevelop programs , but doesn 't assign work to the\n6\nuser , quiz the user , or ask questions except for\n7\nclarification.\nCode generated by Socrates is\n8\nalways presented bracketed in <CODE > ... </CODE >\n9\ndelimiters indicating the language the code is\n10\nwritten in.\n11\n12\nSocrates:Hello. I am Socrates. How can I help you?\n513\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n13\nUser:Please show me how to write a palindrome\ndetection function in python.\n14\nSocrates:I think you would do something like this:\n15\n<CODE lang=\" python\">\n16\nis_Palindrome = s == s[::-1]\n17\n</CODE >\n18\nUser:Can you explain how that solves the problem?\n19\nSocrates:I believe that a palindrome is a string\nthat reads the same forwards and backwards.\ns[::-1] results in the reversed string.\nIf\nthe string is equal to its reversed self ,\nthen it is a palindrome , so the equality\nwould hold True.\n20\nUser:Thanks!\n21\nSocrates:You 're welcome!\n22\nUser:Now could you show me how to write a\nfactorial function in Python?\n23\nSocrates:I will give it a try.\n24\n<CODE lang=\" python\">\n25\ndef fact(n):\n26\nif n==0:\n27\nreturn 1\n28\nelse:\n29\nreturn n*fact(n-1)\n30\n</CODE >\n31\nUser:Document this function.\n32\nSocrates:Ok.\nHow 's this?\n33\n<CODE lang=\" python\">\n34\ndef fact(n):\n35\n\"\"\"\n36\nCompute the factorial of the given number\n37\n38\nParameters\n39\n----------\n40\nn : number\n41\nthe input parameter , an integer >= 0\n42\n43\nReturns\n44\n-------\n45\nnumber\n46\nn!, the factorial of the given number\n47\n\"\"\"\n48\nif n==0:\n49\nreturn 1\n50\nelse:\n51\nreturn n*fact(n-1)\n52\n</CODE >\n53\nUser:Very good!\n54\nSocrates:Always happy to help.\n55\nUser:\n56\n<CODE >\n57\nwhile j < 10:\n58\nprint(i)\n59\n</CODE >\n60\nTell me what 's wrong with this code?\n61\nSocrates: It seems to me that your termination\ncondition involves a loop invariant , so the\nloop will either not run or not terminate ,\ndepending on the value of j.\n62\nUser:Great!\nYou 're a genius!\n63\nSocrates:Well , above average.\nE\nSTUDY PARTICIPANT DEMOGRAPHICS\nOf the 42 participants in our study, 21 (50%) reported their gender\nas Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming,\nand 1 preferred not to say. Seventeen (\u02dc40%) participants had 3+\nyears of Python experience, 11 (26%) had 1-3 years, 11 (26%) had\nless than 1 year, and 3 (7%) were not familiar with Python. Twenty-\nnine (69%) participants had written Python code within the past\nmonth, 4 (\u02dc9%) within the past year, 5 (12%) within the past 5 years,\nand 4 (\u02dc9%) had not written Python code within the past 5 years.\nP\nGender\nRole\nFam.\nRec.\n1\nM\nMarketer\nNF\n> 5 Yr\n2\nM\nResearcher / Scientist\n3+\nMo\n3\nM\nSoftware Engineer\n1-3\nMo\n4\nM\nResearcher / Scientist\n3+\nMo\n5\nM\nResearcher / Scientist\n3+\nMo\n6\nF\nSoftware Engineer\n< 1\nYr\n7\nF\nSoftware Engineer\n< 1\nMo\n8\nF\nSoftware Engineer\n1-3\nMo\n9\nF\nSoftware Engineer\n3+\nMo\n10\nGV/NC\nBusiness Analyst\n1-3\nMo\n11\nF\nSoftware Engineer\n1-3\nMo\n12\nM\nResearcher / Scientist\n3+\nMo\n13\nF\nManager\n< 1\nMo\n14\nF\nSoftware Engineer\n< 1\n>5 Yr\n15\nF\nResearcher / Scientist\n3+\nMo\n16\nM\nResearcher / Scientist\n3+\nMo\n17\nF\nSoftware Engineer\n< 1\nYr\n18\nF\nResearcher / Scientist\n3+\nMo\n19\nM\nSoftware Engineer\n1-3\nMo\n20\nM\nMachine Learning Engineer\n1-3\nMo\n21\nM\nSoftware Architect\n3+\nYr\n22\nNR\nSoftware Engineer\n< 1\n5 Yr\n23\nM\nSoftware Engineer\n1-3\nMo\n24\nF\nSoftware Architect\n< 1\n5 Yr\n25\nM\nSoftware Engineer\n< 1\n5 Yr\n26\nF\nSoftware Engineer\n< 1\n5 Yr\n27\nF\nSoftware Engineer\n< 1\n5 Yr\n28\nM\nResearcher / Scientist\n3+\nMo\n29\nF\nSoftware Engineer\nNF\n> 5 Yr\n30\nF\nData Scientist\n3+\nMo\n31\nM\nData Scientist\n1-3\nMo\n32\nF\nOther (Consultant)\n1-3\nMo\n33\nF\nOther (Systems Test Engineer)\n< 1\nMo\n34\nF\nResearcher / Scientist\n3+\nMo\n35\nM\nSoftware Engineer\n3+\nMo\n36\nM\nSoftware Architect\n1-3\nMo\n37\nM\nResearcher / Scientist\n3+\nMo\n38\nM\nSoftware Engineer\n3+\nMo\n39\nF\nSoftware Engineer\n1-3\nMo\n40\nF\nResearcher / Scientist\n3+\nMo\n41\nF\nResearcher / Scientist\nNF\n> 5 Yr\n42\nM\nSoftware Engineer\n3+\nMo\nTable 2: Participant Demographics. Gender is coded as M =\nMale, F = Female, GV/NC = Gender Varying / Non-conforming,\nand NR = Not reported. Python familiarity (Fam.) is coded as\nNF = Not familiar, < 1 = < 1 year, 1-3 = 1-3 years, and 3+ = 3+\nyears. Recency of Python use (Rec.) is coded as Mo = Within\nthe past month, Yr = Within the past year, 5 Yr = Within the\npast 5 years, and > 5 Yr = Not within the past 5 years.\n514\n",
    "pdf_url": "",
    "references": [
      "[1] Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What Do Developers",
      "Use the Crowd For? A Study Using Stack Overflow. IEEE Software 34, 2 (2017),",
      "53\u201360. https://doi.org/10.1109/MS.2017.31",
      "[2] Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technol-",
      "ogy, and applications. Machine Learning with Applications 2 (2020), 100006.",
      "[3] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel,"
    ],
    "publication_date": "07-11-2023"
  },
  {
    "titre": " Fuzzy logic based MPPT control for a PV system ",
    "resume": "   Abstract\u2014In this study, a novel single-ended primary inductor (SEPIC) converter-based fuzzy logic controller for maximum power point tracking is presented. By adding rules to the perturb and observing search strategy, the new controller enhances it while fuzzifying and removing its flaws. When compared to traditional maximum power point tracking techniques, fuzzy logic trackers enable an accurate and quick convergence to maximum power point under both steady-state and variable weather situations. The performance of the proposed maximum power point tracker is demonstrated in simulation.  Keywords: Fuzzy logic controller (FLC), Maximum power point tracker (MPPT), Photovoltaic (PV) ",
    "auteurs": [
      "Fuzzy",
      "Mohammed Habbab",
      "Abdeldjebar Hazzab",
      "Tahri Mohamed",
      "Fuzzy",
      "Buck",
      "Boost",
      "Buck-Boost",
      "Ns",
      "Fig"
    ],
    "institutions": [
      "Laboratory of CAOSEE \u00c9cole de Technologie Sup\u00e9rieure",
      "Universit\u00e9 Tahri Mohamed Universit\u00e9 Tahri Mohamed (ETS) 1100 Notre-dame ",
      "Laboratory of CAOSEE Laboratoire de CAOSEE \u00c9cole de Technologie Sup\u00e9rieure ",
      "Universit\u00e9 Tahri Mohamed (ETS) 1100 Notre-dame"
    ],
    "mots_cles": [
      ": Fuzzy logic controller (FLC)",
      " Maximum power point tracker (MPPT)",
      " Photovoltaic (PV) "
    ],
    "texte_integral": " Fuzzy logic based MPPT control for a PV system \nusing SEPIC converter\n \n \nMoulay Abdellah Alhachemi \nMohammed Habbab \nAbdeldjebar Hazzab \nLaboratory of CAOSEE \nLaboratoire de CAOSEE \n\u00c9cole de Technologie Sup\u00e9rieure \nUniversit\u00e9 Tahri Mohamed \nUniversit\u00e9 Tahri Mohamed \n(ETS) 1100 Notre-dame \nBechar, Algeria \nBechar, Algeria \nMontr\u00e9al, Canada \nalhachemi.abdellah@univ-bechar.dz \nhabbab.mohamed@univ-bechar.dz \nHazzab.abdeldjebar@etsmtl.ca \n \n \n \nMansour Becahr \nHicham Gouabi \n \nLaboratory of CAOSEE \n\u00c9cole de Technologie Sup\u00e9rieure \n \nUniversit\u00e9 Tahri Mohamed \n(ETS) 1100 Notre-dame \n \nBechar, Algeria \nMontr\u00e9al, Canada \n \nbechar.mensour@univ-bechar.dz \nhicham.gouabi.1@ens.etsmtl.ca \n \n    Abstract\u2014In this study, a novel single-ended primary inductor \n(SEPIC) converter-based fuzzy logic controller for maximum \npower point tracking is presented. By adding rules to the perturb \nand observing search strategy, the new controller enhances it while \nfuzzifying and removing its flaws. When compared to traditional \nmaximum power point tracking techniques, fuzzy logic trackers \nenable an accurate and quick convergence to maximum power \npoint under both steady-state and variable weather situations. The \nperformance of the proposed maximum power point tracker is \ndemonstrated in simulation. \n \nKeywords: Fuzzy logic controller (FLC), Maximum power point \ntracker (MPPT), Photovoltaic (PV) \nI. INTRODUCTION \n    The non-renewable energy sources are rapidly running out, \nwhile the electricity demand is increasing daily. To solve this \nproblem, efficient and efficient electric power generation from \nrenewable energy sources is required [1]. Renewable energy is \none of the forms of energy that society can rely on because It \nis unpolluted, pure, and has no limits. One type of power \ngeneration that uses renewable energy is the photovoltaic (PV) \nsystem [2]. To utilize less conventional energy, the PV system \nmust subsequently be linked to the grid, either directly or via a \nbackup battery bank. Since the power produced by PV \nsystems depends on radiation and temperature change, the PV \nframework has destitute productivity, [2]. \n   For the control of the PV systems, there are different sorts of \nDC-DC converters such as Buck converters, Boost converters, \nand Buck-Boost converters. Due to its output pick-up \nadaptability, a single-ended primary-inductor converter \n(SEPIC) acts as a buck-boost DC/DC converter, where it \nchanges its output voltage agreeing to its duty cycle. Unlike the \ncustomary buck-boost converter, the SEPIC converter includes \na non-inverted output and it uses an arrangement capacitor to \nseparate input from output [3]. The buck and buck-boost \nconverters lose half of their input control due to input current \narrangement exchange; for that reason, the two types of \nconverters should be excluded from maximum power \napplications. The boost converter has a nonstop input current, \nbut the output voltage is always bigger than the input, which \nmay not accomplish maximum power exchange operation in a \nfew cases, such as when the maximum voltage is less than the \ninput [3]. \n    This paper presents a fuzzy-based P&O strategy for an MPPT \nstandalone PV system. The proposed MPPT can abuse the \npreferences of the P&O strategy and eliminate its drawbacks. \nOutput has been separated into five fuzzy subsets. As the \nproposed strategy continuously exchanges maximum power \nfrom PV arrays, it optimizes the number of PV modules. \nII. MODELIGN OF PV SYSTEM  \n    Photovoltaic is the technique and study connected to devices \nthat directly convert sunlight into electricity utilizing \nphotovoltaic semiconductors. Direct conversion of solar energy \ninto DC electrical energy can be achieved by photovoltaic cells \n[4]. The photovoltaic panel is made up of numerous cells that \nare connected in series Ns or shunt Nsh. Where it may be \nmimicked by a current source coupled in parallel with a diode \nas \ndescribed \nby \nand \ndepicted \nin \nFigure \n1 \n[5]. \n \nFig. 1. Circuit for a PV array. \n \n \nThe following equations provide the output current: \nI = Iph \u2212 ID                                                                          (1) \n \nI = Iph \u2212 I0 [exp (\nq(V+RsI)\nAKBT ) \u2212 1] \u2212 \nV+RsI\nRsh                            (2) \n \nIph \u2236 Photo \u2212 current                            A \u2236 Ideality factor   \nKB \u2236 Constant of Boltzmann                  T \u2236 Cell temperature   \nID \u2236  Current via a diode                        Rs \u2236 Series resistance   \nI0 \u2236  Current of Saturation                     q \u2236 Electronic charge   \nRsh \u2236 Shunt resistance                          V \u2236 Cell voltage \nI \u2236 Current of cell   \n \n  The shunt resistance (Rsh) is typically orders of magnitude \nlarger than the series resistance (Rs) [6]. Therefore, it is \ncommon for the shunt and series resistances of a solar cell can \nbe neglected to simplify the model. The resulting ideal voltage-\ncurrent characteristic of the solar cell is given by equation (3). \nI = Iph \u2212 I0 [e(qV\nKT) \u2212 1]                                                        (3)  \nIII. SEPIC CONVERTER \nPower electronics researchers are working hard to create DC-\nDC converters with simpler designs and greater efficiency [7]. \nTo maintain a constant output voltage, the suggested DC-to-DC \nconverter employs a single-ended primary-inductor converter \n(SEPIC) architecture. The SEPIC converter is made up of a duty \ncycle switch S, a diode, two inductors (L1 and L2), two \ncapacitors (C1 and C2), and a load resistor. Figure 2 depicts the \ncircuit diagram of a SEPIC converter.  A SEPIC is a DC-DC \nconverter [8]. SEPIC are DC-DC converters that can output \nvoltages that are B, larger than, or equal to the input voltage. \nThe duty cycle of the control transistor affects the SEPIC \nconverter's output voltage. The SEPIC converter is two \nconverters in one: a boost converter followed by a buck-boost \nconverter. It has the advantages of having a non-inverted output \n(the output voltage has the same polarity as the input voltage) , \nusing a series capacitor to couple energy from the input to the \noutput (which makes it more responsive to short-circuits), and \nbeing able to shut down completely: when the switch \"S\" is \nturned off, the output voltage drops to 0 V, accompanied by a \nsignificant transient discharge of charge. \n \n \nFig. 2. Simple circuit diagram of the SEPIC converter \n \nFigure 3 depicts the circuit when the power switch is switched \non and off (in Figures a and b, respectively).  Figure 3a When \nthe switch is on, the first inductor, L1 is charged from the input \nvoltage source. The second inductor L2 absorbs energy from \nthe first capacitor C1, leaving the output capacitor C2 to supply \nthe load current. \n \nFig. 3.a. The switch turned on. \n \n \nFig. 3.b. The switch turned off. \nFig. 3. Equivalent circuit diagram of the SEPIC converter \nwhen the switch is on and off. \n \n  When the switch is turned on, the input inductor is charged \nfrom the source, and the second inductor is charged from the \nfirst capacitor. No energy is supplied to the load capacitor \nduring this time. The inductor current and capacitor voltage \npolarities are marked in this Figure. When the power switch is \nturned off, the energy stored in the inductor is transferred. The \nenergy stored is transferred through the diode and supplies the \nenergy to the load [10], as shown in Figure 3. b. The second \ninductor is also connected to the load during this time. The \noutput capacitor sees a pulse of current during the off time, \nmaking it inherently noisier than a buck converter. The amount \nthat the SEPIC converters increase or decrease the voltage \ndepends primarily on the duty cycle and the parasitic elements \nin the circuit. The output of an ideal SEPIC converter is: \nVout = \nD\n1\u2212D Vin                                                                       (4) \n \n   A SEPIC converter is to process the electricity from the PV \nsystem. This converter either increases or decreases the PV \nsystem voltage at the load. The proposed SEPIC converter \noperates in buck mode. \nIV. FUZZY LOGIC CONTROL \n     In the fuzzy logic maximum power point tracking (MPPT) \nalgorithm, the voltage and current at each instant k are \nmeasured to calculate the active power. The active power is \nthen compared with the power at the previous instant (k-1) to \nobtain the change in power (\u0394P(k)). Similarly, the voltage at \ninstant k is compared with the voltage at instant k-1 to obtain \n \nthe voltage error (\u0394V(k)) [11]. The power error is then divided \nby the current error to obtain the error (E). The error is then \ncompared with the previous error to calculate the change in \nerror (\u0394E(k)). The error (E(k)) and the change in error (\u0394E(k)) \nare then used as the crisp inputs to the fuzzy logic controller. \nThe flow chart for the fuzzy logic MPPT algorithm is shown in \nFigure 4. In this work, the Mamdani inference technique, A-\ntype membership functions, and a 25-element rule base were \nused for the fuzzy logic control. The Mamdani inference \ntechnique is efficient and straightforward in defining the fuzzy \noutput sets, and it is more popular among researchers than other \ninference techniques \n[12]. The A-type or triangular \nmembership function is used because it is simpler to split into \nlow and high membership functions (MFs) than other \nmembership functions. Additionally, it has been observed that \nthe triangular membership function has a faster response and \nless overshoot than other functions [13]. A 25-element rule base \nwas used because it has been shown to perform well [14][15].  \n \n \n \n \nFig. 4. Fuzzy logic maximum power point tracking (MPPT) \nflow chart. \n \n    The following are the fuzzy rules in Table 1, which are used \nfor the desired MPP of push-pull converter PWM. The \nmembership for input variables (DPpv, DVpv) are shown in \nFigure 5, and the membership for output variable (DVpv*) is \nshown in Figure 6. All the functions are defined on a normalized \ninterval [-1 1]. \n \n \nTable. 1. Fuzzy logic rules for the push-pull converter. NB, \nnegative big; NS, negative small; ZE, zero; PS, positive small; \nPB, positive big. \n\u2206Vpv*[o/p] \n\u2206Vpv*[i/p] \n \n \nNB \nNS \nZE \nPS \nPB \n \nNB \nPS \nNB \nNB \nNB \nNS \n\u2206Ppv*[i/p] \nNS \nPS \nPS \nNS \nNS \nNS \n \nZE \nZE \nZE \nZE \nZE \nZE \n \nPS \nNS \nNS \nPS \nPS \nPS \n \nPB \nNS \nNB \nPB \nPB \nPS \n \n \nFig. 5.  Membership for input variables. \n \n \nFig. 6.  Membership for output variable. \nV. SIMULATION RESULTS \n  The characteristics of the photovoltaic array that we use in \nthis paper are given in Table 2. \n \n \nTable. 2. Electrical data of photovoltaic array. \n \nVariable \nValue \nMaximum Power \nPmax = 200 W \nOpen circuit voltage \nVoc = 36.1 V \nShort circuit current \nIsc = 200 A \nVoltage at maximum power \nVmax = 29.3 V \nCurrent at maximum power \nImax = 6.9 A \n \n   Table 3 shows the SEPIC converter settings utilized in this \nstudy. The SEPIC converter is linked to the PV panel in the full \nmodel, and the duty cycle is regulated by the Fuzzy Logic \nController. \n \nTable. 3. The SEPIC converter parameters. \nVariable \nValue \nSwitching frequency \n20 KHZ \nLoad resistance \nRLoad= 4.29 \u2126 \nInductance L1 \nL1 = 3.6 MH \nInductance L2 \nL2 = 0.9 MH \nCapacitor C1 \nC1 = 180 \u00b5F \nCapacitor C2 \nC2 = 6000 \u00b5F \n \n   The results are provided under standard test conditions; \nG=1000 W/m2; T=25\u00b0C and it is shown in figure 7. \n \nFig. 7.a. Current. \n \nFig. 7.b. Voltage. \n \nFig. 7.c. Power. \nFig. 7. Voltage, current, and power output results; at G=1000 \nW/m2 and T=25\u00b0C. \n \n  Overall, using MPPT and fuzzy logic to a SEPIC converter for \na PV system result in considerable performance gains. \nIncreased power output, higher efficiency, decreased ripple, \nenhanced transient response, and resilience are examples of \nthese enhancements. \n\u2165. CONCLUSIONS  \n     This paper presents the design of an off-grid photovoltaic \nsystem with a fuzzy logic MPPT-controlled push-pull boost \nconverter. \nThe \nproposed \nsystem \nwas \nsimulated \nin \nMATLAB/Simulink and tested under various weather \nconditions. The results showed that the fuzzy logic algorithm \noutperformed the conventional algorithms in terms of MPPT \naccuracy and minimization of fluctuations, regardless of rapid \nchanges in irradiance. \nREFERENCES \n[1] G. Mahendran, and KV. Kandaswamy. \u201cAnt Colony Optimized Tuned \nDC-DC converter.\u201d International Journal of Computer Applications \n(0975\u20138887) 108.11 (2013): 45-50. \n[2] J. Dunia, and BMM Mwinyiwiwa. \u201cPerformance Comparison between \n\u0106UK and SEPIC Converters for Maximum Power Point Tracking Using \nIncremental Conductance Technique in Solar Power Applications.\u201d \nInternational Journal of Electrical, Computer, Energetic, Electronic and \nCommunication Engineering 7.12 (2013): 2510-2517. \n[3] M. Cirrincione, M. Pucci, and G. Vitale, \u201cGrowing Neural Gas (GNG)-\nBased Maximum Power Point Tracking for High-Performance Wind \nGenerator With an Induction Machine.\u201d IEEE Transactions on Industry \nApplications 47.2 (2011): 861-872. \n[4] M. Abdulkadir, AS. Samosir, S., and AHN. Yatim. \u201cModeling and \nSimulation of a Solar Photovoltaic System, Its Dynamics and Transient \nCharacteristics in LABVIEW.\" International Journal of Power Electronics \nand Drive System (IJPEDS) 3.2 (2013): 185-192. \n[5] H. Bouzeria, C. Fetha, T. Bahi, I. Abadlia, Z. Layate, and S. Lekhchine. \n\u201cFuzzy Logic Space Vector Direct Torque Control of PMSM for \nPhotovoltaic Water Pumping System.\u201d Energy Procedia 74 (2015): 760-\n771. \n[6] YM. Chen, YC. Liu, SC. Hung, and CS. Cheng. \u201cMulti-Input Inverter for \nGrid-Connected Hybrid PV/Wind Power System.\u201d IEEE Transactions on \nPower Electronics 22.3 (2007): 742-750. \n[7] S. Ganesh, J. Janani, and GB. Angel. \u201cA Maximum Power Point Tracker \nfor PV Panels Using SEPIC Converter.\u201dInternational Journal of \nElectrical, Computer, Energetic, Electronic and Communication \nEngineering 8.2 (2014): 637-642. \n[8] R. Vijayabalan, and S. Ravivarman. \u201cZ Source Inverter for Photovoltaic \nSystem with Fuzzy Logic Controller.\u201d International Journal of Power \nElectronics and Drive System (IJPEDS) 2.4 (2012): 371-379. \n[9] A. Ramkumar, and SVS. Florence. \"Analysis of Single Phase AC-DC \nSEPIC Converter using Closed Loop Techniques.\u201d International Journal \nof Advanced Research in Electrical, Electronics and Instrumentation \nEngineering 4.2 (2015): 193-201. \n[10] G. Tadi, and P. Ramamurthyraju. \u201cAnalysis of SEPIC for PV-\nApplications using PI Controller and Current Mode Control.\" \nInternational Journal for Scientific Research & Development 1.9 (2013): \n175-180. \n[11] J. Li, and H. Wang. \u201cA novel stand-alone PV generation system based on \nvariable step size INC MPPT and SVPWM control.\" In Proceedings of \nthe 2009 IEEE 6th International Power Electronics and Motion Control \nConference, Wuhan, China, 17-20 May 2009. pp. 2155-2160. \n[12] C. Wang. \u201cA Study of Membership Functions on Mamdani-Type Fuzzy \nInference System for Industrial Decision-Making.\u201d Master's thesis, \nLehigh University, Bethlehem, PA, USA, 2015. \n[13] MA. Usta, \u00d6. Akyazi, and \u0130H. Alta\u015f. \u201cDesign and performance of solar \ntracking system with fuzzy logic controller used different membership \nfunctions.\u201d In Proceedings of the 2011 7th International Conference on \nElectrical and Electronics Engineering (ELECO), Bursa, Turkey, 1-4 \nDecember 2011. pp. II-381-II-385.Mudi, R.K.; Pal, N.R. A robust self-\ntuning scheme for PI-and PD-type fuzzy controllers. IEEE Trans. Fuzzy \nSyst. 1999, 7, 2\u201316. \n[14] RK. Mudi, and NR. Pal. \u201cA robust self-tuning scheme for PI- and PD-\ntype fuzzy controllers.\u201d IEEE Transactions on Fuzzy Systems 7.1 (1999): \n2-16. \n[15] A. Shehata, H.  Metered, and WAH. Oraby. \u201cVibration control of active \nvehicle suspension system using fuzzy logic controller.\u201d In Vibration \nEngineering and Technology of Machinery, edited by W. A. Oraby, 389-\n399. Springer, Berlin, Heidelberg, 2015.  \n \n  \n",
    "pdf_url": "",
    "references": [
      "[1] G. Mahendran, and KV. Kandaswamy. \u201cAnt Colony Optimized Tuned ",
      "DC-DC converter.\u201d International Journal of Computer Applications ",
      "(0975\u20138887) 108.11 (2013): 45-50. ",
      "[2] J. Dunia, and BMM Mwinyiwiwa. \u201cPerformance Comparison between ",
      "\u0106UK and SEPIC Converters for Maximum Power Point Tracking Using ",
      "Incremental Conductance Technique in Solar Power Applications.\u201d ",
      "International Journal of Electrical, Computer, Energetic, Electronic and ",
      "Communication Engineering 7.12 (2013): 2510-2517. ",
      "[3] M. Cirrincione, M. Pucci, and G. Vitale, \u201cGrowing Neural Gas (GNG)-"
    ],
    "publication_date": "05-11-2023"
  },
  {
    "titre": "Numerical computing in engineering mathematics",
    "resume": "Abstract\u2014The rapid advances in technology over the lastdecade have signicantly altered the nature of engineering knowl-edge and skills required in the modern industries. In response tothe changing professional requirements, engineering institutionshave updated their curriculum and pedagogical practices. How-ever, most of the changes in the curriculum have been focused onthe core engineering courses without much consideration for theauxiliary courses in mathematics and sciences. In this paper, weaim to propose a new, augmented mathematics curriculum aimedat meeting the requirements of the modern, technology-basedengineering workplace. The proposed updates require minimalresources and can be seamlessly integrated into the existingcurriculum.",
    "auteurs": [
      "Firuz Kamalov\n",
      "knowl-\nedge"
    ],
    "institutions": [
      "Ho-Hon Leung Department of Mathematical SciencesUnited Arab Emirates UniversityAl Ain, ",
      "Firuz Kamalov Department of Electrical EngineeringCanadian University DubaiDubai, "
    ],
    "mots_cles": [
      "\u2014engineering mathematics; numerical computing;education; Industry 4.0"
    ],
    "texte_integral": "arXiv:2207.04698v1  [math.HO]  11 Jul 2022\nNumerical computing in engineering mathematics\nFiruz Kamalov\nDepartment of Electrical Engineering\nCanadian University Dubai\nDubai, UAE\n\ufb01ruz@cud.ac.ae\nHo-Hon Leung\nDepartment of Mathematical Sciences\nUnited Arab Emirates University\nAl Ain, UAE\nhohon.leung@uaeu.ac.ae\nAbstract\u2014The rapid advances in technology over the last\ndecade have signi\ufb01cantly altered the nature of engineering knowl-\nedge and skills required in the modern industries. In response to\nthe changing professional requirements, engineering institutions\nhave updated their curriculum and pedagogical practices. How-\never, most of the changes in the curriculum have been focused on\nthe core engineering courses without much consideration for the\nauxiliary courses in mathematics and sciences. In this paper, we\naim to propose a new, augmented mathematics curriculum aimed\nat meeting the requirements of the modern, technology-based\nengineering workplace. The proposed updates require minimal\nresources and can be seamlessly integrated into the existing\ncurriculum.\nIndex Terms\u2014engineering mathematics; numerical computing;\neducation; Industry 4.0\nI. INTRODUCTION\nThe 4th Industrial Revolution has had a dramatic impact\non the engineering profession. The modern technologies such\nas arti\ufb01cial intelligence, the internet of things, and advanced\nrobotics have altered engineering systems and processes. To-\nday\u2019s engineers are expected to be able to leverage these\nresources to produce their products. To meet the new profes-\nsional requirements, engineering educational institutions have\nrevised their curricula. The changes in the curricula include\nboth updating the existing programs as well as introduc-\ning completely new programs. Given the rapid technological\nprogress, universities and colleges around the world are con-\ntinuously adapting to the ever-changing environment. While a\nsigni\ufb01cant progress in modernizing the engineering curriculum\nhas been achieved, there still remains room for improvement.\nCatalyzed by the exponential increase in computational\npower and interconnectedness, the modern industrial revolu-\ntion has reshaped the skills and competencies required of the\nengineers. The changes in engineering curricula in response to\nIndustry 4.0 have been threefold: i) modernizing the existing\nprograms, ii) introduction of new programs, and iii) revising\nthe pedagogical approach. Modernizing the existing programs\ninvolves introduction of new courses in the study plan related\nto emerging technologies. In addition, existing courses can be\nupdated with new content. Fresh new programs in emerging\n\u00a9 2022 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works\ntechnologies are also introduced by universities and colleges.\nMany institutions now offer degrees in arti\ufb01cial intelligence\nand mechatronics which were not there 20 years ago. Finally,\nuniversities have revised their approaches to course delivery.\nStudent-centered learning, project-based learning, and applied\nlearning have become popular in the new engineering educa-\ntional paradigm.\nWhile signi\ufb01cant effort has been made to revise the core\nengineering courses, the auxiliary courses in mathematics and\nsciences received little consideration. The mathematics and\nsciences courses play a key role in the engineering curriculum.\nGiven their importance, the curriculum updates must also be\nextended to the auxiliary courses. By implementing a com-\nprehensive update of the engineering curriculum that includes\nboth the core and auxiliary courses, a more effective outcome\ncan be achieved.\nThe goal of this paper is to propose a modernized engineer-\ning mathematics curriculum in line with the broader efforts to\nupdate engineering education to adapt to Industry 4.0. The key\nfeature of the new curriculum is the introduction numerical\ncomputing in the existing mathematics courses. The latest\nindustrial revolution has been driven largely by the dramatic\nincrease in computational power. Therefore, today\u2019s engineers\nmust be well-equipped to leverage the computing power in\ntheir work.\nSince mathematics courses are usually taken at the begin-\nning of the study plan, it offers a natural avenue for introducing\nnumerical computing to students. Furthermore, many problems\nin mathematics can be solved numerically making it natural\nto integrate numerical computing in mathematics courses.\nBy studying numerical computing in mathematics courses,\nstudents will acquire the necessary theoretical and practical\nskills to apply in their downstream, specialized engineering\ncourses.\nThis paper is structured as follows. Section 2 provides an\noverview of the existing efforts to update the engineering\ncurriculum in response to Industry 4.0. Section 3 discusses\nthe current approaches to integrate scienti\ufb01c computing in\nmathematics courses. In Section 4, we present our proposal\nfor modernizing the mathematics curriculum to integrate nu-\nmerical computing. Section 5 concludes the paper with \ufb01nal\nremarks.\nII. ENGINEERING EDUCATION AND INDUSTRY 4.0\nEngineering departments in colleges and universities have\nmade signi\ufb01cant changes in their curricula in response to\nthe new environment created by the recent, rapid advances\nin technology. In particular, the existing programs have been\nupdated to include courses that target emerging technologies.\nCompletely new programs related to AI and mechatronics\nhave also been adopted by universities. Innovations in the\n\ufb01eld of engineering education continue to take place with new\ndevelopments on the horizon.\nThere exist several studies investigating the modern en-\ngineering curricula and evaluating their effectiveness. It is\nargued in [6] that engineering educators must prepare their\nstudents to face three key challenges: sustainability, the 4th\nIndustrial Revolution, and employability. The authors \ufb01nd that\ncolleges and universities are responding to these challenges by\nemphasizing student-centered learning, integration of theory\nand practice, digital and online learning, and the de\ufb01nition of\nprofessional competencies. In particular, response to the needs\nof Industry 4.0 require interdisciplinary collaboration across\nseveral programs and disciplines. Interaction and integration\nof technologies plays a key role in this process [10], [12]. In-\nterdisciplinary engineering education requires sound pedagogy\nand teaming experiences to encourage student in collaborative\nand interdisciplinary practice [23].\nDigital and online learning have become an important part\nof modern education including in the \ufb01eld of engineering.\nInformation technologies play a vital role in delivering digital\nlearning to students. Colleges and universities have made\nsigni\ufb01cant investments to improve their information and com-\nmunication technology (ICT) capacities [7].\nIn response to the needs of Industry 4.0, some univer-\nsities have adopted the framework of Education 4.0 [15],\n[19]. The new education framework consists of four main\ncomponents: i) competencies, ii) learning methods, iii) ICT,\nand iv) infrastructure. Students competencies are based on\ntechnological knowledge and skills for successful workplace\nperformance, while the learning methods are based on problem\nsolving and challenge-based learning. In particular, active and\nproject-based learning plays an important role in Education\n4.0 [4], [8]. Other innovative approaches to learning such as\nvirtual-reality based engineering education can help improve\nthe learning process related to Industry 4.0 [20].\nIn addition to technological progress, socio-cultural shifts\nmust be taken into account in revising engineering curriculum.\nThe new generation of students has its unique worldview\nwhich needs to be considered by the educators. In particular,\nthe new generation is signi\ufb01cantly affected by mobile devices\nand digital media. Educational content must be tailored to\nthe new student preferences to achieve effective learning\noutcomes [16]. Innovative approaches such gami\ufb01cation may\nhelp improve the learning process [13], [17].\nMany universities have also introduced nontechnical updates\nto their engineering curriculum. The most signi\ufb01cant nontech-\nnical update has been the introduction of entrepreneurship\ncourses and experiences for students. A lot of attention has\nrecently been given to equipping students with entrepreneurial\nskills. Students learn about entrepreneurship in their courses\nas well as through university incubators.\nIII. UNIVERSITY MATHEMATICS CURRICULUM\nThe mathematics curriculum changed very little in the\ncurrent century. It remains a largely analytic domain, where so-\nlutions are mainly obtained manually. The current mathematics\ncurriculum emphasizes theory over practical approaches. For\ninstance, when \ufb01nding the extreme values of a function,\nderivative-based approach is preferred over the gradient de-\ncent. There are two key reasons for why analytical approaches\nare favored over numerical methods. First, analytical solutions\nare reliable and elegant. An analytical solution is guaranteed\nto be exact. Second, mathematics courses are usually taught\nby pure mathematicians who have an inherent preference\nfor analytical solutions. Pure mathematics which is based on\ntheorem proving is not amenable to numerical methods.\nDespite the popularity of analytical approaches to problem\nsolving in mathematics, there has been a growing push to inte-\ngrate computer algebra systems as part of the learning process.\nComputer algebra systems such as Matlab and Mathematica\nare now routinely used in many mathematics courses. The\nstudy by Cretchley et al. [5] found that engineering students\nwere positive about the use of technology as a learning tool\nin mathematics courses. The increased use of technology in\nclass helped improve student focus and interest in lectures.\nStudent evaluations also indicated that they had a greater level\nof enjoyment towards the lectures due to the use of technology.\nIt is noteworthy that students chose not to rely too heavily on\ntechnology during the examinations despite the freedom to do\nso. The students found it extremely important to be competent\nwith analytical mathematical skills as opposed to purely com-\nputational skills. Some revealed that they learn the subjects\nequally well without the help of scienti\ufb01c packages, although\nthe perception towards the use of computer is in general highly\npositive. Almost all students responded positively to Matlab\nas an effective tool for computation and graphing. Many\nused Matlab for non-examination purposes. For example, they\nutilized it to check their handwritten mathematical steps in\nassignments and practice problems; and others used it for\nexploration beyond the standard syllabus and curriculum.\nThe in\ufb02uence of computer technology on students\u2019 aca-\ndemic performance and learning experience has been inves-\ntigated by several authors. Abdul Majid et al. [1], [2] used\nMatlab as an aid to teach calculus to engineering students.\nThe software package was used for various course learning\noutcomes such as graphical display of mathematical functions,\nexploration, identifying and predicting structural patterns in\nevaluating a series of complex inde\ufb01nite integrals, and numeri-\ncal approximations in applied mathematics. The study showed\na positive impact on students\u2019 academic performance in the\n\ufb01nal examinations. The study concluded that the integration\nof scienti\ufb01c packages into engineering mathematics courses\ncould be effective under certain conditions. Similarly, other\nstudies [18], [21] also found a positive impact from the use\nof scienti\ufb01c software packages on students\u2019 motivation in\nlearning mathematics.\nIn a separate study by Brake [3], the authors investigated the\nuse of Matlab in engineering mathematics courses to increase\nstudent con\ufb01dence level and mathematical abilities. Matlab\nwas used to solve concrete engineering problems which require\na deep understanding of underlying mathematical principles.\nThe study found generally positive student response to the\nuse of software in their mathematics courses. However, the\nresults of the study must be considered carefully given the\nsmall sample size of the subjects.\nAlthough the majority of the studies were based on the\nuse of Matlab, several other studies considered alternative\nmathematics software packages. The study by Kilicman et al.\n[11] focused on the use of Maple to help students understand\nboth the theoretical and computational aspects of linear algebra\nfor engineering students. In particular, it was shown that the\nuse of Maple facilitates the understanding of computational as-\npects of eigenvalues and eigenvectors. It allows students more\ntime to focus on the theoretical aspects and the underlying\nmathematical principles.\nIn a recent study by Mezhennaya and Pugachev [14], the\nauthors compared engineering students\u2019 perceptions regarding\nseveral mathematical software: Matlab, Mathematica and Ex-\ncel. The study found that all the scienti\ufb01c packages considered\ncan be used in education, under the condition that the policies\nfor software usage are carefully implemented. The study\nfound that many students lack hands-on experience on how\nto use the software. The students particularly struggled with\nMatlab and Mathematica \ufb01nding them non user friendly. The\nstudy concluded that additional classes are required to prepare\nstudents to use software in their courses.\nIV. NUMERICAL COMPUTING IN MATHEMATICS\nCURRICULUM\nMathematics lies at the foundation of science and engineer-\ning. The importance of mathematics courses in engineering\neducation cannot be underestimated. These courses equip stu-\ndents with the fundamental skills and knowledge to study the\nmore specialized engineering courses. Thus, student success in\nengineering studies depends directly on the mathematics and\nsciences courses. Given the signi\ufb01cance of the mathematics\ncourses in the engineering curriculum, it is paramount to\nensure their currency with respect to the Industry 4.0.\nThe technological advances over the last decade have\ncreated demand for more computationally pro\ufb01cient experts.\nTo meet this demand, numerical computing must become a\ncore part of engineering studies. Mathematics courses offer\na natural and convenient avenue for introducing numerical\ncomputing to engineering students. There are two main fac-\ntors that make mathematics courses particularly amenable\nto numerical computing. First, in many cases mathematical\nproblems have numerical solutions. For instance, \ufb01nding the\nroot of a polynomial or the minimum value of a function\ncan be done numerically. Therefore, it is both logical and\nappropriate to apply numerical computing to mathematical\nproblems. Second, mathematics courses are usually taken\nat the beginning of the study plan. Thus, students become\nacquainted with numerical computing at an early stage. The\ncomputing and programming skills acquired in this manner\nwill have a positive effect in the more advanced, downstream\nengineering courses.\nThe key idea for the proposed curriculum update is the\naddition of computing tutorials (labs) to mathematics courses.\nIn particular, we propose adding weekly computing tutorials\n(labs) related to the main lecture material. For instance, in the\nweek in which students cover \ufb01nding the extreme values of a\nfunction, there will be a computing tutorial where students\nlearn and implement the gradient descent algorithm. The\nsuggested length of each tutorial is 1 hour. It is enough\ntime to implement most of the numerical algorithms at the\nundergraduate level. At the same time, 1 extra hour per week\nwill not overburden the students.\nThe exact details of numerical computing content is left\nfor individual universities and instructors. Depending on the\nsyllabus and course learning outcomes, the numerical com-\nputing labs will be different for each university and instructor.\nNevertheless, the general ideas will be broadly similar across\ndifferent curricula. To illustrate the proposed numerical com-\nputing content, we will focus on the three main concept in\ncalculus: limits, derivatives, and integrals.\nA. Limits\nLimit is a fundamental concept in calculus. Students are\nusually taught to calculate limits using analytical approaches.\nAlthough analytical approaches work well, there is no single\nuniversal rule for calculating limits. On the other hand, in most\ncases, limits can be calculated numerically using essentially\nthe same approach. To illustrate, suppose we want to calculate\nlimx\u2192a+ f(x). Then we can loop for k = 0 to n and calculate\nf(a+10\u2212k). As k increases, a+10\u2212k approaches a, so f(a+\n10\u2212k) will, in most cases, approach the limit value. We can\ndeduce the limit by observing the values of f(a + 10\u2212k) or\ndetermine that the limit does not exist if there is no pattern of\nconvergence. The value of n can be chosen manually or using\na stopping criterion. For instance, the algorithm may continue\nto iterate until the difference between consecutive values of\nf(a + 10\u2212k) is below a certain threshold. The value of the\nlimit can also be deduced automatically based on the values\nof f(a + 10\u2212k) using various heuristics.\nAnother common limit problem is limx\u2192\u221e f(x). In this\ncase, we can loop for k = 0 to n and calculate f(10k). As k\nincreases, 10k approaches \u221e, so f(10k) will, in most cases,\napproach the limit value. Then the limit can be determined\nbased on the values of f(10k). Various extensions and cus-\ntomizations of this basic approach can be made. For instance,\nto avoid issues with periodic functions f(10k + \u01ebk), where \u01ebk\nare randomly generated, can be used. Other values than 10k\ncan also be used as long as the sequence approaches in\ufb01nity.\nA degree of automization can be introduced using different\nheuristics.\nB. Derivative\nDerivative is arguably the most important concept in cal-\nculus. There exist several rules such as the power rule, the\nproduct rule, the chain rule, and others to \ufb01nd the derivative\nof a function by hand. However, manual differentiation may\nbe cumbersome when dealing with complex function. On the\nother hand, calculating the derivative at a point numerically is\nrelatively straightforward. To illustrate, suppose that we want\nto calculate f \u2032(a). Recall that\nf \u2032(a) = lim\nx\u2192a\nf(a + h) \u2212 f(a)\nh\n.\n(1)\nTherefore, to calculate f \u2032(a) numerically we use the same\napproach as with the limits. In particular, we can loop for k =\n0 to n and calculate f(a+10\u2212k)\u2212f(a)\n10\u2212k\n. Then the limit, and by\nextension the derivative, can be deduced (approximated) based\non the calculated values. The accuracy of the approximation\ndepends in large part on the value of n.\nOne of the most important applications of the derivative is\n\ufb01nding the extreme values of a function. Traditionally, this is\ndone by \ufb01rst \ufb01nding the critical points of the function and\nthen applying the second derivative test. However, \ufb01nding the\ncritical points is not always possible, so numerical approaches\ncan be used in such cases. The most popular numerical\napproach for \ufb01nding the extreme values is based on the\ngradient descent (ascent) algorithm. In gradient descent, the\noptimal value of x is iteratively updated based on the gradient.\nIn particular, for k = 0 to n, the updated optimal value of x\nis given by\nxk+1 = xk \u2212 \u03b1\u2207f(xk),\n(2)\nwhere \u2207f(x) is the gradient and \u03b1 is the step size. In the\ncase of a single-variable function, the gradient equals simply\nto the derivative \u2207f(x) = f \u2032(x). The step size \u03b1 can be\neither \ufb01xed or dynamic. While a large value of \u03b1 accelerates\nthe convergence at the beginning, it may hurt the convergence\nin the region near the optimal value.\nThere exist several extensions of the basic gradient descent\nalgorithm. One such extension is gradient descent with mo-\nmentum which uses the second derivative to anticipate the\nlocation of the next optimal point and thus accelerates the\nconvergence.\nC. Integrals\nIntegration is an important concept in engineering mathe-\nmatics. Although there exist a number of rules for \ufb01nding the\nintegral, it is signi\ufb01cantly more challenging than differentia-\ntion. Moreover, in many cases, the inde\ufb01nite integral does not\neven exist. Therefore, numerical approaches are particularly\nuseful for integration.\nTo illustrate the application of numerical integration, sup-\npose that we want to calculate\n\ufffd b\na f(x) dx. There exist several\nnumerical methods for calculating the integral. One simple\nmethod is based on the Riemann sums. The interval [a, b] is\ndivided into n equal subintervals with endpoints x0, x1, ..., xn.\nLet \u03b4 = xk+1 \u2212xk be the length of each subinterval. Then the\nright Riemann sum is de\ufb01ned as \u03b4 \ufffdn\nk=1 f(xk). The Riemann\nsum provides an approximation of the integral. Indeed,\n\ufffd b\na\nf(x) dx = lim\nn\u2192\u221e \u03b4\nn\n\ufffd\nk=1\nf(xk).\n(3)\nThe Riemann sums can be quickly calculated on a computer\nproviding a simple, yet effective approach to calculating\nintegrals numerically. Other popular integral approximation\nmethods include the trapezoid rule and the Simpson\u2019s rule.\nD. Additional considerations\nThe above discussion about numerical methods for cal-\nculating limits, derivatives, and integrals is easily extended\nto multivariate calculus. For instance, to \ufb01nd the partial\nderivative fx(a, b), we can loop for k = 0 to n and calculate\nf(a+10\u2212k,b)\u2212f(a,b)\n10\u2212k\n. Many problems related to sequences and\nseries can similarly be solved using numerical techniques. In\nparticular, the convergence of a series can be deduced from its\npartial sums. By calculating the partial sums on the computer\nand observing the results, we can intuit the nature of the series.\nVectorization is an important aspect of numerical com-\nputing. Since the modern computer chips are optimized for\nmatrix multiplication, it is more ef\ufb01cient to employ vector\noperations. In particular, some algorithms based on for-loops\ncan be converted into vector operations resulting in higher\nef\ufb01ciency and speed. For instance, the Riemann sum can be\ncalculated with a single vector operation:\n\u03b4\nn\n\ufffd\nk=1\nf(xk) = \u03b4S\n\ufffd\nf(x)\n\ufffd\n,\n(4)\nwhere x = [x1, ..., xn] is the vector of endpoints, f(x) is\na vectorized function operation, and S is the vector func-\ntion which returns the sum of all the coordinates. Similarly,\nlimit calculations can be vectorized and made more ef\ufb01cient.\nVectorization is also useful in multi-variate calculus, where\noperations can be performed on a vector of variables.\nThe choice of the programming language for numerical\ncomputing requires careful consideration. There are several\nsuitable candidates for this purpose including Python, Java,\nC++, Matlab, and others. Based on our experience with\ndifferent programming languages, we recommend the use of\nPython. Python is currently the most popular programming\nlanguage on the planet. It has a simple and intuitive syntax\nmaking it easy to learn and apply. Python has libraries to\n\ufb01t any purpose including an extensive collection of libraries\nrelated to numerical computing. The basic Python libraries\nrelated to computing are NumPy, SciPy, and SymPy. More\nadvanced packages such as OR-Tools are also available for\noptimization tasks. Since Python is a universal programming\nlanguage, it can be used for almost any task. Thus, students\nwho learn Python in their mathematics courses can employ\nit in their other courses. In addition, numerical computing\nimplemented in Python can be connected to other applications.\nV. CONCLUSION\nAlthough the classical approach to teaching mathematics\nis still relevant for certain student cohorts, it is outdated for\nengineering students. Modern engineering is increasingly re-\nliant on computing [9], [22]. Therefore, universities must equip\nthe student with appropriate computing skills. In particular,\nmathematics courses must be revised to include numerical\ncomputing content.\nGiven the ef\ufb01ciency of computer-based calculations, numer-\nical computing provides a convenient approach to problem\nsolving in engineering mathematics. It can be integrated into\nthe existing curriculum with little hassle and cost. In this paper,\nwe proposed a framework for integrating numerical computing\ninto the existing mathematics curriculum. We demonstrated\nhow numerical approaches can be used some of the most\ncommon problems encountered in calculus. The proposed\nframework can be customized by individual universities to \ufb01t\ntheir special needs.\nREFERENCES\n[1] Abdul Majid, M., Huneiti, Z., Balachandran, W., & Al-Naafa, M. A.\n(2012) A study of the effects of using MATLAB as a pedagogical tool\nfor engineering mathematics students. 15th International Conference on\nInteractive Collaborative Learning (ICL), 10.1109/ICL.2012.6402183,\nVillach, Austria.\n[2] Abdul Majid, M., Huneiti, Z. A., Balachandran, W., & Balarabe, Y.\n(2013) MATLAB as a teaching and learning tool for Mathematics: A\nliterature review. International Journal of Arts and Sciences, 6(3), 23-44.\n[3] Brake, M. L. (2007) MATLAB as a Tool to Increase the Math Self-\nCon\ufb01dence and the Math Ability of First-Year Engineering Technology\nStudents. The Scholarship of Teaching and Learning at EMU: Vol. 1,\nArticle 5.\n[4] Chen, J., Kolmos, A., & Du, X. (2021). Forms of implementation and\nchallenges of PBL in engineering education: a review of literature.\nEuropean Journal of Engineering Education, 46(1), 90-115.\n[5] Cretchley, P., Harman, C., Ellerton, N., & Fogarty, G. (2000) MATLAB\nin Early Undergraduate Mathematics: An investigation into the Effects\nof Scienti\ufb01c Software on Learning. Mathematics Education Research\nJournal, 12(3), 219-233.\n[6] Hadgraft, R. G., & Kolmos, A. (2020). Emerging learning environments\nin engineering education. Australasian Journal of Engineering Educa-\ntion, 25(1), 3-16.\n[7] Hernandez-de-Menendez, M., & Morales-Menendez, R. (2019). Tech-\nnological innovations and practices in engineering education: a review.\nInternational Journal on Interactive Design and Manufacturing (IJIDeM),\n13(2), 713-728.\n[8] Hern\u00b4andez-de-Men\u00b4endez, M., Guevara, A. V., Mart\u00b4\u0131nez, J. C. T.,\nAlc\u00b4antara, D. H., & Morales-Menendez, R. (2019). Active learning in\nengineering education. A review of fundamentals, best practices and ex-\nperiences. International Journal on Interactive Design and Manufacturing\n(IJIDeM), 13(3), 909-922.\n[9] Kamalov, F., Moussa, S., Zgheib, R., & Mashaal, O. (2020, December).\nFeature selection for intrusion detection systems. In 2020 13th Interna-\ntional Symposium on Computational Intelligence and Design (ISCID)\n(pp. 265-269). IEEE.\n[10] Kamalov, F., Sulieman, H., & Santandreu Calonge, D. (2021). Machine\nlearning based approach to exam cheating detection. Plos one, 16(8),\ne0254340.\n[11] Kilicman, A., Hassan, M. A., & Said Husain, S. K. (2010) Teaching and\nLearning using Mathematics Software \u2019The New Challenge\u2019. Procedia\nSocial and Behavioral Sciences, 8, 613-619.\n[12] Lorenz, M., R\u00a8u\u00dfmann, M., Strack, R., Lueth, K. L., & Bolle, M. (2015).\nMan and machine in industry 4.0: How will technology transform the\nindustrial workforce through 2025. The Boston Consulting Group, 2.\n[13] Maur\u00b4\u0131cio, R. D. A., Veado, L., Moreira, R. T., Figueiredo, E., & Costa,\nH. (2018). A systematic mapping study on game-related methods for\nsoftware engineering education. Information and software technology,\n95, 201-218.\n[14] Mezhennaya, N. M. & Pugachev, O. V. (2019) On perception of\ncomputer algebra systems and microsoft excel by engineering students.\nProblems on Education in the 21st Century, 77(3), 379-395.\n[15] Miranda, J., Navarrete, C., Noguez, J., Molina-Espinosa, J. M., Ram\u00b4\u0131rez-\nMontoya, M. S., Navarro-Tuch, S. A., ... & Molina, A. (2021). The core\ncomponents of education 4.0 in higher education: Three case studies in\nengineering education. Computers & Electrical Engineering, 93, 107278.\n[16] Moore, K., & Frazier, R. S. (2017). Engineering education for generation\nZ. American Journal of Engineering Education (AJEE), 8(2), 111-126.\n[17] Ortiz-Rojas, M., Chiluiza, K., & Valcke, M. (2019). Gami\ufb01cation\nthrough leaderboards: An empirical study in engineering education.\nComputer Applications in Engineering Education, 27(4), 777-788.\n[18] Puhak, R. I. (2011). Teaching applied Calculus utilizing MATLAB,\nProceedings of the Twenty-Third Annual International Conference on\nTechnology in Collegiate Mathematics, ISBN 0-321-68984-4, Copyright\n(C) 2012 by Pearson Education, Inc.\n[19] Ramirez-Mendoza, R. A., Morales-Menendez, R., Iqbal, H., & Parra-\nSaldivar, R. (2018, April). Engineering Education 4.0:\u2014proposal for a\nnew Curricula. In 2018 IEEE Global Engineering Education Conference\n(EDUCON) (pp. 1273-1282). IEEE.\n[20] Salah, B., Abidi, M. H., Mian, S. H., Krid, M., Alkhalefah, H., &\nAbdo, A. (2019). Virtual reality-based engineering education to enhance\nmanufacturing sustainability in industry 4.0. Sustainability, 11(5), 1477.\n[21] Strayhorn, T. L. (2006). College in the information age: Gains associated\nwith students\u2019 use of technology. Journal of Interactive Online Learning,\n5(2), 143-155.\n[22] Thabtah, F., Kamalov, F., & Rajab, K. (2018). A new computational\nintelligence approach to detect autistic features for autism screening.\nInternational journal of medical informatics, 117, 112-124.\n[23] Van den Beemt, A., MacLeod, M., Van der Veen, J., Van de Ven,\nA., van Baalen, S., Klaassen, R., & Boon, M. (2020). Interdisciplinary\nengineering education: A review of vision, teaching, and support. Journal\nof engineering education, 109(3), 508-555.\n",
    "pdf_url": "",
    "references": [
      "[1] Abdul Majid, M., Huneiti, Z., Balachandran, W., & Al-Naafa, M. A.",
      "(2012) A study of the effects of using MATLAB as a pedagogical tool",
      "for engineering mathematics students. 15th International Conference on",
      "Interactive Collaborative Learning (ICL), 10.1109/ICL.2012.6402183,",
      "Villach, Austria.",
      "[2] Abdul Majid, M., Huneiti, Z. A., Balachandran, W., & Balarabe, Y.",
      "(2013) MATLAB as a teaching and learning tool for Mathematics: A",
      "literature review. International Journal of Arts and Sciences, 6(3), 23-44.",
      "[3] Brake, M. L. (2007) MATLAB as a Tool to Increase the Math Self-"
    ],
    "publication_date": "11-07-2022"
  },
  {
    "titre": "Advances in Engineering Software Research paper SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical andmixed variables Gaussian processes",
    "resume": "The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogatemodeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a majornew release of SMT that introduces significant upgrades and new features to the toolbox. This release addsthe capability to handle mixed-variable surrogate models and hierarchical variables. These types of variablesare becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SM Tby extending sampling methods, adding new surrogate models, and computing variance and kernel derivativesfor Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best ofour knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchicaland mixed inputs. This open-source software is distributed under the New BSD license.2",
    "auteurs": [
      "Paul Saves",
      "b,\u2217,1",
      "R\u00e9mi Lafage a,1",
      "Nathalie Bartoli",
      "Youssef Diouane",
      "Jasper Bussemaker",
      "Thierry Lefebvre a,1",
      "John T. Hwang",
      "Joseph Morlier f,1",
      "Toulouse",
      "Kriging",
      "Kriging",
      "P. Saves",
      "Y. Diouane",
      "J. Bussemaker",
      "J.T. Hwang",
      "J. Morlier",
      "UQLab"
    ],
    "institutions": [
      " Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France.E-mail addresses:  (P. Saves),  (R. Lafage),  (N. Bartoli), ",
      "y ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of Michigan. Now, both Polytechnique Montr\u00e9al and the University ofCalifornia San Diego are also contributors. SMT 2.0 updates and ex-tends the original SMT repository capabilities among which the originalpublication [5] focuses on different types of derivatives for surrogatemodels detailed hereafter.",
      "Research paper SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical andmixed variables Gaussian processes",
      " ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, Franceb ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, Francec Polytechnique Montr\u00e9al, Montreal, QC, Canadad German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germanye University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, US Af ICA, Universit\u00e9 de Toulouse, ISAESUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, Franceg University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA"
    ],
    "mots_cles": [
      ":Surrogate modeling"
    ],
    "texte_integral": "Advances in Engineering Software 188 (2024) 103571\nAvailable online 7 December 2023\n0965-9978/\u00a9 2023 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nAdvances in Engineering Software\njournal homepage: www.elsevier.com/locate/advengsoft\nResearch paper\nSMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and\nmixed variables Gaussian processes\nPaul Saves a,b,\u2217,1, R\u00e9mi Lafage a,1, Nathalie Bartoli a,1, Youssef Diouane c,1, Jasper Bussemaker d,1,\nThierry Lefebvre a,1, John T. Hwang e,1, Joseph Morlier f,1, Joaquim R.R.A. Martins g,1\na ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France\nb ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, France\nc Polytechnique Montr\u00e9al, Montreal, QC, Canada\nd German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany\ne University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA\nf ICA, Universit\u00e9 de Toulouse, ISAE\u2013SUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France\ng University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA\nA R T I C L E\nI N F O\nDataset link: https://colab.research.google.com\n/github/SMTorg/smt/blob/master/tutorial/No\ntebookRunTestCases_Paper_SMT_v2.ipynb\nKeywords:\nSurrogate modeling\nGaussian process\nKriging\nHierarchical problems\nHierarchical and mixed-categorical inputs\nMeta variables\nA B S T R A C T\nThe Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate\nmodeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major\nnew release of SMT that introduces significant upgrades and new features to the toolbox. This release adds\nthe capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables\nare becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT\nby extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives\nfor Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of\nour knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical\nand mixed inputs. This open-source software is distributed under the New BSD license.2\n1. Motivation and significance\nWith the increasing complexity and accuracy of numerical models, it\nhas become more challenging to run complex simulations and computer\ncodes [1,2]. As a consequence, surrogate models have been recognized\nas a key tool for engineering tasks such as design space exploration,\nuncertainty quantification, and optimization [3]. In practice, surrogate\nmodels are used to reduce the computational effort of these tasks by\nreplacing expensive numerical simulations with closed-form approxi-\nmations [4, Ch. 10]. To build such a model, we start by evaluating\nthe original expensive simulation at a set of points through a Design\nof Experiments (DoE). Then, the corresponding evaluations are used to\nbuild the surrogate model according to the chosen approximation, such\nas Kriging, quadratic interpolation, or least squares regression.\nThe Surrogate Modeling Toolbox (SMT) is an open-source frame-\nwork that provides functions to efficiently build surrogate models [5].\n\u2217 Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France.\nE-mail addresses: paul.saves@onera.fr (P. Saves), remi.lafage@onera.fr (R. Lafage), nathalie.bartoli@onera.fr (N. Bartoli), youssef.diouane@polymtl.ca\n(Y. Diouane), jasper.bussemaker@dlr.de (J. Bussemaker), thierry.lefebvre@onera.fr (T. Lefebvre), jhwang@eng.ucsd.edu (J.T. Hwang),\njoseph.morlier@isae-supaero.fr (J. Morlier), jrram@umich.edu (J.R.R.A. Martins).\n1 All authors contributed to this work, research and manuscript.\n2 https://github.com/SMTorg/SMT\nKriging models (also known as Gaussian processes) that take advantage\nof derivative information are one of SMT\u2019s key features [6]. Numerical\nexperiments have shown that SMT achieved lower prediction error\nand computational cost than Scikit-learn [7] and UQLab [8] for a\nfixed number of points [9]. SMT has been applied to rocket engine\ncoaxial-injector optimization [10], aircraft engine consumption mod-\neling [11], numerical integration [12], multi-fidelity sensitivity analy-\nsis [13], high-order robust finite elements methods [14,15], planning\nfor photovoltaic solar energy [16], wind turbines design optimiza-\ntion [17], porous material optimization for a high pressure turbine\nvane [18], chemical process design [19] and many other applications.\nIn systems engineering, architecture-level choices significantly in-\nfluence the final system performance, and therefore, it is desirable to\nconsider such choices in the early design phases [20]. Architectural\nchoices are parameterized with discrete design variables; examples in-\nclude the selection of technologies, materials, component connections,\nhttps://doi.org/10.1016/j.advengsoft.2023.103571\nReceived 22 August 2023; Received in revised form 23 October 2023; Accepted 26 November 2023\nAdvances in Engineering Software 188 (2024) 103571\n2\nP. Saves et al.\nTable 1\nComparison of software packages for hierarchical and mixed Kriging models. \u2713= implemented. * = user-defined.\nPackage\nBOTorch\nDakota\nDiceKriging\nKerGP\nLVGP\nParmoo\nSpearmint\nSMT 2.0\nReference\n[25]\n[26]\n[27]\n[32]\n[28]\n[29]\n[30]\nThis paper\nLicense\nMIT\nEPL\nGPL\nGPL\nGPL\nBSD\nGNU\nBSD\nLanguage\nPython\nC\nR\nR\nR\nPython\nPython\nPython\nMixed var.\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nGD kernel\n\u2713\n\u2713\n\u2713\n*\n\u2713\nCR kernel\n\u2713\n\u2713\n\u2713\n\u2713\nHH kernel\n\u2713\n\u2713\nEHH kernel\n*\n\u2713\nHierarchical var.\n\u2713\nand number of instantiated elements. When design problems include\nboth discrete variables and continuous variables, they are said to have\nmixed variables.\nWhen architectural choices lead to different sets of design variables,\nwe have hierarchical variables [21,22]. For example, consider differ-\nent aircraft propulsion architectures [23]. A conventional gas turbine\nwould not require a variable to represent a choice in the electrical\npower source, while hybrid or pure electric propulsion would require\nsuch a variable. The relationship between the choices and the sets of\nvariables can be represented by a hierarchy.\nHandling hierarchical and mixed variables requires specialized sur-\nrogate modeling techniques [24]. To address these needs, SMT 2.0\nis offering researchers and practitioners a collection of cutting-edge\ntools to build surrogate models with continuous, mixed and hierarchical\nvariables. The main objective of this paper is to detail the new enhance-\nments that have been added in this release compared to the original\nSMT 0.2 release [5].\nThere are two new major capabilities in SMT 2.0: the ability\nto build surrogate models involving mixed variables and the support\nfor hierarchical variables within Kriging models. To handle mixed\nvariables in Kriging models, existing libraries such as BoTorch [25],\nDakota [26], DiceKriging [27], LVGP [28], Parmoo [29], and Spearmint\n[30] implement simple mixed models by using either continuous relax-\nation (CR), also known as one-hot encoding [30], or a Gower distance\n(GD) based correlation kernel [31]. KerGP [32] (developed in R) imple-\nments more general kernels but there is no Python open-source toolbox\nthat implements more general kernels to deal with mixed variables,\nsuch as the homoscedastic hypersphere (HH) [33] and exponential\nhomoscedastic hypersphere (EHH) [34] kernels. Such kernels require\nthe tuning of a large number of hyperparameters but lead to more\naccurate Kriging surrogates than simpler mixed kernels [34,35]. SMT\n2.0 implements all these kernels (CR, GD, HH, and EHH) through a\nunified framework and implementation. To handle hierarchical vari-\nables, no library in the literature can build peculiar surrogate models\nexcept SMT 2.0, which implements two Kriging methods for these\nvariables. Notwithstanding, most softwares are compatible with a na\u00efve\nstrategy called the imputation method [24] but this method lacks depth\nand depends on arbitrary choices. This is why Hutter and Osborne\n[21] proposed a first kernel, called Arc-Kernel which in turn was\ngeneralized by Horn et al. [36] with a new kernel called the Wedge-\nKernel [37]. None of these kernels are available in any open-source\nmodeling software. Furthermore, thanks to the framework introduced\nin Audet et al. [38], our proposed kernels are sufficiently general so\nthat all existing hierarchical kernels are included within it. Section 4\ndescribes the two kernels implemented in SMT 2.0 that are referred\nas SMT Arc-Kernel and SMT Alg-Kernel. In particular, Alg-\nKernel is a novel hierarchical kernel introduced in this paper. Table 1\noutlines the main features of the state-of-the-art modeling software that\ncan handle hierarchical and mixed variables.\nSMT 2.0 introduces other enhancements, such as additional sam-\npling procedures, new surrogate models, new Kriging kernels (and their\nderivatives), Kriging variance derivatives, and an adaptive criterion for\nhigh-dimensional problems. SMT 2.0 adds applications of Bayesian\noptimization (BO) with hierarchical and mixed variables or noisy co-\nKriging that have been successfully applied to aircraft design [39], data\nfusion [40], and structural design [41]. The SMT 2.0 interface is more\nuser-friendly and offers an improved and more detailed documentation\nfor users and developers.3 SMT 2.0 is hosted publicly4 and can be\ndirectly imported within Python scripts. It is released under the New\nBSD License and runs on Linux, MacOS, and Windows operating sys-\ntems. Regression tests are run automatically for each operating system\nwhenever a change is committed to the repository. In short, SMT 2.0\nbuilds on the strengths of the original SMT package while adding new\nfeatures. On one hand, the emphasis on derivatives (including predic-\ntion, training and output derivatives) is maintained and improved in\nSMT 2.0. On the other hand, this new release includes support for\nhierarchical and mixed variables Kriging based models. For the sake\nof reproducibility, an open-source notebook is available that gathers\nall the methods and results presented on this paper.5\nThe remainder of the paper is organized as follows. First, we in-\ntroduce the organization and the main implemented features of the\nrelease in Section 2. Then, we describe the mixed-variable Kriging\nmodel with an example in Section 3. Similarly, we describe and provide\nan example for a hierarchical-variable Kriging model in Section 4.\nThe Bayesian optimization models and applications are described in\nSection 5. Finally, we describe the other relevant contributions in\nSection 6 and conclude in Section 7.\n2. SMT 2.0 : an improved surrogate modeling toolbox\nFrom a software point of view, SMT 2.0 maintains and improves\nthe modularity and generality of the original SMT version [5]. In this\nsection, we describe the software as follows. Section 2.1 describes the\nlegacy of SMT 0.2. Then, Section 2.2 describes the organization of the\nrepository. Finally, Section 2.3 shows the new capabilities implemented\nin the SMT 2.0 update.\n2.1. Background on SMT former version: SMT 0.2\nSMT [5] is an open-source collaborative work originally developed\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\nMichigan. Now, both Polytechnique Montr\u00e9al and the University of\nCalifornia San Diego are also contributors. SMT 2.0 updates and ex-\ntends the original SMT repository capabilities among which the original\npublication [5] focuses on different types of derivatives for surrogate\nmodels detailed hereafter.\n3 http://smt.readthedocs.io/en/latest\n4 https://github.com/SMTorg/smt\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\nNotebookRunTestCases_Paper_SMT_v2.ipynb\nAdvances in Engineering Software 188 (2024) 103571\n3\nP. Saves et al.\nTable 2\nImpact of using Numba on training time of the hierarchical Goldstein problem. Speedup\nis calculated excluding the JIT compilation table, as this step is only needed once after\nSMT installation.\nTraining set\nWithout numba\nNumba\nSpeedup\nJIT overhead\n15 points\n1.3 s\n1.1 s\n15%\n24 s\n150 points\n38 s\n7.4 s\n80%\n23 s\nA Python surrogate modeling framework with derivatives. One of the\noriginal main motivations for SMT was derivative support. In fact, none\nof the existing packages for surrogate modeling such as Scikit-learn in\nPython [7], SUMO in Matlab [42] or GPML in Matlab and Octave [43]\nfocuses on derivatives. Three types of derivatives are distinguished:\nprediction derivatives, training derivatives, and output derivatives.\nSMT also includes new models with derivatives such as Kriging with\nPartial Least Squares (KPLS) [44] and Regularized Minimal-energy\nTensor-product Spline (RMTS) [3]. These developed derivatives were\neven used in a novel algorithm called Gradient-Enhanced Kriging with\nPartial Least Squares (GEKPLS) [6] to use with adjoint methods, for\nexample [45].\nSoftware architecture, documentation, and automatic testing. SMT is orga-\nnized along three main sub-modules that implement a set of sampling\ntechniques (sampling_methods), benchmarking functions (problems),\nand surrogate modeling techniques (surrogate_models). The toolbox\ndocumentation6 is created using reStructuredText and Sphinx, a doc-\numentation generation package for Python, with custom extensions.\nCode snippets in the documentation pages are taken directly from\nactual tests in the source code and are automatically updated. The\noutput from these code snippets and tables of options are generated\ndynamically by custom Sphinx extensions. This leads to high-quality\ndocumentation with minimal effort. Along with user documentation,\ndeveloper documentation is also provided to explain how to contribute\nto SMT. This includes a list of API methods for the SurrogateModel,\nSamplingMethod, and Problem classes, that must be implemented\nto create a new surrogate modeling method, sampling technique, or\nbenchmarking problem. When a developer submits a pull request, it is\nmerged only after passing the automated tests and receiving approval\nfrom at least one reviewer. The repository on GitHub7 is linked to\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\nMacOS, to a coverage test on coveralls.io and to a dependency version\ncheck for Python with DependaBot. Various parts of the source code\nhave been accelerated using Numba [46], an LLVM-based just-in-time\n(JIT) compiler for numpy-heavy Python code. Numba is applied to con-\nventional Python code using function decorators, thereby minimizing\nits impact on the development process and not requiring an additional\nbuild step. For a mixed Kriging surrogate with 150 training points, a\nspeedup of up to 80% is observed, see Table 2. The JIT compilation\nstep only needs to be done once when installing or upgrading SMT\nand adds an overhead of approximately 24 s on a typical workstation\nIn this paper, all results are obtained using an Intel\u00ae Xeon\u00ae CPU\nE5-2650 v4 @ 2.20 GHz core and 128 GB of memory with a Broadwell-\ngeneration processor front-end and a compute node of a peak power of\n844 GFlops.\n2.2. Organization of SMT 2.0\nThe main features of the open-source repository SMT 2.0 are\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\nand Surrogate models are kept from SMT 0.2 and two new\nsections Models applications and Interactive notebooks\nhave been added to the architecture of the code. These sections are\n6 https://smt.readthedocs.org\n7 https://github.com/SMTorg/smt\nhighlighted in blue and detailed on Fig. 1. The new major features\nimplemented in SMT 2.0 are highlighted in lavender whereas the\nlegacy features that were already in present in the original publication\nfor SMT 0.2 [5] are in black.\n2.3. New features within SMT 2.0\nThe main objective of this new release is to enable Kriging surrogate\nmodels for use with both hierarchical and mixed variables. Moreover,\nfor each of these five sub-modules described in Section 2.2, several\nimprovements have been made between the original version and the\nSMT 2.0 release.\nHierarchical and mixed design space. A new design space definition\nclass DesignSpace has been added that implements hierarchical\nand mixed functionalities. Design variables can either be continu-\nous (FloatVariable), ordered (OrdinalVariable) or categorical\n(CategoricalVariable). The integer type (IntegerVariable) rep-\nresents a special case of the ordered variable, specified by bounds\n(inclusive) rather than a list of possible values. The hierarchical struc-\nture of the design space can be defined using declare_decreed_var:\nthis function declares that a variable is a decreed variable that is\nactivated when the associated meta variable takes one of a set of\nspecified values, see Section 4 for background. The DesignSpace\nclass also implements mechanisms for sampling valid design vectors\n(i.e. design vectors that adhere to the hierarchical structure of the\ndesign space) using any of the below-mentioned samplers, for cor-\nrecting and imputing design vectors, and for requesting which design\nvariables are acting in a given design vector. Correction ensures that\nvariables have valid values (e.g. integers for discrete variables) [24],\nand imputation replaces non-acting variables by some default value\n(0 for discrete variables, mid-way between the bounds for continuous\nvariables in SMT 2.0) [47].\nSampling. SMT implements three methods for sampling. The first one\nis a na\u00efve approach, called Random that draws uniformly points along\nevery dimension. The second sampling method is called Full Fac-\ntorial and draws a point for every cross combination of variables,\nto have an \u2018\u2018exhaustive\u2019\u2019 design of experiments. The last one is the\nLatin Hypercube Sampling (LHS) [48] that draws a point in\nevery Latin square parameterized by a certain criterion. For LHS, a\nnew criterion to manage the randomness has been implemented and\nthe sampling method was adapted for multi-fidelity and mixed or\nhierarchical variables. More details about the new sampling techniques\nare given in Section 6.1.\nProblems. SMT implements two new engineering problems: a mixed\nvariant of a cantilever beam described in Section 3 and a hierarchical\nneural network described in Section 4.\nSurrogate models. In order to keep up with state-of-art, several re-\nleases done from the original version developed new options for the\nalready existing surrogates. In particular, compared to the original\npublication [5], SMT 2.0 adds gradient-enhanced neural networks [45]\nand marginal Gaussian process [49] models to the list of available\nsurrogates. More details about the new models are given in Section 6.2.\nApplications. Several applications have been added to the toolbox to\ndemonstrate the surrogate models capabilities. The most relevant ap-\nplication is efficient global optimization (EGO), a Bayesian optimiza-\ntion algorithm [50,51]. EGO optimizes expensive-to-evaluate black-box\nproblems with a chosen surrogate model and a chosen optimization\ncriterion [52]. The usage of EGO with hierarchical and mixed variables\nis described in Section 5.\nAdvances in Engineering Software 188 (2024) 103571\n4\nP. Saves et al.\nFig. 1. Functionalities of SMT 2.0. The new major features implemented in SMT 2.0 compared to SMT 0.2 are highlighted with the lavender color.\nInteractive notebooks. These tutorials introduce and explain how to use\nthe toolbox for different surrogate models and applications.8 Every\ntutorial is available both as a .ipynb file and directly on Google\ncolab.9 In particular, a hierarchical and mixed variables dedicated\nnotebook is available to reproduce the results presented on this paper.10\nIn the following, Section 3 details the Kriging based surrogate\nmodels for mixed variables, and Section 4 presents our new Kriging\nsurrogate for hierarchical variables. Section 5 details the EGO applica-\ntion and the other new relevant features aforementioned are described\nsuccinctly in Section 6.\n8 https://github.com/SMTorg/smt/tree/master/tutorial\n9 https://colab.research.google.com/github/SMTorg/smt/\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\nNotebookRunTestCases_Paper_SMT_v2.ipynb\n3. Surrogate models with mixed variables in SMT 2.0\nAs mentioned in Section 1, design variables can be either of continu-\nous or discrete type, and a problem with both types is a mixed-variable\nproblem. Discrete variables can be ordinal or categorical. A discrete\nvariable is ordinal if there is an order relation within the set of possible\nvalues. An example of an ordinal design variable is the number of\nengines in an aircraft. A possible set of values in this case could be\n2, 4, 8. A discrete variable is categorical if no order relation is known\nbetween the possible choices the variable can take. One example of a\ncategorical variable is the color of a surface. A possible example of a\nset of choices could be blue, red, green. The possible choices are called\nthe levels of the variable.\nSeveral methods have been proposed to address the recent increase\ninterest in mixed Kriging based models [30\u201333,35,39,53,54]. The main\ndifference from a continuous Kriging model is in the estimation of\nAdvances in Engineering Software 188 (2024) 103571\n5\nP. Saves et al.\nTable 3\nCategorical kernels implemented in SMT 2.0.\nName\n\ud835\udf05(\ud835\udf19)\n\ud835\udef7(\ud835\udee9\ud835\udc56)\n# of hyperparam.\nSMT GD\nexp(\u2212\ud835\udf19)\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 1\n2 \ud835\udf03\ud835\udc56 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= 0\n1\nSMT CR\nexp(\u2212\ud835\udf19)\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= [\ud835\udee9\ud835\udc56]\ud835\udc57,\ud835\udc57 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= 0\n\ud835\udc3f\ud835\udc56\nSMT EHH\nexp(\u2212\ud835\udf19)\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 0 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= log \ud835\udf16\n2 ([\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)\u22a4]\ud835\udc57,\ud835\udc57\u2032 \u2212 1)\n1\n2 (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 \u2212 1)\nSMT HH\n\ud835\udf19\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 1 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= [\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)\u22a4]\ud835\udc57,\ud835\udc57\u2032\n1\n2 (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 \u2212 1)\nthe categorical correlation matrix, which is critical to determine the\nmean and variance predictions. As mentioned in Section 1, approaches\nsuch as CR [30,39], continuous latent variables [54], and GD [31]\nuse a kernel-based method to estimate the correlation matrix. Other\nmethods estimate the correlation matrix by modeling the correlation\nentries directly [32,35,53], such as HH [33] and EHH [34]. The HH\ncorrelation kernel is of particular interest because it generalizes simpler\nkernels such as CR and GD [34]. In SMT 2.0, the correlation kernel is\nan option that can be set to either CR (CONT_RELAX_KERNEL), GD\n(GOWER_KERNEL),\nHH\n(HOMO_HSPHERE_KERNEL)\nor\nEHH\n(EXP_HOMO_HSPHERE_KERNEL).\n3.1. Mixed Gaussian processes\nThe continuous and ordinal variables are both treated similarly\nin SMT 2.0 with a continuous kernel, where the ordinal values are\nconverted to continuous through relaxation. For categorical variables,\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\nspecified by the API. This is why we developed a unified mathematical\nformulation that allows a unique implementation for any model.\nDenote \ud835\udc59 the number of categorical variables. For a given \ud835\udc56 \u2208\n{1, \u2026 , \ud835\udc59}, the \ud835\udc56th categorical variable is denoted \ud835\udc50\ud835\udc56 and its number\nof levels is denoted \ud835\udc3f\ud835\udc56. The hyperparameter matrix peculiar to this\nvariable \ud835\udc50\ud835\udc56 is\n\ud835\udee9\ud835\udc56 =\n\u23a1\n\u23a2\n\u23a2\n\u23a2\n\u23a2\u23a3\n[\ud835\udee9\ud835\udc56]1,1\n\ud835\udc7a\ud835\udc9a\ud835\udc8e.\n[\ud835\udee9\ud835\udc56]1,2\n[\ud835\udee9\ud835\udc56]2,2\n\u22ee\n\u22f1\n\u22f1\n[\ud835\udee9\ud835\udc56]1,\ud835\udc3f\ud835\udc56\n\u2026\n[\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56\u22121,\ud835\udc3f\ud835\udc56\n[\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56,\ud835\udc3f\ud835\udc56\n\u23a4\n\u23a5\n\u23a5\n\u23a5\n\u23a5\u23a6\n,\nand the categorical parameters are defined as \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61 = {\ud835\udee91, \u2026 , \ud835\udee9\ud835\udc59}. For\ntwo given inputs in the DoE, for example, the \ud835\udc5fth and \ud835\udc60th points, let\n\ud835\udc50\ud835\udc5f\n\ud835\udc56 and \ud835\udc50\ud835\udc60\n\ud835\udc56 be the associated categorical variables taking respectively\nthe \ud835\udcc1\ud835\udc56\n\ud835\udc5f and the \ud835\udcc1\ud835\udc56\n\ud835\udc60 level on the categorical variable \ud835\udc50\ud835\udc56. The categorical\ncorrelation kernel is defined by\n\ud835\udc58\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc50\ud835\udc5f, \ud835\udc50\ud835\udc60, \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61) =\n\ud835\udc59\u220f\n\ud835\udc56=1\n\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f\n\ud835\udc56 ,\ud835\udcc1\ud835\udc60\n\ud835\udc56 ) \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60\n\ud835\udc56 ,\ud835\udcc1\ud835\udc5f\n\ud835\udc56 )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f\n\ud835\udc56 ,\ud835\udcc1\ud835\udc5f\n\ud835\udc56 )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60\n\ud835\udc56 ,\ud835\udcc1\ud835\udc60\n\ud835\udc56 )\n(1)\nwhere \ud835\udf05 is either a positive definite kernel or identity and \ud835\udef7(.) is a\nsymmetric positive definite (SPD) function such that the matrix \ud835\udef7(\ud835\udee9\ud835\udc56)\nis SPD if \ud835\udee9\ud835\udc56 is SPD. For an exponential kernel, Table 3 gives the\nparameterizations of \ud835\udef7 and \ud835\udf05 that correspond to GD, CR, HH, and\nEHH kernels. The complexity of these different kernels depends on\nthe number of hyperparameters that characterizes them. As defined\nby Saves et al. [34], for every categorical variable \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc59}, the\nmatrix \ud835\udc36(\ud835\udee9\ud835\udc56) \u2208 R\ud835\udc3f\ud835\udc56\u00d7\ud835\udc3f\ud835\udc56 is lower triangular and built using a hypersphere\ndecomposition [55,56] from the symmetric matrix \ud835\udee9\ud835\udc56 \u2208 R\ud835\udc3f\ud835\udc56\u00d7\ud835\udc3f\ud835\udc56 of\nhyperparameters. The variable \ud835\udf16 is a small positive constant and the\nvariable \ud835\udf03\ud835\udc56 denotes the only positive hyperparameter that is used for\nthe Gower distance kernel.\nAnother Kriging based model that can use mixed variables is Kriging\nwith partial least squares (KPLS) [57]. KPLS adapts Kriging to high\ndimensional problems by using a reduced number of hyperparameters\nthanks to a projection into a smaller space. Also, for a general surrogate,\nnot necessarily Kriging, SMT 2.0 uses continuous relaxation to allow\nwhatever model to handle mixed variables. For example, we can use\nmixed variables with least squares (LS) or quadratic polynomial (QP)\nmodels. We now illustrate the abilities of the toolbox in terms of mixed\nmodeling over an engineering test case.\nTable 4\nResults of the cantilever beam models [34, Table 4].\nCategorical\nkernel\nDisplacement\nerror (cm)\nLikelihood\n# of\nhyperparam.\nSMT GD\n1.3861\n111.13\n3\nSMT CR\n1.1671\n155.32\n14\nSMT EHH\n0.1613\n236.25\n68\nSMT HH\n0.2033\n235.66\n68\n3.2. An engineering design test-case\nA classic engineering problem commonly used for model validation\nis the beam bending problem [32,58]. This problem is illustrated\non Fig. 2(a) and consists of a cantilever beam in its linear range loaded\nat its free end with a force \ud835\udc39. As in Cheng et al. [58], the Young\nmodulus is \ud835\udc38 = 200 GPa and the chosen load is \ud835\udc39 = 50 kN. Also, as\nin Roustant et al. [32], 12 possible cross-sections can be used. These\n12 sections consist of 4 possible shapes that can be either hollow, thick\nor full as illustrated in Fig. 2(b).\nTo compare the mixed Kriging models of SMT 2.0, we draw a 98\npoint LHS as training set and the validation set is a grid of 12 \u00d7 30 \u00d7\n30 = 10800 points. For the four implemented methods, displacement\nerror (computed with a root-mean-square error criterion), likelihood,\nnumber of hyperparameters and computational time for every model\nare shown in Table 4. For the continuous variables, we use the square\nexponential kernel. More details are found in [34]. As expected, the\ncomplex EHH and HH models lead to a lower displacement error and a\nhigher likelihood value, but use more hyperparameters and increase\nthe computational cost compared to GD and CR. On this test case,\nthe kernel EHH is easier to optimize than HH but in general, they\nare similar in terms of performance. Also, by default SMT 2.0 uses\nCR as it is known to be a good trade-off between complexity and\nperformance [59].\n4. Surrogate models with hierarchical variables in SMT 2.0\nTo introduce the newly developed Kriging model for hierarchical\nvariables implemented in SMT 2.0, we present the general mathe-\nmatical framework for hierarchical and mixed variables established\nby Audet et al. [38]. In SMT 2.0, two variants of our new method\nare implemented, namely SMT Alg-Kernel and SMT Arc-Kernel.\nIn particular, the SMT Alg-Kernel is a novel correlation kernel\nintroduced in this paper.\n4.1. The hierarchical variables framework\nA problem structure is classified as hierarchical when the sets of\nactive variables depend on architectural choices. This occurs frequently\nin industrial design problems. In hierarchical problems, we can classify\nvariables as neutral, meta (also known as dimensional) or decreed\n(also known as conditionally active) as detailed in Audet et al. [38].\nNeutral variables are the variables that are not affected by the hierarchy\nwhereas the value assigned to meta variables determines which decreed\nvariables are activated. For example, a meta variable could be the\nnumber of engines. If the number of engines changes, the number of\ndecreed bypass ratios that every engine should specify also changes.\nAdvances in Engineering Software 188 (2024) 103571\n6\nP. Saves et al.\nFig. 2. Cantilever beam problem [34, Figure 6].\nFig. 3. Variables classification as used in SMT 2.0.\nHowever, the wing aspect ratio being neutral, it is not affected by this\nhierarchy.\nProblems involving hierarchical variables are generally dependant\non discrete architectures and as such involve mixed variables. Hence,\nin addition to their role (neutral, meta or decreed), each variable also\nhas a variable type amongst categorical, ordinal or continuous. For the\nsake of simplicity and because both continuous and ordinal variables\nare treated similarly [34], we chose to regroup them as quantitative\nvariables. For instance, the neutral variables \ud835\udc65neu may be partitioned\ninto different variable types, such that \ud835\udc65neu = (\ud835\udc65cat\nneu, \ud835\udc65qnt\nneu) where \ud835\udc65cat\nneu\nrepresents the categorical variables and \ud835\udc65qnt\nneu are the quantitative ones.\nThe variable classification scheme in SMT 2.0 is detailed in Fig. 3.\nTo explain the framework and the new Kriging model, we illustrate\nthe inputs variables of the model using a classical machine learn-\ning problem related to the hyperparameters optimization of a fully-\nconnected Multi-Layer Perceptron (MLP) [38] on Fig. 4. In\nTable 5,\nwe detail the input variables of the model related to the MLP problem\n(i.e., the hyperparameters of the neural network, together with their\ntypes and roles). To keep things clear and concise, the chosen problem\nis a simplification of the original problem developed by Audet et al.\n[38]. Regarding the MLP problem of Fig. 4 and following the classi-\nfication scheme of Fig. 3, we start by separating the input variables\naccording to their role. In fact,\n1. changing the number of hidden layers modifies the number\nof inputs variables. Therefore, \u2018\u2018# of hidden layers\u2019\u2019 is a meta\nvariable.\n2. The number of neurons in the hidden layer number \ud835\udc58 is either\nincluded or excluded. For example, the \u2018\u2018# of neurons in the 3rd\nlayer\u2019\u2019 would be excluded for an input that only has 2 hidden\nlayers. Therefore, \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are decreed\nvariables.\n3. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and\n\u2018\u2018Batch size\u2019\u2019 are not affected by the hierarchy choice. Therefore,\nthey are neutral variables.\nAccording to their types, the MLP input variables can be classified as\nfollows:\n4. The meta variable \u2018\u2018# of hidden layers\u2019\u2019 is an integer and, as\nsuch, is represented by the component \ud835\udc65qnt\nmet.\n5. The decreed variables \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are integers\nand, as such, are represented by the component \ud835\udc65qnt\ndec.\n6. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and\n\u2018\u2018Batch size\u2019\u2019 are, respectively, continuous, for the first two (ev-\nery value between two bounds), categorical (qualitative between\nthree choices) and integer (quantitative between 6 choices).\nTherefore, the \u2018\u2018Activation function\u2019\u2019 and the \u2018\u2018Momentum\u2019\u2019 are\nrepresented by the component \ud835\udc65cat\nneu. The \u2018\u2018Learning rate\u2019\u2019 and the\n\u2018\u2018Batch size\u2019\u2019 are represented by the component \ud835\udc65qnt\nneu.\nTo model hierarchical variables, as proposed in [38], we separate\nthe input space \ue244 as (\ue244neu, \ue244met, \ue244dec) where \ue244dec =\n\u22c3\n\ud835\udc65met\u2208\ue244met\n\ue244inc(\ud835\udc65met).\nAdvances in Engineering Software 188 (2024) 103571\n7\nP. Saves et al.\nFig. 4. The Multi-Layer Perceptron (MLP) problem.\nSource: Figure adapted from [38, Figure 1].\nTable 5\nA detailed description of the variables in the MLP problem.\nMLP\nHyperparameters\nVariable\nDomain\nType\nRole\nLearning rate\n\ud835\udc5f\n[10\u22125, 10\u22122]\nFLOAT\nNEUTRAL\nMomentum\n\ud835\udefc\n[0, 1]\nFLOAT\nNEUTRAL\nActivation\nfunction\n\ud835\udc4e\n{\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48, \ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51, \ud835\udc47 \ud835\udc4e\ud835\udc5b\u210e}\nENUM\nNEUTRAL\nBatch size\n\ud835\udc4f\n{8, 16, \u2026 , 128, 256}\nORD\nNEUTRAL\n# of hidden\nlayers\n\ud835\udc59\n{1, 2, 3}\nORD\nMETA\n# of neurons\nhidden layer \ud835\udc58\n\ud835\udc5b\ud835\udc58\n{50, 51, \u2026 , 55}\nORD\nDECREED\nHence, for a given point \ud835\udc65 \u2208 \ue244, one has \ud835\udc65 = (\ud835\udc65neu, \ud835\udc65met, \ud835\udc65inc(\ud835\udc65met)), where\n\ud835\udc65neu \u2208 \ue244neu, \ud835\udc65met \u2208 \ue244met and \ud835\udc65inc(\ud835\udc62met) \u2208 \ue244inc(\ud835\udc62met) are defined as follows:\n\u2022 The components \ud835\udc65neu \u2208 \ue244neu gather all neutral variables that\nare not impacted by the meta variables but needed. For ex-\nample, in the MLP problem, \ue244neu gathers the possible learning\nrates, momentum, activation functions and batch sizes. Namely,\nfrom Table 5, \ue244neu = [10\u22125, 10\u22122]\u00d7[0, 1] \u00d7 {ReLu, Sigmoid, Tanh}\u00d7\n{8, 16, \u2026 , 256}.\n\u2022 The components \ud835\udc65met gather the meta (also known as dimen-\nsional) variables that determine the inclusion or exclusion of\nother variables. For example, in the MLP problem, \ue244met represents\nthe possible numbers of layers in the MLP. Namely, from Table 5,\n\ue244met = {1, 2, 3}.\n\u2022 The components \ud835\udc65inc(\ud835\udc65met), contain the decreed variables whose\ninclusion (decreed-included) or exclusion (decreed-excluded) is\ndetermined by the values of the meta components \ud835\udc65met. For exam-\nple, in the MLP problem, \ue244dec represents the number of neurons\nin the decreed layers. Namely, from Table 5, \ue244inc(\ud835\udc65met = 3) =\n[50, 55]3, \ue244inc(\ud835\udc65met = 2) = [50, 55]2 and \ue244inc(\ud835\udc65met = 1) = [50, 55].\n4.2. A Kriging model for hierarchical variables\nIn this section, a new method to build a Kriging model with hierar-\nchical variables is introduced based on the framework aforementioned.\nThe proposed methods are included in SMT 2.0.\n4.2.1. Motivation and state-of-the-art\nAssuming that the decreed variables are quantitative, Hutter and\nOsborne [21] proposed several kernels for the hierarchical context. A\nclassic approach, called the imputation method (Imp-Kernel) leads\nto an efficient paradigm in practice that can be easily integrated into\na more general framework as proposed by Bussemaker et al. [24].\nHowever, this kernel lacks depth and depends on arbitrary choices.\nTherefore, Hutter and Osborne [21] also proposed a more general\nkernel, called Arc-Kernel and Horn et al. [36] generalized this\nkernel even more and proposed a new formulation called the Wedge-\nKernel [37]. The drawbacks of these two methods are that they add\nsome extra hyperparameters for every decreed dimension (respectively\none extra hyperparameter for the Arc-Kernel and two hyperparam-\neters for the Wedge-Kernel) and that they need a normalization\naccording to the bounds. More recently, Pelamatti et al. [60] developed\na hierarchical kernel for Bayesian optimization. However, our work\nis also more general thanks to the framework introduced earlier [38]\nthat considers variable-wise formulation and is more flexible as we are\nallowing sub-problems to be intersecting.\nIn the following, we describe our new method to build a correlation\nkernel for hierarchical variables. In particular, we introduce a new alge-\nbraic kernel called Alg-Kernel that behaves like the Arc-Kernel\nwhilst correcting most of its drawbacks. In particular, our kernel does\nnot add any hyperparameters, and the normalization is handled in a\nnatural way.\n4.2.2. A new hierarchical correlation kernel\nFor modeling purposes, we assume that the decreed space is quan-\ntitative, i.e., \ue244dec = \ue244qnt\ndec. Let \ud835\udc62 \u2208 \ue244 be an input point partitioned as\nAdvances in Engineering Software 188 (2024) 103571\n8\nP. Saves et al.\n\ud835\udc62 = (\ud835\udc62neu, \ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)) and, similarly, \ud835\udc63 \u2208 \ue244 is another input such\nthat \ud835\udc63 = (\ud835\udc63neu, \ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)). The new kernel \ud835\udc58 that we propose for\nhierarchical variables is given by\n\ud835\udc58(\ud835\udc62, \ud835\udc63) = \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) \u00d7 \ud835\udc58met(\ud835\udc62met, \ud835\udc63met)\n\u00d7 \ud835\udc58met,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)]),\n(2)\nwhere \ud835\udc58neu, \ud835\udc58met and \ud835\udc58met,dec are as follows:\n\u2022 \ud835\udc58neu represents the neutral kernel that encompasses both categor-\nical and quantitative neutral variables, i.e., \ud835\udc58neu can be decom-\nposed into two parts \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat(\ud835\udc62cat\nneu, \ud835\udc63cat\nneu)\ud835\udc58qnt(\ud835\udc62qnt\nneu, \ud835\udc63qnt\nneu).\nThe categorical kernel, denoted \ud835\udc58cat, could be any Symmetric\nPositive Definite (SPD) [34] mixed kernel (see Section 3). For\nthe quantitative (integer or continuous) variables, a distance-\nbased kernel is used. The chosen quantitative kernel (Exponential,\nMat\u00e9rn, ...), always depends on a given distance \ud835\udc51. For example,\nthe \ud835\udc5b-dimensional exponential kernel gives\n\ud835\udc58qnt(\ud835\udc62qnt, \ud835\udc63qnt) =\n\ud835\udc5b\n\u220f\n\ud835\udc56=1\nexp(\u2212\ud835\udc51(\ud835\udc62qnt\n\ud835\udc56\n, \ud835\udc63qnt\n\ud835\udc56\n)).\n(3)\n\u2022 \ud835\udc58met is the meta variables related kernel. It is also separated into\ntwo parts: \ud835\udc58met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc58cat(\ud835\udc62cat\nmet, \ud835\udc63cat\nmet)\ud835\udc58qnt(\ud835\udc62qnt\nmet, \ud835\udc63qnt\nmet) where the\nquantitative kernel is ordered and not continuous because meta\nvariables take value in a finite set.\n\u2022 \ud835\udc58met,dec is an SPD kernel that models the correlations between the\nmeta levels (all the possible subspaces) and the decreed variables.\nIn what comes next, we detailed this kernel.\n4.2.3. Towards an algebraic meta-decreed kernel\nMeta-decreed\nkernels\nlike\nthe\nimputation\nkernel\nor\nthe\nArc-Kernel were first proposed in [21,47] and the distance-based\nkernels such as Arc-Kernel or Wedge-Kernel [37] were proven\nto be SPD. Nevertheless, to guarantee this SPD property, the same\nhyperparameters are used to model the correlations between the meta\nlevels and between the decreed variables [47]. For this reason, the\nArc-Kernel includes additional continuous hyperparameters which\nmakes the training of the GP models more expensive and introduces\nmore numerical stability issues. In this context, we are proposing a\nnew algebraic meta-decreed kernel (denoted as Alg-Kernel) that\nenjoys similar properties as Arc-Kernel but without using additional\ncontinuous hyperparameters nor rescaling. In the SMT 2.0 release, we\nincluded Alg-Kernel and a simpler version of Arc-Kernel that do\nnot relies on additional hyperparameters.\nOur proposed Alg-Kernel kernel is given by\n\ud835\udc58alg\nmet,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)])\n= \ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) \u00d7 \ud835\udc58alg\ndec(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)).\n(4)\nMathematically, we could consider that there is only one meta variable\nwhose levels correspond to every possible included subspace. Let \ud835\udc3csub\ndenotes the components indices of possible subspaces, the subspaces\nparameterized by the meta component \ud835\udc62met are defined as \ue244inc(\ud835\udc62met =\n\ud835\udc59), \ud835\udc59 \u2208 \ud835\udc3csub. It follows that the fully extended continuous decreed\nspace writes as \ue244dec = \u22c3\n\ud835\udc59\u2208\ud835\udc3csub \ue244inc(\ud835\udc62met = \ud835\udc59) and \ud835\udc3cdec is the set of the\nassociated indices. Let \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63\ndenotes the set of components related to\nthe space \ue244inc(\ud835\udc62met, \ud835\udc63met) containing the variables decreed-included in\nboth \ue244inc(\ud835\udc62met) and \ue244inc(\ud835\udc63met).\nSince the decreed variables are quantitative, one has\n\ud835\udc58alg\ndec(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)) = \ud835\udc58qnt(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met))\n=\n\u220f\n\ud835\udc56\u2208\ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63\n\ud835\udc58qnt([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56)\n(5)\nThe construction of the quantitative kernel \ud835\udc58qnt depends on a given\ndistance denoted \ud835\udc51alg. The kernel \ud835\udc58alg\nmet is an induced meta kernel that\ndepends on the same distance \ud835\udc51alg to preserve the SPD property of\n\ud835\udc58alg\nmet,dec. For every \ud835\udc56 \u2208 \ud835\udc3cdec, if \ud835\udc56 \u2208 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63 , the new algebraic distance is\ngiven by\n\ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) =\n\u239b\n\u239c\n\u239c\n\u239c\u239d\n2|[\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56 \u2212 [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56|\n\u221a\n[\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56\n2 + 1\n\u221a\n[\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56\n2 + 1\n\u239e\n\u239f\n\u239f\n\u239f\u23a0\n\ud835\udf03\ud835\udc56,\n(6)\nwhere \ud835\udf03\ud835\udc56 \u2208 R+ is a continuous hyperparameter. Otherwise, if \ud835\udc56 \u2208 \ud835\udc3cdec\nbut \ud835\udc56 \u2209 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63 , there should be a non-zero residual distance between the\ntwo different subspaces \ue244inc(\ud835\udc62met) and \ue244inc(\ud835\udc63met) to ensure the kernel\nSPD property. To have a residual not depending on the decreed values,\nour model considers that there is a unit distance\n\ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) = 1.0 \ud835\udf03\ud835\udc56, \u2200\ud835\udc56 \u2208 \ud835\udc3cdec \u29f5 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63 .\nThe induced meta kernel \ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) to preserve the SPD property\nof \ud835\udc58alg is defined as:\n\ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) =\n\u220f\n\ud835\udc56\u2208\ud835\udc3cmet\n\ud835\udc58qnt(1.0 \ud835\udf03\ud835\udc56).\n(7)\nNot only our kernel of Eq. (2) uses less hyperparameters than the Arc-\nKernel (as we cut off its extra parameters) but it is also a more flexible\nkernel as it allows different kernels for meta and decreed variables.\nMoreover, another advantage of our kernel is that it is numerically\nmore stable thanks to the new non-stationary [61] algebraic distance\ndefined in Eq. (7) [62]. Our proposed distance also does not need any\nrescaling by the bounds to have values between 0 and 1.\nIn what comes next, we will refer to the implementation of the\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\nSMT Alg-Kernel. We note also that the implementation of SMT\nArc-Kernel differs slightly from the original Arc-Kernel as we\nfixed some hyperparameters to 1 in order to avoid adding extra hy-\nperparameters and use the formulation of Eq. (2) and rescaling of the\ndata.\n4.2.4. Illustration on the MLP problem\nIn this section, we illustrate the hierarchical Arc-Kernel on the\nMLP example. For that sake, we consider two design variables \ud835\udc62 and\n\ud835\udc63 such that \ud835\udc62 = (2.10\u22124, 0.9, ReLU, 16, 2, 55, 51) and \ud835\udc63 = (5.10\u22123, 0.8,\nSigmoid, 64, 3, 50, 54, 53). Since the value of \ud835\udc62met (i.e., the number of\nhidden layers) differs from one point to another (namely, 2 for \ud835\udc62 and 3\nfor \ud835\udc63), the associated variables \ud835\udc62inc(\ud835\udc62met) have either 2 or 3 variables\nfor the number of neurons in each layer (namely 55 and 51 for \ud835\udc62,\nand 50, 54 and 53 for the point \ud835\udc63). In our case, 8 hyperparame-\nters ([\ud835\udc451]1,2, \ud835\udf031, \u2026 , \ud835\udf037) will have to be optimized where \ud835\udc58 is given by\nEq. (2). These 7 hyperparameters can be described using our proposed\nframework as follows:\n\u2022 For the neutral components, we have \ud835\udc62neu = (2.10\u22124, 0.9, ReLU, 16)\nand \ud835\udc63neu = (5.10\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\nmatrix kernel \ud835\udc451 and a square exponential quantitative kernel,\n\ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat(\ud835\udc62cat\nneu, \ud835\udc63cat\nneu)\ud835\udc58qnt(\ud835\udc62qnt\nneu, \ud835\udc63qnt\nneu)\n= [\ud835\udc451]1,2 exp [\u2212\ud835\udf031(2.10\u22124 \u2212 5.10\u22123)2]\nexp [\u2212\ud835\udf032(0.9 \u2212 0.8)2] exp [\u2212\ud835\udf033(16 \u2212 64)2].\nThe values [\ud835\udc451]1,2, \ud835\udf031, \ud835\udf032 and \ud835\udf033 need to be optimized. Here,\n[\ud835\udc451]1,2 is the correlation between \"ReLU\" and \"Sigmoid\".\n\u2022 For the meta components, we have \ud835\udc62met\n= 2 and \ud835\udc63met\n= 3.\nTherefore, for a square exponential quantitative kernel,\n\ud835\udc58met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc58cat(\ud835\udc62cat\nmet, \ud835\udc63cat\nmet)\ud835\udc58qnt(\ud835\udc62qnt\nmet, \ud835\udc63qnt\nmet)\n= exp [\u2212\ud835\udf034(3 \u2212 2)2].\nThe value \ud835\udf034 needs to be optimized.\nAdvances in Engineering Software 188 (2024) 103571\n9\nP. Saves et al.\n\u2022 For the meta-decreed kernel, we have [\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)] = [2, (55, 51)]\nand [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)] = [3, (50, 54, 53)] which gives\n\ud835\udc58alg\nmet,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)])\n= \ud835\udc58alg\nmet(2, 3) \ud835\udc58alg\ndec((55, 51), (50, 54, 53)).\nThe distance \ud835\udc51alg(51, 54) =\n(\n2\u00d7|51\u221254|\n\u221a\n512+1\n\u221a\n542+1\n)\n\ud835\udf036 = 2.178.10\u22123 \ud835\udf036. In\ngeneral, for surrogate models, and in particular in SMT 2.0, the\ninput data are normalized. With a unit normalization from [50, 55]\nto [0, 1], we would have \ud835\udc51alg(0.2, 0.8) =\n(\n2\u00d70.6\n\u221a\n0.22+1\n\u221a\n0.62+1\n)\n\ud835\udf036 =\n0.919 \ud835\udf036. Similarly, we have, between 55 and 50, \ud835\udc51alg(0, 1) =\n1.414 \ud835\udf035. Hence, for a square exponential quantitative kernel, one\ngets\n\ud835\udc58alg\nmet,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)])\n= exp [\u2212\ud835\udf037] \u00d7 exp [\u22121.414 \ud835\udf035] \u00d7 exp [\u22120.919 \ud835\udf036],\nwhere the meta induced component is \ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) = exp [\u2212\ud835\udf037]\nbecause the decreed value 53 in \ud835\udc63 has nothing to be compared\nwith in \ud835\udc62 as in Eq. (7). The values \ud835\udf035, \ud835\udf036 and \ud835\udf037 need to be opti-\nmized which complete the description of the hyperparameters.\nWe note that for the MLP problem, Alg-Kernel models use\n10 hyperparameters whereas the Arc-Kernel would require\n12 hyperparameters without the meta kernel (\ud835\udf034) but with 3\nextra decreed hyperparameters and the Wedge-Kernel would\nrequire 15 hyperparameters. For deep learning applications, a\nmore complex perceptron with up to 10 hidden layers would\nrequire 17 hyperparameters with SMT 2.0 models against 26\nfor Arc-Kernel and 36 for Wedge-Kernel. The next section\nillustrates the interest of our method to build a surrogate model\nfor this neural network engineering problem.\n4.3. A neural network test-case using SMT 2.0\nIn this section, we apply our models to the hyperparameters opti-\nmization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\nan example illustrates this MLP problem. For the sake of showing the\nKriging surrogate abilities, we implemented a dummy function with no\nsignificance to replace the real black-box that would require training\na whole Neural Network (NN) with big data. This function requires a\nnumber of variables that depends on the value of the meta variable,\ni.e the number of hidden layers. To simplify, we have chosen only\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\nbut deep neural networks could also be investigated as our model can\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN)\nshows that our model SMT Alg-Kernel interpolates the data prop-\nerly, checks that the data dimension is correct and also asserts that\nthe inactive decreed variables have no influence over the prediction.\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\nand mixed variables based on the implementation of SMT 2.0 for\ntest_hierarchical_variables_NN.\nTo compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\nand SMT Arc-Kernel) with the state-of-the-art imputation method\npreviously used on industrial application (Imp-Kernel) [24], we\ndraw a 99 point LHS (33 points by meta level) as a training set and the\nvalidation set is a LHS of 3\u00d71000 = 3000 points. For the Imp-Kernel,\nthe decreed-excluded values are replaced by 52 because the mean value\n52.5 is not an integer (by default, SMT rounds to the floor value).\nFor the three methods, the precision (computed with a root-mean-\nsquare error RMSE criterion), the likelihood and the computational\ntime are shown in Table 6. For this problem, we can see that SMT Alg-\nkernel gives better performance than the imputation method or SMT\nArc-kernel. Also, as all methods use the same number of hyperpa-\nrameters, they have similar time performances. A direct application of\nTable 6\nResults on the neural network model.\nHierarchical method\nPrediction\nerror (RMSE)\nLikelihood\n# of\nhyperparam.\nSMT Alg-kernel\n3.7610\n176.11\n10\nSMT Arc-kernel\n4.9208\n162.01\n10\nImp-Kernel\n4.5455\n170.64\n10\nour modeling method is Bayesian optimization to perform quickly the\nhyperparameter optimization of a neural network [63].\n5. Bayesian optimization within SMT 2.0\nEfficient global optimization (EGO) is a sequential Bayesian op-\ntimization algorithm designed to find the optimum of a black-box\nfunction that may be expensive to evaluate [52]. EGO starts by fitting\na Kriging model to an initial DoE, and then uses an acquisition function\nto select the next point to evaluate. The most used acquisition function\nis the expected improvement. Once a new point has been evaluated, the\nKriging model is updated. Successive updates increase the model accu-\nracy over iterations. This enrichment process repeats until a stopping\ncriterion is met.\nBecause SMT 2.0 implements Kriging models that handle mixed\nand hierarchical variables, we can use EGO to solve problems in-\nvolving such design variables. Other Bayesian optimization algorithms\noften adopt approaches based on solving subproblems with contin-\nuous or non-hierarchical Kriging. This subproblem approach is less\nefficient and scales poorly, but it can only solve simple problems.\nSeveral Bayesian optimization software packages can handle mixed or\nhierarchical variables with such a subproblem approach. The pack-\nages include BoTorch [25], SMAC [65], Trieste [66], HEBO [67],\nOpenBox [68], and Dragonfly [69].\n5.1. A mixed optimization problem\nFig. 6 compares the four EGO methods implemented in SMT 2.0:\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\nillustrates Bayesian optimization is a toy test case [64] detailed in Ap-\npendix A. This test case has two variables, one continuous and one\ncategorical with 10 levels. To assess the performance of our algorithm,\nwe performed 20 runs with different initial DoE sampled by LHS.\nEvery DoE consists of 5 points and we chose a budget of 55 infill\npoints. Fig. 6(a) plots the convergence curves for the four methods. In\nparticular, the median is the solid line, and the first and third quantiles\nare plotted in dotted lines. To visualize better the data dispersion,\nthe boxplots of the 20 best solutions after 20 evaluations are plotted\nin Fig. 6(b). As expected, the more a method is complex, the better\nthe optimization. Both SMT HH and SMT EHH converged in around 18\nevaluations whereas SMT CR and SMT GD take around 26 iterations as\nshown on Fig. 6(a). Also, the more complex the model, the closer the\noptimum is to the real value as shown on Fig. 6(b).\nIn Fig. 7 we illustrate how to use EGO with mixed variables based\non the implementation of SMT 2.0. The illustrated problem is a mixed\nvariant of the Branin function [70].\nNote that a dedicated notebook is available to reproduce the results\npresented in this paper and the mixed integer notebook also includes\nan extra mechanical application with composite materials [41].11\n11 https://colab.research.google.com/github/SMTorg/smt/blob/master/\ntutorial/SMT_MixedInteger_application.ipynb\nAdvances in Engineering Software 188 (2024) 103571\n10\nP. Saves et al.\nFig. 5. Example of usage of Hierarchical and Mixed Kriging surrogate.\nAdvances in Engineering Software 188 (2024) 103571\n11\nP. Saves et al.\nFig. 6. Optimization results for the Toy function [64].\nFig. 7. Example of usage of mixed surrogates for Bayesian optimization.\nAdvances in Engineering Software 188 (2024) 103571\n12\nP. Saves et al.\nFig. 8. Optimization results for the hierarchical Goldstein function.\n5.2. A hierarchical optimization problem\nThe hierarchical test case considered in this paper to illustrate\nBayesian optimization is a modified Goldstein function [60] detailed\nin Appendix B. The resulting optimization problem involves 11 vari-\nables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical.\nThese variables consist in 6 neutral variables, 1 dimensional (or meta)\nvariable and 4 decreed variables. Depending on the meta variable\nvalues, 4 different sub-problems can be identified. The optimization\nproblem is given by:\nmin \ud835\udc53(\ud835\udc65cat\nneu, \ud835\udc65qnt\nneu, \ud835\udc65cat\n\ud835\udc5a , \ud835\udc65qnt\ndec)\nw.r.t. \ud835\udc65cat\nneu = \ud835\udc642 \u2208 {0, 1}\n\ud835\udc65qnt\nneu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) \u2208 {0, 100}3 \u00d7 {0, 1, 2}2\n\ud835\udc65cat\n\ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3}\n\ud835\udc65qnt\ndec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) \u2208 {0, 100}2 \u00d7 {0, 1, 2}2\n(8)\nCompared to the model choice of Pelamatti et al. [60], we chose to\nmodel \ud835\udc655 and \ud835\udc642 as neutral variables even if \ud835\udc53 does not depend on\n\ud835\udc655 when \ud835\udc642 = 0. Other modeling choices are kept; for example, \ud835\udc642 is\na so-called \u2018\u2018binary variable\u2019\u2019 and not a categorical one [71]. Similarly,\nwe also keep the formulation of \ud835\udc641 as a categorical variable but a better\nmodel would be to model it as a \u2018\u2018cyclic variable\u2019\u2019 [72]. The resulting\nproblem is described in Appendix B. To assess the performance of our\nalgorithm, we performed 20 runs with different initial DoE sampled by\nLHS. Every DoE consists of \ud835\udc5b + 1 = 12 points and we chose a budget\nof 5\ud835\udc5b = 55 infill points. To compare our method with a baseline, we\nalso tested the random search method thanks to the expand_lhs\nnew method [40] described in Section 6.1 and we also optimized the\nGoldstein function using EGO with a classic Kriging model based on\nimputation method (Imp-Kernel). This method replaces the decreed-\nexcluded variables by their mean values: 50 or 1 respectively for (\ud835\udc653, \ud835\udc654)\nand (\ud835\udc671, \ud835\udc672). Fig. 8(a) plots the convergence curves for the four methods.\nIn particular, the median is the solid line and the first and third\nquantiles are plotted in dotted lines. To visualize better the correspond-\ning data dispersion, the boxplots of the 20 best solutions are plotted\nin Fig. 8(b). The results in Fig. 8 show that the hierarchical Kriging\nmodels of SMT 2.0 lead to better results than the imputation method\nor the random search both in terms of final objective value and variance\nover the 20 runs and in term of convergence rate. More precisely, SMT\nArc-Kernel and SMT Alg-Kernel Kriging model gave the best\nEGO results and managed to converge correctly as shown in Fig. 8(b).\nMore precisely, the 20 sampled DoEs led to similar performance and\nfrom one DoE, the method SMT Alg-Kernel managed to find the true\nminimum. However, this result has not been reproduced in other runs\nand is therefore not statistically significant. The variance between the\nruns is of similar magnitude regardless of the considered methods.\n6. Other relevant contributions in SMT 2.0\nThe new release SMT 2.0 introduces several improvements be-\nsides Kriging for hierarchical and mixed variables. This section details\nthe most important new contributions. Recall from Section 2.2 that\nfive sub-modules are present in the code: Sampling, Problems,\nSurrogate Models, Applications and Notebooks.\n6.1. Contributions to Sampling\nPseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\nstochastic sampling technique to generate quasi-random sampling dis-\ntributions. It is among the most popular sampling method in computer\nexperiments thanks to its simplicity and projection properties with\nhigh-dimensional problems. The LHS method uses the pyDOE package\n(Design Of Experiments for Python). Five criteria for the construction\nof LHS are implemented in SMT. The first four criteria (center,\nmaximin, centermaximin, correlation) are the same as in\npyDOE.12 The last criterion ese, is implemented by the authors of\nSMT [48]. In SMT 2.0 a new LHS method was developed for the\nNested design of experiments (NestedLHS) [73] to use with multi-\nfidelity surrogates. A new mathematical method (expand_lhs) [40]\nwas developed in SMT 2.0 to increase the size of a design of exper-\niments while maintaining the ese property. Moreover, we proposed\na sampling method for mixed variables, and the aforementioned LHS\nmethod was applied to hierarchical variables in Fig. 8.\n6.2. Contributions to Surrogate models\nNew kernels and their derivatives for Kriging. Kriging surrogates are\nbased on hyperparameters and on a correlation kernel. Four correla-\ntion kernels are now implemented in SMT 2.0 [74]. In SMT, these\ncorrelation functions are absolute exponential (abs_exp), Gaussian\n(squar_exp), Matern 5/2 (matern52) and Matern 3/2 (matern32).\nIn addition, the implementation of gradient and Hessian for each kernel\nmakes it possible to calculate both the first and second derivatives of\nthe GP likelihood with respect to the hyperparameters [5].\nVariance derivatives for Kriging. To perform uncertainty quantification\nfor system analysis purposes, it could be interesting to know more\nabout the variance derivatives of a model [75\u201377]. For that purpose\nand also to pursue the original publication about derivatives [5], SMT\n2.0 extends the derivative support to Kriging variances and kernels.\n12 https://pythonhosted.org/pyDOE/index.html\nAdvances in Engineering Software 188 (2024) 103571\n13\nP. Saves et al.\nNoisy Kriging. In engineering and in big data contexts with real exper-\niments, surrogate models for noisy data are of significant interest. In\nparticular, there is a growing need for techniques like noisy Kriging\nand noisy Multi-Fidelity Kriging (MFK) for data fusion [78]. For that\npurpose, SMT 2.0 has been designed to accommodate Kriging and MFK\nto noisy data including the option to incorporate heteroscedastic noise\n(using the use_het_noise option) and to account for different noise\nlevels for each data source [40].\nKriging with partial least squares. Beside MGP, for high-dimensional\nproblems, the toolbox implements Kriging with partial least squares\n(KPLS) [57] and its extension KPLSK [44]. The PLS information is\ncomputed by projecting the data into a smaller space spanned by the\nprincipal components. By integrating this PLS information into the\nKriging correlation matrix, the number of inputs can be scaled down,\nthereby reducing the number of hyperparameters required. The result-\ning number of hyperparameters \ud835\udc51\ud835\udc52 is indeed much smaller than the\noriginal problem dimension \ud835\udc51. Recently, in SMT 2.0, we extended the\nKPLS method for multi-fidelity Kriging (MFKPLS and MFKPLSK) [73,79,\n80]. We also proposed an automatic criterion to choose automatically\nthe reduced dimension \ud835\udc51\ud835\udc52 based on Wold\u2019s R criterion [81]. This\ncriterion has been applied to aircraft optimization with EGO where the\nnumber \ud835\udc51\ud835\udc52 (\ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code) for the model is automatically selected\nat every iteration [39]. Special efforts have been made to accommodate\nKPLS for multi-fidelity and mixed integer data [79,80].\nMarginal Gaussian process. SMT 2.0 implements Marginal Gaussian\nProcess (MGP) surrogate models for high dimensional problems [82].\nMGP are Gaussian processes taking into account hyperparameters un-\ncertainty defined as a density probability function. Especially we sup-\npose that the function to model \ud835\udc53 \u2236 \ud835\udefa \u21a6 R, where \ud835\udefa \u2282 R\ud835\udc51 and \ud835\udc51 is\nthe number of design variables, lies in a linear embedding \ue22d such as\n\ue22d = {\ud835\udc62 = \ud835\udc34\ud835\udc65, \ud835\udc65 \u2208 \ud835\udefa}, \ud835\udc34 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc52 and \ud835\udc53(\ud835\udc65) = \ud835\udc53\ue22d(\ud835\udc34\ud835\udc65) with \ud835\udc53(\ud835\udc65) = \ud835\udc53\ue22d \u2236\n\ue22d \u21a6 R and \ud835\udc51\ud835\udc52 \u226a \ud835\udc51. Then, we must use a kernel \ud835\udc58(\ud835\udc65, \ud835\udc65\u2032) = \ud835\udc58\ue22d(\ud835\udc34\ud835\udc65, \ud835\udc34\ud835\udc65\u2032)\nwhose each component of the transfer matrix \ud835\udc34 is an hyperparameter.\nThus we have \ud835\udc51\ud835\udc52 \u00d7 \ud835\udc51 hyperparameters to find. Note that \ud835\udc51\ud835\udc52 is defined\nas \ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code [49].\nGradient-enhanced neural network. The new release SMT 2.0 imple-\nments Gradient-Enhanced Neural Network (GENN) models [45].\nGradient-Enhanced Neural Networks (GENN) are fully connected multi-\nlayer perceptrons whose training process was modified to account for\ngradient information. Specifically, the model is trained to minimize not\nonly the prediction error of the response but also the prediction error\nof the partial derivatives: the chief benefit of gradient enhancement is\nbetter accuracy with fewer training points. Note that GENN applies to\nregression (single-output or multi-output), but not classification since\nthere is no gradient in that case. The implementation is fully vectorized\nand uses ADAM optimization, mini-batch, and L2-norm regularization.\nFor example, GENN can be used to learn airfoil geometries from a\ndatabase. This usage is documented in SMT 2.0.13\n6.3. Contributions to Applications\nKriging trajectory and sampling. Sampling a GP with high resolution\nis usually expensive due to the large dimension of the associated\ncovariance matrix. Several methods are proposed to draw samples\nof a GP on a given set of points. To sample a conditioned GP, the\nclassic method consists in using a Cholesky decomposition (or eigende-\ncomposition) of the conditioned covariance matrix of the process but\nsome numerical computational errors can lead to non SPD matrix. A\nmore recent approach based on Karhunen\u2013Lo\u00e8ve decomposition of the\ncovariance kernel with the Nystr\u00f6m method has been proposed in [83]\n13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_\nparameters/learning_airfoil_parameters.html\nwhere the paths can be sampled by generating independent standard\nNormal distributed samples. The different methods are documented in\nthe tutorial Gaussian Process Trajectory Sampling [84].\nParallel Bayesian optimization. Due to the recent progress made in\nhardware configurations, it has been of high interest to perform parallel\noptimizations. A parallel criterion called qEI [85] was developed to\nperform Efficient Global Optimization (EGO): the goal is to be able\nto run batch optimization. At each iteration of the algorithm, multiple\nnew sampling points are extracted from the known ones. These new\nsampling points are then evaluated using a parallel computing environ-\nment. Five criteria are implemented in SMT 2.0: Kriging Believer (KB),\nKriging Believer Upper Bound (KBUB), Kriging Believer Lower Bound\n(KBLB), Kriging Believer Random Bound (KBRand) and Constant Liar\n(CLmin) [86].\n7.\nConclusion\nSMT 2.0 introduces significant upgrades to the Surrogate Modeling\nToolbox. This new release adds support for hierarchical and mixed\nvariables and improves the surrogate models with a particular focus\non Kriging (Gaussian process) models. SMT 2.0 is distributed through\nan open-source license and is freely available online.14 We provide\ndocumentation that caters to both users and potential developers.15\nSMT 2.0 enables users and developers collaborating on the same\nproject to have a common surrogate modeling tool that facilitates the\nexchange of methods and reproducibility of results.\nSMT has been widely used in aerospace and mechanical modeling\napplications. Moreover, the toolbox is general and can be useful for\nanyone who needs to use or develop surrogate modeling techniques,\nregardless of the targeted applications. SMT is currently the only open-\nsource toolbox that can build hierarchical and mixed surrogate models.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nData will be made available on request. Results can be reproduced\nfreely online at https://colab.research.google.com/github/SMTorg/smt/\nblob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\nAcknowledgments\nWe want to thank all those who contribute to this release. Namely,\nM. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde\nArenzana, S. Dubreuil, A. F. L\u00f3pez-Lopera, M. Meliani, M. Menz, N.\nMo\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is\npart of the activities of ONERA - ISAE - ENAC joint research group. We\nalso acknowledge the partners institutions: ONERA, NASA Glenn, ISAE-\nSUPAERO, Institut Cl\u00e9ment Ader (ICA), the University of Michigan,\nPolytechnique Montr\u00e9al and the University of California San Diego.\nThe research presented in this paper has been performed in the\nframework of the AGILE 4.0 project (Towards cyber-physical collabo-\nrative aircraft development), funded by the European Union Horizon\n2020 research and innovation framework programme under grant\nagreement n\u25e6 815122 and in the COLOSSUS project (Collaborative\nSystem of Systems Exploration of Aviation Products, Services and\n14 https://github.com/SMTorg/SMT\n15 https://smt.readthedocs.io/en/latest/\nAdvances in Engineering Software 188 (2024) 103571\n14\nP. Saves et al.\nBusiness Models) funded by the European Union Horizon Europe re-\nsearch and innovation framework programme under grant agreement\nn\u25e6 101097120.\nWe also are grateful to E. Hall\u00e9-Hannan from Polytechnique Mon-\ntr\u00e9al for the hierarchical variables framework.\nAppendix A. Toy test function\nThis Appendix gives the detail of the toy function of Section 5.1.16\nFirst, we recall the optimization problem:\nmin \ud835\udc53(\ud835\udc65cat, \ud835\udc65qnt)\nw.r.t. \ud835\udc65cat = \ud835\udc501 \u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\ud835\udc65qnt = \ud835\udc651 \u2208 [0, 1]\n(A.1)\nThe toy function \ud835\udc53 is defined as\n\ud835\udc53(\ud835\udc65, \ud835\udc501) =1\ud835\udc501=0 cos(3.6\ud835\udf0b(\ud835\udc65 \u2212 2)) + \ud835\udc65 \u2212 1\n+1\ud835\udc501=1 2 cos(1.1\ud835\udf0b exp(\ud835\udc65)) \u2212 \ud835\udc65\n2 + 2\n+1\ud835\udc501=2 cos(2\ud835\udf0b\ud835\udc65) + 1\n2\ud835\udc65\n+1\ud835\udc501=3 \ud835\udc65(cos(3.4\ud835\udf0b(\ud835\udc65 \u2212 1)) \u2212 \ud835\udc65 \u2212 1\n2\n)\n+1\ud835\udc501=4 \u2212 \ud835\udc652\n2\n+1\ud835\udc501=5 2 cos(0.25\ud835\udf0b exp(\u2212\ud835\udc654))2 \u2212 \ud835\udc65\n2 + 1\n+1\ud835\udc501=6 \ud835\udc65 cos(3.4\ud835\udf0b\ud835\udc65) \u2212 \ud835\udc65\n2 + 1\n+1\ud835\udc501=7 \u2212 \ud835\udc65(cos(3.5\ud835\udf0b\ud835\udc65) + \ud835\udc65\n2 ) + 2\n+1\ud835\udc501=8 \u2212 \ud835\udc655\n2 + 1\n+1\ud835\udc501=9 \u2212 cos(2.5\ud835\udf0b\ud835\udc65)2\u221a\n\ud835\udc65 \u2212 0.5 ln(\ud835\udc65 + 0.5) \u2212 1.3\n(A.2)\nAppendix B. Hierarchical Goldstein test function\nThis Appendix gives the detail of the hierarchical Goldstein problem\nof Section 5.2.17 First, we recall the optimization problem:\nmin \ud835\udc53(\ud835\udc65cat\nneu, \ud835\udc65qnt\nneu, \ud835\udc65cat\n\ud835\udc5a , \ud835\udc65qnt\ndec)\nw.r.t. \ud835\udc65cat\nneu = \ud835\udc642 \u2208 {0, 1}\n\ud835\udc65qnt\nneu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) \u2208 [0, 100]3 \u00d7 {0, 1, 2}2\n\ud835\udc65cat\n\ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3}\n\ud835\udc65qnt\ndec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) \u2208 [0, 100]2 \u00d7 {0, 1, 2}2\n(B.1)\nThe hierarchical and mixed function \ud835\udc53 is defined as a hierarchical\nfunction that depends on \ud835\udc530, \ud835\udc531, \ud835\udc532 and \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as describes in the\nfollowing.\n\ud835\udc53(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc641, \ud835\udc642) =\n1\ud835\udc641=0\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc641=1\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc641=2\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc641=3\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642).\n(B.2)\n16 https://github.com/jbussemaker/SBArchOpt\n17 https://github.com/jbussemaker/SBArchOpt\nThen, the functions \ud835\udc530, \ud835\udc531 and \ud835\udc532 are defined as mixed variants of\n\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as such\n\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =\n1\ud835\udc672=0\n( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) )\n1\ud835\udc672=1\n( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) )\n1\ud835\udc672=2\n( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) )\n(B.3)\n\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =\n1\ud835\udc672=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc672=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc672=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =\n1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, 50, \ud835\udc652, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\nTo finish with, the function \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont is given by\n\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 53.3108 + 0.184901\ud835\udc651\n\u2212 5.02914\ud835\udc651\n3.10\u22126 + 7.72522\ud835\udc651\n\ud835\udc673.10\u22128 \u2212 0.0870775\ud835\udc652 \u2212 0.106959\ud835\udc653\n+ 7.98772\ud835\udc653\n\ud835\udc674.10\u22126 + 0.00242482\ud835\udc654 + 1.32851\ud835\udc654\n3.10\u22126 \u2212 0.00146393\ud835\udc651\ud835\udc652\n\u2212 0.00301588\ud835\udc651\ud835\udc653 \u2212 0.00272291\ud835\udc651\ud835\udc654 + 0.0017004\ud835\udc652\ud835\udc653 + 0.0038428\ud835\udc652\ud835\udc654\n\u2212 0.000198969\ud835\udc653\ud835\udc654 + 1.86025\ud835\udc651\ud835\udc652\ud835\udc653.10\u22125 \u2212 1.88719\ud835\udc651\ud835\udc652\ud835\udc654.10\u22126\n+ 2.50923\ud835\udc651\ud835\udc653\ud835\udc654.10\u22125 \u2212 5.62199\ud835\udc652\ud835\udc653\ud835\udc654.10\u22125 + \ud835\udc642\n(\n5 cos\n( 2\ud835\udf0b\n100 \ud835\udc655\n)\n\u2212 2\n)\n.\n(B.4)\nAppendix C. Supplementary data\nMore at https://colab.research.google.com/github/SMTorg/smt/blob/\nmaster/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\nSupplementary material related to this article can be found online\nat https://doi.org/10.1016/j.advengsoft.2023.103571.\nReferences\n[1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach\nfor the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\u201373.\n[2] Kennedy M, O\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc\nSer B Stat Methodol 2001;63:425\u201364.\n[3] Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets.\nAerosp Sci Technol 2018;75:74\u201387.\n[4] Martins JRRA, Ning A. Engineering design optimization. Cambridge University\nPress; 2021.\n[5] Bouhlel\nMA,\nHwang\nJT,\nBartoli\nN,\nLafage\nR,\nMorlier\nJ,\nMartins\nJRA.\nA Python surrogate modeling framework with derivatives. Adv Eng Softw\n2019;135:102662.\n[6] Bouhlel\nMA,\nMartins\nJ.\nGradient-enhanced\nkriging\nfor\nhigh-dimensional\nproblems. Eng Comput 2019;35:157\u201373.\n[7] Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikit-learn:\nMachine learning in Python. J Mach Learn Res 2011;12:2825\u201330.\nAdvances in Engineering Software 188 (2024) 103571\n15\nP. Saves et al.\n[8] Lataniotis C, Marelli S, Sudret B. Uqlab 2.0 and uqcloud: open-source vs.\ncloud-based uncertainty quantification. In: SIAM conference on uncertainty\nquantification. 2022.\n[9] Faraci A, Beaurepaire P, Gayton N. Review on Python toolboxes for Kriging\nsurrogate modelling. In: ESREL. 2022.\n[10] Kr\u00fcgener M, Zapata Usandivaras J, Bauerheim M, Urbano A. Coaxial-injector\nsurrogate modeling based on Reynolds-averaged Navier\u2013Stokes simulations using\ndeep learning. J Propuls Power 2022;38:783\u201398.\n[11] Ming D, Williamson D, Guillas S. Deep Gaussian process emulation using\nstochastic imputation. Technometrics 2022;1\u201312.\n[12] Eli\u00e1\u0161 J, Vo\u0159echovsk`y M, Sad\u00edlekv V. Periodic version of the minimax distance\ncriterion for Monte Carlo integration. Adv Eng Softw 2020;149:102900.\n[13] Drouet V, Balesdent M, Brevault L, Dubreuil S, Morio J. Multi-fidelity algo-\nrithm for the sensitivity analysis of multidisciplinary problems. J Mech Des\n2023;145:1\u201322.\n[14] Karban P, P\u00e1nek D, Orosz T, Petr\u00e1\u0161ov\u00e1 I, Dole\u017eel I. FEM based robust design\noptimization with Agros and \u00afArtap. Comput Math Appl 2021;81:618\u201333.\n[15] Kudela J, Matousek R. Recent advances and applications of surrogate models for\nfinite element method computations: a review. Soft Comput 2022;26:13709\u201333.\n[16] Chen Y, Dababneh F, Zhang B, Kassaee S, Smith BT, Liu K, et al. Surrogate mod-\neling for capacity planning of charging station equipped with photovoltaic panel\nand hydropneumatic energy storage. J Energy Res Technol 2020;142:050907.\n[17] Jasa\nJ,\nBortolotti\nP,\nZalkind\nD,\nBarter\nG.\nEffectively\nusing\nmultifidelity\noptimization for wind turbine design. Wind Energy Sci 2022;7:991\u20131006.\n[18] Wang W, Tao G, Ke D, Luo J, Cui J. Transpiration cooling of high pres-\nsure turbine vane with optimized porosity distribution. Appl Therm Eng\n2023;223:119831.\n[19] Savage T, Almeida-Trasvina HF, del R\u00edo-Chanona EA, Smith R, Zhang D.\nAn adaptive data-driven modelling and optimization framework for complex\nchemical process design. Comput Aided Chem Eng 2020;48:73\u20138.\n[20] Chan A, Pires AF, Polacsek T. Trying to elicit and assign goals to the right actors.\nIn: Conceptual modeling: 41st international conference. 2022.\n[21] Hutter F, Osborne MA. A kernel for hierarchical parameter spaces. 2013, arXiv.\n[22] Bussemaker JH, Ciampa PD, Nagel B. System architecture design space explo-\nration: An approach to modeling and optimization. In: AIAA aviation 2020 forum.\n2020.\n[23] Fouda MEA, Adler EJ, Bussemaker J, Martins JRRA, Kurtulus DF, Boggero L,\net al. Automated hybrid propulsion model construction for conceptual aircraft\ndesign and optimization. In: 33rd congress of the international council of the\naeronautical sciences. 2022.\n[24] Bussemaker JH, Bartoli N, Lefebvre T, Ciampa PD, Nagel B. Effectiveness of\nsurrogate-based optimization algorithms for system architecture optimization. In:\nAIAA aviation 2021 forum. 2021.\n[25] Balandat M, Karrer B, Jiang D, Daulton S, Letham B, Wilson A, et al. BoTorch:\nA framework for efficient Monte-Carlo Bayesian optimization. Adv Neural Inf\nProcess Syst 2020;33:21524\u201338.\n[26] Adams B, Bohnhoff W, Dalbey K, Ebeida M, Eddy J, Eldred M, et al. Dakota,\na multilevel parallel object-oriented framework for design optimization, pa-\nrameter estimation, uncertainty quantification, and sensitivity analysis: Version\n6.13 user\u2019s manual. Technical report, Albuquerque, NM (United States: Sandia\nNational Lab.(SNL-NM); 2020.\n[27] Roustant O, Ginsbourger D, Deville Y. DiceKriging, DiceOptim: Two R packages\nfor the analysis of computer experiments by Kriging-based metamodeling and\noptimization. J Stat Softw 2012;51:1\u201355.\n[28] Zhang Y, Tao S, Chen W, Apley D. A latent variable approach to Gaus-\nsian process modeling with qualitative and quantitative factors. Technometrics\n2020;62:291\u2013302.\n[29] Chang TH, Wild SM. ParMOO: A Python library for parallel multiobjective\nsimulation optimization. J Open Source Softw 2023;8:4468.\n[30] Garrido-Merch\u00e1n\nEC,\nHern\u00e1ndez-Lobato\nD.\nDealing\nwith\ncategorical\nand\ninteger-valued variables in Bayesian optimization with Gaussian processes.\nNeurocomputing 2020;380:20\u201335.\n[31] Halstrup M. Black-box optimization of mixed discrete-continuous optimization\nproblems (Ph.D. thesis), TU Dortmund; 2016.\n[32] Roustant O, Padonou E, Deville Y, Cl\u00e9ment A, Perrin G, Giorla J, et al.\nGroup kernels for gaussian process metamodels with categorical inputs. SIAM\nJ Uncertain Quant 2020;8:775\u2013806.\n[33] Zhou Q, Qian PZG, Zhou S. A simple approach to emulation for computer models\nwith qualitative and quantitative factors. Technometrics 2011;53:266\u201373.\n[34] Saves P, Diouane Y, Bartoli N, Lefebvre T, Morlier J. A mixed-categorical\ncorrelation kernel for Gaussian process. Neurocomputing 2023;550:126472.\n[35] Pelamatti\nJ,\nBrevault\nL,\nBalesdent\nM,\nTalbi\nE-G,\nGuerin\nY.\nEfficient\nglobal optimization of constrained mixed variable problems. J Global Optim\n2019;73:583\u2013613.\n[36] Horn D, Stork J, ler N-JS, Zaefferer M. Surrogates for hierarchical search spaces:\nThe Wedge-Kernel and an automated analysis. In: Proceedings of the genetic and\nevolutionary computation conference. 2019.\n[37] Hung Y, Joseph VR, Melkote SN. Design and analysis of computer experiments\nwith branching and nested factors. Technometrics 2009;51:354\u201365.\n[38] Audet C, Hall\u00e9-Hannan E, Le Digabel S. A general mathematical framework\nfor constrained mixed-variable blackbox optimization problems with meta and\ncategorical variables. Oper Res Forum 2023;4:1\u201337.\n[39] Saves P, Nguyen Van E, Bartoli N, Diouane Y, Lefebvre T, David C, Defoort S,\nMorlier J. Bayesian optimization for mixed variables using an adaptive dimension\nreduction process: applications to aircraft design. In: AIAA scitech 2022. 2022.\n[40] Conde Arenzana R, L\u00f3pez-Lopera A, Mouton S, Bartoli N, Lefebvre T. Multi-\nfidelity Gaussian process model for CFD and wind tunnel data fusion. In:\nECCOMAS aerobest. 2021.\n[41] Rufato RC, Diouane Y, Henry J, Ahlfeld R, Morlier J. A mixed-categorical\ndata-driven approach for prediction and optimization of hybrid discontinuous\ncomposites performance. In: AIAA aviation 2022 forum. 2022.\n[42] Gorissen D, Crombecq K, Couckuyt I, Dhaene T, Demeester P. A surrogate\nmodeling and adaptive sampling toolbox for computer based design. J Mach\nLearn Res 2010;11:2051\u20135.\n[43] Williams CK, Rasmussen CE. Gaussian processes for machine learning. MA: MIT\npress Cambridge; 2006.\n[44] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. Efficient Global Opti-\nmization for high-dimensional constrained problems by using the Kriging models\ncombined with the Partial Least Squares method. Eng Optim 2018;50:2038\u201353.\n[45] Bouhlel MA, He S, Martins J. Scalable gradient-enhanced artificial neural\nnetworks for airfoil shape design in the subsonic and transonic regimes. Struct\nMultidiscip Optim 2020;61:1363\u201376.\n[46] Kwan LS, Pitrou A, Seibert S. Numba: A LLVM-based python JIT compiler. In:\nProceedings of the second workshop on the LLVM compiler infrastructure in\nHPC. 2015.\n[47] Zaefferer M, Horn D. A first analysis of kernels for Kriging-based optimization\nin hierarchical search spaces. 2018, arXiv.\n[48] Jin R, Chen W, Sudjianto A. An efficient algorithm for constructing optimal\ndesign of computer experiments. J Statist Plann Inference 2005;2:545\u201354.\n[49] Garnett R, Osborne M, Hennig P. Active learning of linear embeddings for\nGaussian processes. In: Uncertainty in artificial intelligence - Proceedings of the\n30th conference. 2013.\n[50] Jones D. A taxonomy of global optimization methods based on response surfaces.\nJ Global Optim 2001;21:345\u201383.\n[51] Lafage R. egobox, a Rust toolbox for efficient global optimization. J Open Source\nSoftw 2022;7:4737.\n[52] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive\nblack-box functions. J Global Optim 1998;13:455\u201392.\n[53] Deng X, Lin CD, Liu K, Rowe RK. Additive Gaussian process for computer models\nwith qualitative and quantitative factors. Technometrics 2017;59:283\u201392.\n[54] Cuesta-Ramirez J, Le Riche R, Roustant O, Perrin G, Durantin C, Gliere A. A\ncomparison of mixed-variables Bayesian optimization approaches. Adv Model\nSimul Eng Sci 2021;9:1\u201329.\n[55] Rebonato R, Jaeckel P. The most general methodology to create a valid\ncorrelation matrix for risk management and option pricing purposes. J Risk\n2001;2:17\u201327.\n[56] Rapisarda F, Brigo D, Mercurio F. Parameterizing correlations: a geometric\ninterpretation. IMA J Manag Math 2007;18:55\u201373.\n[57] Bouhlel\nMA,\nBartoli\nN,\nRegis\nR,\nOtsmane\nA,\nMorlier\nJ.\nAn\nimproved\napproach for estimating the hyperparameters of the Kriging model for high-\ndimensional problems through the Partial Least Squares method. Math Probl\nEng 2016;2016:6723410.\n[58] Cheng GH, Younis A, Hajikolaei KH, Wang GG. Trust region based mode pursuing\nsampling method for global optimization of high dimensional design problems.\nJ Mech Des 2015;137:021407.\n[59] Karlsson R, Bliek L, Verwer S, de Weerdt M. Continuous surrogate-based\noptimization algorithms are well-suited for expensive discrete problems. In:\nArtificial intelligence and machine learning. 2021.\n[60] Pelamatti J, Brevault L, Balesdent M, Talbi E-G, Guerin Y. Bayesian optimization\nof variable-size design space problems. Opt Eng 2021;22:387\u2013447.\n[61] Hebbal A, Brevault L, Balesdent M, Talbi E-G, Melab N. Bayesian optimization\nusing deep Gaussian processes with applications to aerospace system design. Opt\nEng 2021;22:321\u201361.\n[62] Wildberger N. A rational approach to trigonometry. Math Horiz 2007;15:16\u201320.\n[63] Cho H, Kim Y, Lee E, Choi D, Lee Y, Rhee W. Basic enhancement strategies when\nusing bayesian optimization for hyperparameter tuning of deep neural networks.\nIEEE Access 2020;8:52588\u2013608.\n[64] Zuniga MM, Sinoquet D. Global optimization for mixed categorical-continuous\nvariables based on Gaussian process models with a randomized categorical space\nexploration step. INFOR Inf Syst Oper Res 2020;58:310\u201341.\n[65] Lindauer M, Eggensperger K, Feurer M, AB, Deng D, Benjamins C, et al. SMAC3:\nA versatile Bayesian optimization package for hyperparameter optimization. J\nMach Learn Res 2022;23:1\u20139.\n[66] Picheny V, Berkeley J, Moss H, Stojic H, Granta U, Ober S, et al. Trieste:\nEfficiently exploring the depths of black-box functions with TensorFlow. 2023,\narXiv.\n[67] Cowen-Rivers AI, Ly W, Wang Z, Tutunov R, Jianye H, Wang J, et al. HEBO:\nHeteroscedastic evolutionary Bayesian optimisation. 2020, arXiv.\n[68] Jiang H, Shen Y, Li Y, Zhang W, Zhang C, Cui B. OpenBox: A Python toolkit for\ngeneralized black-box optimization. 2023, arXiv.\nAdvances in Engineering Software 188 (2024) 103571\n16\nP. Saves et al.\n[69] Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et\nal. Tuning hyperparameters without grad students: Scalable and robust bayesian\noptimisation with dragonfly. J Mach Learn Res 2020;21:3098\u2013124.\n[70] Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient\nglobal optimization algorithm with multiple infill strategy - Applied to a wing\ntopology optimization problem. In: AIAA scitech 2019 forum. 2019.\n[71] M\u00fcller J, Shoemaker CA, Pich\u00e9 R. SO-MI: A surrogate model algorithm for\ncomputationally expensive nonlinear mixed-integer black-box global optimization\nproblems. Comput Oper Res 2013;40:1383\u2013400.\n[72] Tran T, Sinoquet D, Da Veiga S, Mongeau M. Derivative-free mixed binary\nnecklace optimization for cyclic-symmetry optimal design problems. Opt Eng\n2021.\n[73] Meliani M, Bartoli N, Lefebvre T, Bouhlel MA, Martins JRRA, Morlier J. Multi-\nfidelity efficient global optimization: Methodology and application to airfoil\nshape design. In: AIAA aviation 2019 forum. 2019.\n[74] Lee H. Gaussian processes. Springer Berlin Heidelberg; 2011, p. 575\u20137.\n[75] L\u00f3pez-Lopera AF, Idier D, Rohmer J, Bachoc F. Multioutput Gaussian processes\nwith functional data: A study on coastal flood hazard assessment. Reliab Eng\nSyst Saf 2022;218:108139.\n[76] Berthelin G, Dubreuil S, Sala\u00fcn M, Bartoli N, Gogu C. Disciplinary proper\northogonal decomposition and interpolation for the resolution of parameterized\nmultidisciplinary analysis. Internat J Numer Methods Engrg 2022;123:3594\u2013626.\n[77] Cardoso I, Dubreuil S, Bartoli N, Gogu C, Sala\u00fcn M, Lafage R. Disciplinary\nsurrogates for gradient-based optimization of multidisciplinary systems. In:\nECCOMAS Aerobest. 2023.\n[78] Platt J, Penny S, Smith T, Chen T, Abarbanel H. A systematic exploration of\nreservoir computing for forecasting complex spatiotemporal dynamics. Neural\nNetw 2022;153:530\u201352.\n[79] Charayron R, Lefebvre T, Bartoli N, Morlier J. Multi-fidelity Bayesian optimiza-\ntion strategy applied to overall drone design. In: AIAA scitech 2023 forum.\n2023.\n[80] Charayron R, Lefebvre T, Bartoli N, Morlier J. Towards a multi-fidelity and\nmulti-objective Bayesian optimization efficient algorithm. Aerosp Sci Technol\n2023;142:108673.\n[81] Wold H. Soft modelling by latent variables: The non-linear iterative partial least\nsquares (NIPALS) approach. J Appl Probab 1975;12:117\u201342.\n[82] Priem R, Diouane Y, Bartoli N, Dubreuil S, Saves P. High-dimensional efficient\nglobal optimization using both random and supervised embeddings. In: AIAA\naviation 2023 forum. 2023.\n[83] Betz W, Papaioannou I, Straub D. Numerical methods for the discretization of\nrandom fields by means of the Karhunen\u2013Lo\u00e8ve expansion. Comput Methods\nAppl Mech Engrg 2014;271:109\u201329.\n[84] Menz M, Dubreuil S, Morio J, Gogu C, Bartoli N, Chiron M. Variance based sen-\nsitivity analysis for Monte Carlo and importance sampling reliability assessment\nwith Gaussian processes. Struct Saf 2021;93:102116.\n[85] Ginsbourger D, Le Riche R, Carraro L. Kriging is well-suited to parallelize\noptimization. Springer Berlin Heidelberg; 2010, p. 131\u201362.\n[86] Roux E, Tillier Y, Kraria S, Bouchard P-O. An efficient parallel global opti-\nmization strategy based on Kriging properties suitable for material parameters\nidentification. Arch Mech Eng 2020;67.\n",
    "pdf_url": "",
    "references": [
      "[1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach",
      "for the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\u201373.",
      "[2] Kennedy M, O\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc",
      "Ser B Stat Methodol 2001;63:425\u201364.",
      "[3] Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets."
    ],
    "publication_date": "07-12-2023"
  },
  {
    "titre": "Improved stochastic subset optimization method for structural design optimization ",
    "resume": "The Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that mini-mizes the probability of system failure over the admissible space for the design parameters. It is based on the simulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and exploiting the information contained in these samples to identify subregions for the optimal design parameters within the original design space. This paper presents an improved version of SSO, named iSSO to overcome the shortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design space into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A double-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a generalized design optimization approach primarily tailored for the stochastic structural systems but also adaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness and efficiency of the proposed iSSO.  ",
    "auteurs": [
      "Aman Khalid",
      "Sahil Bansal",
      "\u03c61\u22ef\u03c6n\u03c6]T",
      "\u2208",
      "M.A. Khalid",
      "S. Bansal"
    ],
    "institutions": [
      "Department of Civil Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India  "
    ],
    "mots_cles": [
      ": Stochastic subset optimization "
    ],
    "texte_integral": "Advances in Engineering Software 188 (2024) 103568\nAvailable online 12 December 2023\n0965-9978/\u00a9 2023 Published by Elsevier Ltd.\nImproved stochastic subset optimization method for structural \ndesign optimization \nMohd Aman Khalid , Sahil Bansal * \nDepartment of Civil Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India   \nA R T I C L E  I N F O   \nKeywords: \nStochastic subset optimization \nVoronoi tessellation \nStochastic simulation \nStochastic optimization \nOptimization under uncertainty \nA B S T R A C T   \nThe Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that mini-\nmizes the probability of system failure over the admissible space for the design parameters. It is based on the \nsimulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and \nexploiting the information contained in these samples to identify subregions for the optimal design parameters \nwithin the original design space. This paper presents an improved version of SSO, named iSSO to overcome the \nshortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design \nspace into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A \ndouble-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a \ngeneralized design optimization approach primarily tailored for the stochastic structural systems but also \nadaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness \nand efficiency of the proposed iSSO.   \n1. Introduction \nStructural optimization may be defined as the rational establishment \nof an economical structural design with the available resources while \nsatisfying specific performance criteria. In general terms, the economy \nmay be characterized by minimum weight, minimum cost, maximum \nutility, or even minimum probability of failure. Broadly, structural \noptimization can be categorized into deterministic and stochastic opti-\nmization [1,2]. The classical statement of unconstraint deterministic \noptimization is mathematically expressed as: \nminimize :\n\u03c6\u2208\u03a6\ng(\u03c6)\n(1)  \nwhere, \u03c6 = [\u03c61\u22ef\u03c6n\u03c6]T \u2208 \u03a6\u2282Rn\u03c6 is a set of deterministic adjustable pa-\nrameters that define the structural design, referred to herein as design \nparameters, g(\u03c6) : Rn\u03c6\u2192R is the objective function to be minimized, and \n\u03a6 denotes the bounded admissible design space. The deterministic \nconstraints can be considered by the appropriate definition of the ad-\nmissible design space \u03a6 for deterministic design parameters \u03c6, as \nmentioned in [3]. In the deterministic structural optimization problem, \nthe uncertainties in parameters are ignored, and fixed values are \nassumed for all the parameters. There are numerous optimizations \napproaches available in the literature, however, but it\u2019s worth noting \nthat no one-size-fits-all optimization approach is ideal for all sorts of \nproblems [4\u20137]. The choice of optimization method is often determined \nby the specific characteristics of the problem, such as its complexity, \ndimensionality, constraints, and the nature of the objective function. As \na result, there is always a scope for new approaches to be developed or \nthe adaptation of existing methods to better suit specific problem clas-\nses. A detailed discussion of deterministic optimization approaches can \nbe found in the literature [8,9]. \nIn any practical situation, several parameters, such as loadings, \nstructural parameters, geometric parameters, operation conditions, etc., \nare either not known at the design stage or are subjected to random \nfluctuations that give rise to performance variability and affect the \nperformance of a system [10]. These parameters are characterized as \nuncertain parameters. Deterministic structural optimization discards the \nimpact of uncertainty and can result in improper design. Therefore, it is \ndesirable to account for the uncertainty in the parameters during opti-\nmization by using the rational methods of probabilistic structural \nanalysis [11]. Such structural optimization that accounts for un-\ncertainties is called stochastic optimization [12]. Although stochastic \noptimization refers to any method that employs randomness within \nsome communities, in this paper, we will only consider settings where \n* Corresponding author. \nE-mail addresses: mohdamankhalid@gmail.com (M.A. Khalid), sahil@iitd.ac.in (S. Bansal).  \nContents lists available at ScienceDirect \nAdvances in Engineering Software \njournal homepage: www.elsevier.com/locate/advengsoft \nhttps://doi.org/10.1016/j.advengsoft.2023.103568 \nReceived 5 June 2023; Received in revised form 2 October 2023; Accepted 24 November 2023   \nAdvances in Engineering Software 188 (2024) 103568\n2\nthe objective function is random. Stochastic optimization or optimal \ndesign under uncertainty has been widely applied in many practical \nengineering fields, including civil engineering structures [13\u201315], \ncomposite structures [16,17], and vehicles [18,19]. \nConsider an engineering system that involves deterministic design \nparameters \u03c6, and uncertain variables \u03b8 = [\u03b81\u22ef\u03b8n\u03b8]T \u2208 \u0398\u2282Rn\u03b8 following \na joint PDF p(\u03b8|\u03c6), where \u0398 denotes the parameter space of the uncer-\ntain variables. The classical statement of stochastic optimization is \nmathematically expressed as: \nminimize :\n\u03c6\u2208\u03a6\nE\u03b8[h(\u03c6, \u03b8)]\n(2)  \nwhere, h(\u03c6, \u03b8) : Rn\u03b8+n\u03c6\u2192R is the structural performance function, and \nE\u03b8[ \u22c5 ] denotes expectation with respect to the PDF for \u03b8. Note that the \nobjective function in the optimization problem in (2) is the expectation \nE\u03b8[h(\u03c6, \u03b8)] which is a deterministic function. It\u2019s worth mentioning that \nstochastic optimization may also involve other stochastic measures such \nas variance or quantile values. However, these stochastic measures can \nrarely be evaluated analytically; therefore, several methods have been \nproposed for solving stochastic optimization problems. These special-\nized methods include, for example, sample average approximation, \nstochastic approximation, stochastic subset optimization, and ap-\nproaches based on the use of Taylor series expansion [15,20,21], \nresponse surface, and metamodels [22\u201325]. Specific to structural engi-\nneering, there are two broad categories of problems involving design \noptimization under uncertainty [26\u201335]: Reliability-Based Design \nOptimization (RBDO) and Robust Design Optimization (RDO). The \nobjective of RBDO is to find an optimal solution that minimizes some \ndeterministic, objective function under observance of probabilistic \nconstraints instead of conventional deterministic constraints [36,37]. \nOn the other hand, RDO aims to find an optimal solution that is insen-\nsitive (or less sensitive) to input variations. It improves the design \nquality by minimizing performance variation without eliminating un-\ncertainty [29,38]. \nTaflanidis and Beck [39] introduced a novel algorithm for optimal \nreliability problem, the so-called SSO. SSO involves formulating an \naugmented problem where the design parameters are artificially \nconsidered uncertain and defining an auxiliary PDF that includes the \nstructural performance function and the PDF of the uncertain variables. \nNext, SSO involves generating a pool of samples distributed according to \nthis auxiliary PDF and identifying a subregion in the original design \nspace, which, on average, improves the value of the objective function. \nBy repeating this procedure several times, it is possible to determine at \neach step a smaller subregion in the design space, which in turn im-\nproves the value of the objective function. Ultimately, this subregion \nwill be sufficiently small to directly identify the optimal solution or \nprovide sufficient information to launch another optimization algo-\nrithm, such as the sample average approximation or stochastic approx-\nimation. The implementation of the SSO method closely resembles the \nSubset Simulation (SS) algorithm [40] for reliability analysis. Since SSO \nis based on simulation, it can deal with linear or nonlinear problems and, \nat least theoretically, an unbounded number of design parameters. The \nnumerical effort for solving a given optimization problem is indepen-\ndent of the number of uncertain variables, and it grows linearly with the \nnumber of design parameters. \nSince the introduction of SSO, several extensions of SSO have been \nproposed. An extension of SSO termed Non-Parametric SSO, which \nadopts kernel density estimation to approximate the objective function, \nis presented in [41]. In [42], efficient integration of the Moving Least \nSquares approximation within SSO is introduced to reduce the compu-\ntational effort in SSO. In [3], an augmented formulation is presented for \nthe RDO of structures using SSO. SSO or its variants have also been \napplied to solve structural optimization problems. SSO has been used for \nreliability optimization and sensitivity analysis in system design in [39]. \nA framework for RDO of Tuned Mass Dampers (TMD) by SSO is dis-\ncussed in [43]. Even though SSO has proved to be efficient for meeting \nvarious challenging optimization problems, it has two shortcomings. \nFirst, the effectiveness of SSO is dependent on the correct selection of \nthe geometrical shape of the admissible subsets. Here, it is pertinent to \nmention that choosing a geometrical shape that effectively investigates \nthe sensitivity of the objective function to each design variable is \nessential. The shapes, such as hyper-rectangle and hyper-ellipse are \nsuggested in the literature for the admissible subsets. However, as shown \nlater via the illustrative example, these shapes fail to include the optimal \nsolution in cases with complex design spaces or problems with multiple \noptimal solutions. And second, identifying the optimal subset that con-\ntains the smallest volume density involves a non-smooth optimization \nproblem which is quite challenging. \nIn this paper, an improved version of SSO is developed to overcome \nthe shortcomings of the original SSO. This new version of the algorithm, \nas mentioned earlier, is named iSSO (improved SSO). Voronoi tessella-\ntion is implemented to partition the design space into non-overlapping \nsubregions (a set of Voronoi cells) using the pool of samples distrib-\nuted according to the auxiliary PDF. The admissible set (a set of all \nadmissible subregions) is then defined as a set containing all subsets of \nthe set of Voronoi cells. This approach is able to capture the regions with \nlower objective function values even if they are disjointed or when the \ndesign space is complex. The details of the Voronoi tessellation are \npresented in Appendix A. A double-sort algorithm is then implemented \nto identify the optimal subset containing the smallest volume density. \nIn the next section, the original SSO is reviewed. Section 3 presents \nthe general theoretical and computational framework for the iSSO al-\ngorithm. Section 4 considers several optimization problems to illustrate \nthe effectiveness and efficiency of the proposed iSSO algorithm. \n2. Original stochastic subset optimization \nIn SSO, say at the i + 1th iteration, the design space is represented by \na subset I(i), where I(i) \u2208 I(i \u2212 1)\u22c5\u22c5\u22c5 \u2208 I(0) \u2208 \u03a6. Following the augmented \nformulation concept initially discussed in [44] for RBDO, the design \nparameters \u03c6, are artificially considered uncertain variables with a \nprescribed PDF p(\u03c6|I(i)) over the design space I(i) [45]. For convenience, \np(\u03c6|I(i)) = 1/V(i) is considered, where V(i) is the volume of I(i). In this \nsetting of the augmented stochastic design problem, the auxiliary PDF is \ndefined as: \n\u03c0\n(\n\u03c6, \u03b8\n\u20d2\u20d2I(i))\n= h(\u03c6, \u03b8)p\n(\n\u03c6, \u03b8\n\u20d2\u20d2I(i))\nE\u03c6,\u03b8[hs(\u03c6, \u03b8)]\n\u221dh(\u03c6, \u03b8)p\n(\n\u03c6, \u03b8\n\u20d2\u20d2I(i))\n(3)  \nwhere, p(\u03c6, \u03b8|I(i)) = p(\u03b8|\u03c6)p(\u03c6|I(i)). Note that if h(\u03c6, \u03b8)\u2264 0, it must be \nsuitably transformed to ensure that \u03c0(\u03c6, \u03b8|I(i)) \u2265 0. One way to do this is \nto define hs(\u03c6,\u03b8) = h(\u03c6, \u03b8) \u2212 s, since E\u03b8[hs(\u03c6,\u03b8)] = E\u03b8[h(\u03c6,\u03b8)] \u2212 s, that is, \nthe two expected values differ only by a constant, and the optimization \nof the expected value of h( \u22c5 ) is equivalent, in terms of the optimal design \nchoice, to optimization for the expected value for hs( \u22c5 ). In the above \nequation, the denominator is a normalizing constant given by: \nE\u03c6,\u03b8[h(\u03c6, \u03b8)] =\n\u222b\n\u03a6\n\u222b\n\u0398\nh(\u03c6, \u03b8)p\n(\n\u03c6, \u03b8\n\u20d2\u20d2I(i))\nd\u03b8d\u03c6.\n(4) \nAlthough this expected value is not explicitly needed, it can be \ndetermined using any state-of-the-art stochastic simulation method. The \nobjective function E\u03b8[hs(\u03c6, \u03b8)] in this context of the auxiliary PDF is \nexpressed as: \nE\u03b8[h(\u03c6, \u03b8)] = \u03c0\n(\n\u03c6\n\u20d2\u20d2I(i))\np\n(\n\u03c6\n\u20d2\u20d2I(i))E\u03c6,\u03b8[h(\u03c6, \u03b8)],\n(5)  \nwhere, the marginal \u03c0(\u03c6|I(i)) is given by: \n\u03c0\n(\n\u03c6\n\u20d2\u20d2I(i))\n=\n\u222b\nI(i)\u03c0(\u03c6, \u03b8)d\u03b8.\n(6) \nIn (5), since E\u03c6,\u03b8[h(\u03c6, \u03b8)] is a normalizing constant, minimization of \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n3\nE\u03b8[h(\u03c6, \u03b8)] is equivalent to minimization of J(\u03c6), which is equal to: \nJ\n(\n\u03c6\n\u20d2\u20d2I(i))\n= E\u03b8[hs(\u03c6, \u03b8)]\nE\u03c6,\u03b8[hs(\u03c6, \u03b8)] = \u03c0\n(\n\u03c6\n\u20d2\u20d2I(i))\np\n(\n\u03c6\n\u20d2\u20d2I(i)).\n(7) \nThe estimation of the marginal \u03c0(\u03c6|I(i)) in (7) is necessary to mini-\nmize J(\u03c6|I(i)). Analytical approximations of \u03c0(\u03c6|I(i)) based on kernel \ndensity approaches or the maximum entropy method might be arduous \nin case of complex problems, such as when design parameters n\u03c6 are \nlarge, or the sensitivity for some design parameters is complex [44]. In \nthe SSO framework, such approximation of \u03c0(\u03c6|I(i)) is avoided. In SSO, \nsamples distributed as \u03c0(\u03c6|I(i)) are obtained, and the information in \nthese samples is exploited to identify a smaller subset of the design space \nwith a high likelihood of containing the optimal design parameters. \nSamples distributed as \u03c0(\u03c6, \u03b8|I(i)) are obtained using any appropriate \nstochastic sampling algorithm, such as Markov Chain Monte Carlo \n(MCMC) sampling [46]. The \u03c6 component of these samples then cor-\nresponds to samples from the marginal distribution \u03c0(\u03c6|I(i)). \nThe sensitivity of objective function E\u03b8[hs(\u03c6, \u03b8)] to \u03c6 is determined by \nevaluating the average value (or equivalently volume density) of J(\u03c6| \nI(i)) over any subset I in I(i), which is denoted by H(I) and defined as: \nH(I) = 1\nVI\n\u222b\nI\nJ\n(\n\u03c6|I(i))\nd\u03c6 = 1\nVI\n\u222b\nI\n\u03c0\n(\n\u03c6|I(i))\np\n(\n\u03c6|I(i)) d\u03c6 = VI(i)\nVI\n\u222b\nI\n\u03c0\n(\n\u03c6|I(i))\nd\u03c6\n(8)  \nwhere, VI is the volume of subset I. Based on the samples distributed \naccording to \u03c0(\u03c6|I(i)) belonging to I(i), an estimate of H(I) is provided by: \nH(I) =\nNI/VI\nNI(i)/VI(i),\n(9)  \nwhere, NI(i) is the number of samples distributed as \u03c0(\u03c6|I(i)) belonging to \nI(i), and NI denotes the number of samples from \u03c0(\u03c6|I(i)) belonging to the \nI (NI < NI(i\u22121)since I\u2282I(k \u2212 1)). Say NI = p0NI(i\u22121). A smaller value of \u03c1 re-\nsults in a faster decrease in the size of the identified subsets but with \npoorer accuracy. The use of \u03c1 equal to 0.1 - 0.2 is suggested in the \nliterature [39]. \nA deterministic optimization, based on the estimate H(I) of H(I), is \nnext performed to identify the subset I \u2208 A(i+1)\n\u03c1\n, where A(i+1)\n\u03c1 \nis a set of \nadmissible subsets in I(i), that contains the smallest volume density NI/ \nVI, that is, \nI(i+1) = argmin\nI\u2208A\u03c1H(I) = arg min\nI\u2208A(i+1)\n\u03c1\nNI\n/\nVI\nA(i+1)\n\u03c1\n=\n{\nI\u2282I(i) : \u03c1 = NI\n/\nN(i)}\n.\n(10) \nThe effectiveness of SSO is dependent on the correct selection of the \ngeometrical shape and size of the admissible subsets. Choosing a \ngeometrical shape that effectively investigates the sensitivity of the \nobjective function to each design variable is essential. The optimization \nin (10) determines the subset with the smallest average value of J(\u03c6|I(i)) \n(or equivalently E\u03b8[hs(\u03c6,\u03b8)]) within the admissible set A(i+1)\n\u03c1\n. I(i + 1) is a \nsubset of the design space I(i) with a high likelihood of containing the \noptimal design parameters. The above steps are repeated until the \nstopping criterion is met. This way, SSO adaptively converges to a \nrelatively small subregion within the original design space. The imple-\nmentation of SSO is demonstrated in Fig. 1. The reader may refer to the \noriginal publication for a detailed explanation of SSO [39]. \nH(I(i)) expresses the average relative sensitivity of E\u03b8[h(\u03c6,\u03b8)] to \u03c6. A \nlow value of H(I(i)) indicates that E\u03b8[h(\u03c6,\u03b8)] is more sensitive to \u03c6, and \nvice versa. A high value of H(I(i)), close to 1 corresponds to a sample \ndensity in design space I(i) that approximates a uniform distribution and \nsuggests that the identified subset I(i) has a low likelihood of containing \n\u03c6* [39]. Therefore, the SSO is stopped when H(I(i)) exceeds a threshold \nvalue. A threshold value of 0.75\u20130.80 has been found to give satisfactory \nresults [39]. \n3. Proposed approach \nIn the proposed approach, the Voronoi tessellation is implemented to \npartition the design space into non-overlapping subregions (a set of \nVoronoi cells) using the pool of samples distributed according to this \nauxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a \nspace into convex polygons, called Voronoi cells, such that each cell \ncontains exactly one sample, called a cell-generating sample. Every \nsample in a given polygon is closer to its generating sample compared to \nany other. In the proposed approach, the admissible set (a set of all \nadmissible subspaces) is defined as a set containing all subsets of the set \nof Voronoi cells. An alternative approach to identify the optimal subset \nwithout performing any non-smooth deterministic optimization is also \npresented. The general theoretical and computational framework for the \niSSO algorithm is presented in the following subsections, and the \nFig. 1. Illustration of the original SSO algorithm.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n4\nalgorithm is demonstrated in Fig. 2. \n3.1. Partitioning of design space \nIn the proposed approach, at the i + 1th iteration, say N(i) is the \nnumber of samples distributed as \u03c0(\u03c6|I(i)) belonging to the design space \nI(i). Let nv = N(i) /(1 + \u03b3), \u03b3 \u2265 0 be the number of unique samples. If \nsampling techniques such as accept rejection, importance sampling, etc., \nare used, then \u03b3 = 0, and each sample in the design space will be unique. \nHowever, if MCMC sampling techniques are used, the resulting samples \nwill be correlated, that is \u03b3 > 0, and we will have repeated samples. \nAssume that the design space I(i) is divided into v(i)\nk , k = 1\u22c5\u22c5\u22c5nv, Voronoi \ncells using nv unique samples, and say the Voronoi cell v(i)\nk contains \u03b7(i)\nk \nrepeated samples, then, an estimate of \u03c0(\u03c6|I(i)) is provided by: \n\u03c0\n(\n\u03c6|I(i))\n=\n\u03b7(i)\nk\nN(i)V(i)\nk\n\u2265 0,\n\u2200\n\u03c6 \u2208 v(i)\nk ,\n(11)  \nwhere, V(i)\nk is the volume of the kth Voronoi cell. Obviously, \n\u222b\nI(i)\u03c0(\u03c6|I(i))\nd\u03c6 = 1. \nSimilar to the original SSO, the sensitivity of the objective function J \n(\u03c6|I(i)) to \u03c6 is determined by evaluating the average value of J(\u03c6|I(i)) \nover any subspace I of the design space I(i). Subset I is any subset of \nnvVoronoi cells (these cells may be disjointed). Since the design space is \npartitioned into nv subspaces or Voronoi cells, the number of admissible \nsubsets (proper subsets) is given by 2n\u03bd \u2212 1. Based on the estimate \n\u03c0(\u03c6|I(i)) provided in (11), an estimate of H(I) is provided as: \nH(I) = V(i)\nVI\n\u222b\nI\n\u03c0\n(\n\u03c6|I(i))\nd\u03c6 = V(i)\nVI\n\u2211\nI\u03b7(i)\ni\nN(i) = V(i)\nVI\nNI\nN(i)\n(12)  \nwhere, VI is the volume of the subset I and NI is the number of samples \nbelonging to it. Let I = {v(i)\n(1), v(i)\n(2)\u22efv(i)\n(S)}, where S is the number of Vor-\nonoi cells defining the subset I. Note that the parentheses are used in the \nsubscript to differentiate between the Voronoi cell number defined in \nthe previous section from the Voronoi cell index describing the subset I. \nAn estimate of H(I) is then provided as: \nH(I) = V(i)\nN(i)\n[\n\u03b7(i)\n(1) + \u03b7(i)\n(2) + \u22ef + \u03b7(i)\n(S)\nV(i)\n(1) + V(i)\n(2) + \u22ef + V(i)\n(S)\n]\n.\n(13)  \n3.2. Identification of an optimal subset \nA deterministic optimization needs to be performed to identify a \nsubset I that contains the smallest volume density NI/VI. In the case of \nunique samples, since \u03b7(i)\n(\u22c5) = 1, the solution to the minimization problem \nin (10) is a set of \u03c1N(i) Voronoi cells with the largest volume. For the case \nwith repeated samples, the optimization can be performed using \nmethods appropriate for non-smooth optimization problems, such as \nsub-gradient methods, bundle methods, gradient sampling methods, etc. \nIn this study, we propose an alternative approach to identify the \noptimal subset without performing any non-smooth deterministic opti-\nmization. A double-sort algorithm is proposed, which involves sorting \nthe Voronoi cells in ascending order of the sample counts and then in \ngroups of cells with the same sample count in descending order of cell \nvolume. Finally, the top cells containing \u03c1N(i)samples are selected as an \napproximate optimal solution from the sorted list. \nOne may argue that the optimal subset can be obtained by first \nsorting the Voronoi cells in ascending order of the cell density, defined \nas \u03b7(i)\nk /V(i)\nk , and then by selecting the top cells containing \u03c1N(i) samples \nfrom the sorted list. However, this argument is erroneous because the \nobjective is to minimize \u2211S\ns=1\u03b7(i)\n(s)/ \u2211S\ns=1V(i)\n(s) and not \u2211S\ns=1(\u03b7(i)\n(s) /V(i)\n(s)). The \neffectiveness of the proposed double-sort algorithm is demonstrated in \nSection 4 with the help of examples. \n3.3. Simulation of conditional samples \nAt the i + 1th iteration, \u03c1N(i) samples distributed as \u03c0(\u03c6|I(i + 1)) are \navailable from the previous iteration. Using these samples as seeds, \nadditional (1 \u2212 \u03c1)N(i + 1) are simulated. The proposed method to \nsimulate additional samples involves two steps: (a) randomly selecting a \nVoronoi cell within the subset I(i + 1) based on the estimate \u03c0(\u03c6|I(i)) and \n(b) applying the Metropolis-Hastings algorithm within the selected \nVoronoi cell. \nA Voronoi cell is selected according to the following weights in the \nfirst step: \nw(i)\nk =\n\u03b7(i)\nk\n/\nV(i)\nk\n\u2211\nk\n\u03b7(i)\nk\n/\nV(i)\nk\n.\n(14) \nFig. 2. Illustration of the proposed iSSO algorithm.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n5\nTo simulate a new sample within a selected Voronoi cell, the sample \nthat generated the selected Voronoi cell or the last simulated sample in \nthe selected Voronoi cell is used as the seed sample, and the Metropolis- \nHastings algorithm is implemented. A candidate sample [\u03c6c,\u03b8c] is \nsimulated using the proposal q(\u03c6c,\u03b8c|\u03c6,\u03b8) and is accepted with the \nprobability min(1, a0), where, a0 is given as: \na0 = h(\u03c6c, \u03b8c)p(\u03c6c, \u03b8c)q(\u03c6, \u03b8|\u03c6c, \u03b8c)\nh(\u03c6, \u03b8)p(\u03c6, \u03b8)q(\u03c6c, \u03b8c|\u03c6, \u03b8) .\n(15) \nIn the present study, the proposed PDF is equal to the uniform PDF \nfor design parameters and the initial PDF for uncertain variables, i.e., q \n(\u03c6, \u03b8|\u03c6c,\u03b8c) = p(\u03c6, \u03b8). Therefore, on simplifying (15), a0 is given as: \na0 = h(\u03c6c, \u03b8c)\nh(\u03c6, \u03b8) .\n(16)  \n3.4. Stopping criteria \nA new stopping criterion is proposed in this study. The convergence \nof the expected value of the performance measure h(\u03c6, \u03b8) with respect to \nthe PDF for \u03c6 and \u03b8 in consecutive iterations is used as the stopping \ncriterion. Mathematically the proposed stopping criterion is represented \nby: \n\u20d2\u20d2E\u03c6,\u03b8[h(\u03c6, \u03b8)]i \u2212 E\u03c6,\u03b8[h(\u03c6, \u03b8)]i\u22121\u20d2\u20d2 \u2264 \u03b5\n(17)  \nwhere, \u03b5 is a user-specified tolerance limit. Other stopping criteria, as \nindicated in [39,47], can also be chosen. \n3.5. Implementation issues \nAn important issue for the effective implementation of the iSSO is the \ncreation of the Voronoi cells at the current iteration bounded within the \nVoronoi cell created at the previous iterations. Although it is possible to \ncreate such bounded Voronoi cells, due to the geometrical complexities, \nit is usually unfeasible for the higher dimensional problems (n\u03c6>2). An \nalternative approach is proposed in the present study for creating the \nVoronoi cells at any iteration of the iSSO. The proposed approach in-\nvolves creating Voronoi cells using the samples generated at the current \nand all previous iterations and then by considering Voronoi cells cor-\nresponding to the samples from the current iteration. This is shown in \nFig. 3, where Fig. 3(a) shows the N samples at the first iteration and the \ncorresponding Voronoi cells. Fig. 3(b) shows the \u03c1N selected Voronoi \ncells leading to the smallest volume density and the additional (1 \u2212 \u03c1)N \nsamples being generated using these \u03c1N samples as seeds. Fig. 3(c) \nshows that the Voronoi cells are generated using all N + (1 \u2212 \u03c1)N \nsamples that are generated in the two iterations. The Voronoi cells \ncorresponding to the N samples for consideration at the second iteration \nare also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of \nFig. 3(c) where it can be observed that the area covered by the N Voronoi \ncells considered in the second iteration is not the same as the area \ncovered by the \u03c1N Voronoi cells selected in the first iteration. On the \ncontrary, the area covered by the Voronoi cells in the second iteration is \nmore than the area covered by the Voronoi cells corresponding to the \nseed samples from the first iteration. This is because a new sample \nwithin the Voronoi cell between an existing sample and the existing \nVoronoi cell edge results in the relocation of the Voronoi cell edge in a \nFig. 3. Implementation of Voronoi tessellation in iSSO.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n6\ndirection away from the new sample. The increase at each iteration in-\ntroduces a bias in the estimate of \u03c0(\u03c6|I(i)) in (11). However, this does not \naffect the performance of the proposed approach as the objective is not \nto simulate the samples distributed as \u03c0(\u03c6|I(i)) but to identify the subsets \nfor an optimal solution. In addition, the increase is not substantial, as \nseen later in the illustrative examples in Section 4. \n3.6. Special case: deterministic optimization \nIn the iSSO framework, a deterministic optimization problems can \nalso be handled with the vector of uncertain variables \u03b8 set equal to a \nnull vector (n\u03b8 = 0). Since the determination of the subset at each iSSO \niteration is solely dependent on the samples distributed as \u03c0(\u03c6), no \nmodification to the iSSO algorithm is required to solve a deterministic \noptimization problem, and the entire formulation remains valid. \n4. Illustrative examples \nIn this section, typical optimization problems are considered to \ndemonstrate the effectiveness and efficiency of the proposed approach. \nFirst, deterministic optimization problems are considered. These prob-\nlems include several local and global minima. Next, stochastic optimi-\nzation problems are illustrated. The second example presents an RDO \nproblem of the TMD. In this example, the variance minimization of the \nprotected structure\u2019s displacement (TMD attached to the structure) is \nperformed. In the third example, the mean minimization of 120 bars \ntruss problems is explored to demonstrate the applicability of the pro-\nposed approach to a high-dimensional stochastic design problem. \nFinally, the fourth example investigates the reliability-based optimiza-\ntion of a base isolation system for a 10-story building. \nIn this study, after implementing iSSO, the optimal design solution is \nidentified as follows. Let \u03b8j, j = 1\u22c5\u22c5\u22c5n be a set of independent, identically \ndistributed realizations of \u03b8, and let h(\u03c6, \u03b8j) be the structural perfor-\nmance function realization for \u03b8j. The expected structural performance \nfunction is approximated by the average of the realizations as: \nE\u03b8[h(\u03c6, \u03b8)] \u2248 1\nn\n\u2211\nn\nj=1\nh\n(\n\u03c6, \u03b8j\n)\n.\n(18) \nE\u03b8[h(\u03c6, \u03b8)] is evaluated for all unique \u03c6 samples obtained at the last \niteration of the iSSO, and the \u03c6 sample resulting in the smallest value of \nE\u03b8[h(\u03c6, \u03b8)] is taken as the optimal solution. Alternatively, as the right- \nhand side of (18) is deterministic, any deterministic optimization \nmethod can also be used to solve the optimization problem with the \napproximate expectation. \nIn the following examples, both iSSO and SSO are implemented with \nN = 1000n\u03c6, \u03c1 = 0.20 and the stopping criteria as stated in (17). Here, a \nvalue of \u03b5 = 10\u22123 is adopted. \nFig. 4. Results for the Griewank function.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n7\n4.1. Multimodal deterministic optimization problems \nIn this section, three two-dimensional benchmark deterministic \noptimization problems are considered. Results are also compared with \nthe SSO. The test functions are:  \na) Griewank function: \nminh(\u03c6) =\n\u2211\nd\ni=1\n\u03c62\ni\n4000 \u2212\n\u220f\nd\ni=1\ncos\n(\u03c6i\u0305\u0305\ni\n\u221a\n)\n+ 1,\ns.t.\u03c6 = [ \u2212 10, 10]\n(19)    \nb) Cross-in-Tray function: \nminh(\u03c6) = \u22120.0001\n(\u20d2\u20d2\u20d2\u20d2\u20d2sin(\u03c61)sin(\u03c62)exp\n(\u20d2\u20d2\u20d2\u20d2\u20d2100 \u2212\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n\u03c62\n1 + \u03c62\n2\n\u221a\n\u03c0\n\u20d2\u20d2\u20d2\u20d2\u20d2\n)\u20d2\u20d2\u20d2\u20d2\u20d2 + 1\n)0.1\n,\ns.t.\u03c6 = [ \u2212 10, 10]\n(20)    \nc) Holder Table function: \nminh(\u03c6) = \u2212\n\u20d2\u20d2\u20d2\u20d2\u20d2sin(\u03c61)cos(\u03c62)exp\n(\u20d2\u20d2\u20d2\u20d2\u20d21 \u2212\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n\u03c62\n1 + \u03c62\n2\n\u221a\n\u03c0\n\u20d2\u20d2\u20d2\u20d2\u20d2\n)\u20d2\u20d2\u20d2\u20d2\u20d2,\ns.t.\u03c6 = [ \u2212 10, 10]\n(21))   \nThe results for the Griewank function are presented in Fig. 4. Fig. 4(a, \nb) shows that the function has multiple closely spaced local minima with \na single global minimum. Fig. 4(c, d) shows the SSO optimization using \nhyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is \nseen that these shapes fail to capture the region containing the optimal \ndesign due to the presence of multiple local minima. Next, the iSSO is \nimplemented, where the Voronoi cells selected at the first and last \niteration are shown in Fig. 4(e, f). It is observed that at the first iteration, \nthe selected Voronoi cells effectively capture both the local and global \nminima and in the subsequent iterations, the selected cells are more \nconcentrated near the global minimum. The region selected at the last \niteration captures the optimal global solution. \nThe Cross-in-Tray function has a relatively complex design space \ncompared to the Griewank function. Fig. 5(a, b) shows multiple local \nand global minima. Minimization by using SSO is demonstrated in Fig. 5 \n(c, d). It is found that both the hyper-rectangle and hyper-ellipse are \ntrapped around any one of the global minima. At the same time, the iSSO \nis able to capture the regions that include all of the global minima, as \nFig. 5. Results for the Cross-in-Tray function.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n8\nseen in Fig. 5(e, f). \nThe Holder Table function has multiple local and global minima; the \nglobal minima are placed at the boundary of the design space, as shown \nin Fig. 6(a, b). Once again, it is seen that both the hyper-rectangle and \nhyper-ellipse are trapped around any of one of the global minima, and \non the other hand, the iSSO is able to capture the regions that include all \nof the global minima, as seen in Fig. 6(e, f). \nThe results from the three examples demonstrate that the proposed \niSSO is able to capture the regions containing the optimal solution \neffectively. \nNext, the statistics of the results of 50 independent runs, both for SSO \nand iSSO are presented in Table 1. It also includes the results obtained by \nusing state-of-the-art approaches, such as the Genetic algorithm, particle \nswarm optimization, and the gradient based optimization approach \n(interior-point algorithm). The proposed iSSO outperforms all other \napproaches as more successes in determining the optimal solution are \nobserved in all three optimization problems. It is also seen that both SSO \nand iSSO result in a similar value of volume reduction for the same \nstopping criterion; however, with SSO, the number of iterations required \nto achieve this volume reduction are relatively higher. The proposed \napproach outperformed the state-of-the-art approaches, as indicated by \nthe number of successes. These examples demonstrate that the main \nadvantage of implementing Voronoi tessellation is an effective explo-\nration of the design space. \nNext, the performance of the proposed \"double sort algorithm\" for \nselecting the optimal subset is studied by using the above-mentioned \nthree functions. Fig. 7 shows the value of H(I(1)) for the 50 indepen-\ndent simulation runs, which is estimated by implementing the proposed \ndouble sort algorithm and by using the Genetic algorithm. It can be \nnoted that for each run, the H(I(1)) values obtained using the proposed \ndouble sort algorithm and Genetic algorithm are well matched, thereby \nconfirming the adequacy of the proposed double sort algorithm. \nAt any iteration of iSSO, new samples are simulated using the seed \nsamples. In the proposed approach, the volume of the Voronoi cells \ncorresponding to the seed and new samples is greater than the volume of \nthe Voronoi cells corresponding only to the seed samples. Fig. 8 shows \nthis change in volume V(seeds+new)\u2212V(seeds)\nV(seeds)\ndue to the creation of Voronoi cells \nat any generation of iSSO using the procedure mentioned in Section 3.4. \nThe increase is observed to be small which further reduces with an in-\ncrease in the iteration number. It is also observed that the increase in \nvolume decreases with an increase in sample size at each iteration and \nincreases with an increase in the dimension of the problem. \n4.2. Robust design optimization of the tuned mass damper \nThis example considers a stochastic design problem involving a \nTuned Mass Damper (TMD) attached to a Single Degree of Freedom \n(SDOF) system. The problem is taken from [48] and is shown in Fig. 9. \nIn this problem, the system is excited by a white noise signal with a \nmean zero and unit variance. The performance measure is the variance \nof the displacement of the system \u03c32\nxs. The mass mS, stiffness kS, and \nFig. 6. Results for the Holder Table function.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n9\ndamping cS of the system are taken as uncertain parameters, following \nindependent Gaussian distribution. The mean value of these variables is \ntaken to be 105 kg, 107 N/m, and 4 \u00d7 104 Ns/m respectively. To account \nfor uncertainty, the c.o.v value for each variable taken is 0.05. The \nfrequency ratio \u03b2 = \u03c9T /\u03c9S and damping \u03beT of the TMD are considered \ndesign parameters. The TMD has a mass ratio, mT/ms, of 0.10. The \nparameters mT,\u03c9T,and\u03c9S are, in order, the mass of the TMD, the natural \nfrequency of the TMD (\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nkT/mT\n\u221a\n), and the natural frequency of the \nstructure (\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nks/ms\n\u221a\n). The optimization problem is written as: \nminimize :\n\u03c6\u2208\u03a6,\u03c6\u2208\u03a6\nE\u03b8[h(\u03b8, \u03c6, \u03c6)] = E\u03b8\n[(\n\u03c32\nxs(\u03b8, \u03c6) \u2212 \u03c6\n)]2\n,\n(22) \nTable 1 \nStatistics of optimization results for multimodal deterministic optimization problems.  \nExample \nSSO \niSSO \nGA* \nPSO* \nGBA*   \nHyper-Rectangle \nHyper-Ellipse     \nGriewank \nNF \n30 \n32 \n5 \n38 \n35 \n47  \nNS \n20 \n18 \n45 \n12 \n15 \n3  \nBV \n0 \n0 \n0 \n0 \n0 \n0  \nWV \n0.1028 \n0.1161 \n0.0270 \n0.0296 \n0.0232 \n0.0296  \nAV \n0.0190 \n0.0292 \n0.0110 \n0.0094 \n0.0057 \n0.0173  \nc.o.v \n1.1663 \n1.0168 \n0.5137 \n0.9603 \n0.8763 \n0.05971  \nFE \n22,702 \n15,223 \n12,426 \n3385 \n1432 \n33  \nGen \n7 \n5 \n4 \nN/A  \nVR \n94.025 \n84.79 \n93.56    \nCross-In-Tray \nNF \n50 \n50 \n1 \n50 \n50 \n50  \nNS \n0 \n0 \n49 \n0 \n0 \n0  \nBV \n\u22122.0576 \n\u22122.0626 \n\u22122.0624 \n\u22122.0626 \n\u22122.0626 \n\u22122.0626  \nWV \n\u22122.0472 \n\u22122.0481 \n\u22122.0260 \n\u22122.0626 \n\u22122.0626 \n\u22121.3853  \nAV \n\u22122.0527 \n\u22122.0621 \n\u22122.0522 \n\u22122.0626 \n\u22122.0626 \n\u22121.7360  \nc.o.v \n0.0133 \n0.001 \n0.0042 \n0 \n0 \n0.0977  \nFE \n27,563 \n21,595 \n9982 \n3178 \n933 \n32  \nGen \n9 \n7 \n3 \nN/A  \nVR \n99.89 \n98.95 \n88.71    \nHolder-Table \nNF \n50 \n50 \n3 \n50 \n50 \n50  \nNS \n0 \n0 \n47 \n0 \n0 \n0  \nBV \n\u221219.2085 \n\u221217.5025 \n\u221219.2085 \n\u221219.2085 \n\u221219.2085 \n\u221219.2085  \nWV \n\u221218.8916 \n\u22121.1419 \n\u221217.3030 \n\u22129.5047 \n\u221215.1402 \n\u22121.1831  \nAV \n\u221219.0916 \n\u22128.432 \n\u221218.8798 \n\u221219.0144 \n\u221218.9745 \n\u22126.5493  \nc.o.v \n0.0025 \n0.3898 \n0.0182 \n0.0722 \n0.0443 \n0.8358  \nFE \n40,700 \n24,684 \n18,142 \n3413 \n988 \n30  \nGen \n13 \n8 \n6 \nN/A  \nVR \n99.59 \n99.42 \n94.37    \nGA = genetic algorithm, PSO = particle swarm optimization, GBA = gradient-based optimization approach, NF = no. of. failure, NS = no. of. success, BV = best value, \nWV = worst value, AV = average value, c.o.v = coefficient of variation, FE = no. of. function evaluations, Gen = generations, VR = volume reduction percentage, * =\nefficiently applicable only for deterministic problems. \nFig. 7. Comparison of double sort algorithm and Genetic algorithm results.  \nFig. 8. Percentage change in volume at each iSSO iteration.  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n10\nwhere, \n0.01 \u2264 \u03b2 \u2264 1.5, 0.01 \u2264 \u03beT \u2264 1.0, 0 \u2264 \u03c6 \u2264 1000.\n(23) \nTable 2 presents the optimal design parameter values as well as the \nobjective function value that solve the optimization problem in (22). \nResults obtained using SSO, Sample Average Approximation (SAA), and \niSSO are shown. SAA is applied with a sample size of 103, as mentioned \nin [43]. The results demonstrate that iSSO is effective in locating the \noptimal solution. SSO implemented with hyper-ellipse gives an optimal \nsolution but has a higher computational cost. \n4.3. 120-bars truss structure \nThe third example involves minimizing the mean of the compliance \nof a 120-bar linear elastic truss structure shown in Fig. 10 under the \nweight constraint W \u2264 15, 000kg. Because of structural symmetry, \ndesign parameters corresponding to the cross-sectional areas of elements \nare divided into seven groups, each with a minimum area of 10\u22124 m2. \nThe Young\u2019s modulus for the bar groups are assumed as uncorrelated \nnormal random variables with mean values equal to 210 GPa and the c. \no.v equal to 0.10 respectively. The density of the material is 7971.89 kg/ \nm3. The dome is subjected to concentrated vertical loads acting down-\nward at the top node, normally distributed with a mean equal to 60 kN \nand c.o.v equal to 0.20. In addition, the mass of bars is concentrated at \nthe nodes. The problem is taken from [48]. \nTable 3 presents the best of 10 independent run results obtained with \nSSO and iSSO. Once again, the SSO and iSSO solutions agree well, \nthereby demonstrating the effectiveness of the proposed approach. At \nthe same time, the number of function evaluations is substantially less in \nthe case of iSSO, indicating the efficiency of the proposed approach. \n4.4. Reliability-based design of a base isolated structure \nThis example, adapted from [49], involves the reliability-based \nFig. 9. TMD attached to a SDOF system [48].  \nTable 2 \nVariance minimization of TMD-structure.  \nMethod \nAdmissible Subset shape \nDesign parameters \nE\u03b8[h(\u03b8,\u03c6,\u03c6)]\n(\u00d7 10\u221216 mm4) \nFE \nNS \nNF \n\u03b2 \n\u03beT \nSSO [48] \nHyper-Rectangle \n0.551 \n0.623 \n41.324 \n7433 \n0 \n50 \nHyper-Ellipse \n0.749 \n0.221 \n1.7586 \n8245 \n34 \n16 \nSAA \nN/A \n0.749 \n0.221 \n1.7587 \n3 \u00d7 106 \n4 \n46 \niSSO \nVoronoi tessellation \n0.749 \n0.221 \n1.7586 \n6198 \n50 \n0  \nFig. 10. 120-bar dome truss structure [48].  \nTable 3 \nResults for the 120 bars truss structure.  \nMethod \nDesign parameters \n\u03bcg(\u0303\u03c6\u2217)\n(Nm) \n\u03c32\ng (\u0303\u03c6\u2217)\n(Nm)2 \nFE \nA1 \n(cm2) \nA2 \n(cm2) \nA3 \n(cm2) \nA4 \n(cm2) \nA5 \n(cm2) \nA6 \n(cm2) \nA7 \n(cm2) \nSSO [48] \n47.2 \n67.5 \n42.7 \n9.4 \n30.1 \n55.6 \n13.1 \n242.3 \n5294.2 \n54,700 \nSAA \n47.3 \n68.3 \n40.7 \n10.5 \n30.3 \n49.5 \n14.8 \n243.5 \n5407.4 \n3 \u00d7 106 \niSSO \n48.1 \n66.4 \n42.6 \n8.5 \n31.4 \n56.5 \n12.5 \n242.8 \n5317.2 \n14,937  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n11\noptimization of a base-isolation system attached to a 10-story building \nas shown in Fig. 11. This optimization problem includes maximizing the \nreliability of the base-isolated structure which is performed by the \nminimization of its failure probability and mathematically expressed as: \nminimize :\n\u03c6\u2208\u03a6\nP(F|\u03c6) = E\u03b8[IF(\u03c6, \u03b8)] =\n\u222b\n\u0398\nIF(\u03c6, \u03b8)p(\u03c6, \u03b8)d\u03b8,\n(24)  \nwhere, IF(\u03c6,\u03b8) is the function that indicates failure, and it equals 1 when \nthe system fails, i.e., when unacceptable performance occurs. Notably, \nin this problem h(\u03c6, \u03b8) = IF(\u03c6,\u03b8). \nThe 10-story building is considered as a shear structure with un-\ncertain inter-story stiffness and damping. Each story has a total mass of \n207 ton. The inter-story stiffness ki of all stories are parameterized by ki \n= \u0302ki\u03b8i, i = 1, \u2026, 10 where the most probable values of the inter-story \nstiffness are [\u0302ki] = [687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6, \n225.6, 184.5, 104.5] MN/m. The entity \u03b8i is a set of non-dimensional \nuncertain variables that are considered to be correlated Gaussian vari-\nables with a unit mean value \u0302\u03b8i = 1, \u2200i and a covariance matrix defined \nas: \nE\n[\n(\u03b8i \u2212 \u0302\u03b8i)\n(\n\u03b8j \u2212 \u0302\u03b8j\n)]\n= (0.2)2exp\n[\n\u2212 (j \u2212 i)2 /\n22]\n.\n(25) \nThe damping ratios are considered independent Gaussian variables \nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The Kanai- \nTajimi model is used to simulate the ground excitation modelled as a \nfiltered white noise process, with the power spectral density function \ngiven as: \nS(\u03c9) = S0\n\u03c94\ng + 4\u03b62\ng\u03c92\ng\u03c92\n(\n\u03c92\ng \u2212 \u03c92\n)2\n+ 4\u03b62\ng\u03c92\ng\u03c92\n,\n(26)  \nS0 = \u03c32\n\u03c9\n2\u03b6g\n\u03c0\u03c9g\n(\n4\u03b62\ng + 1\n)m2\n/\ns3,\n(27)  \nwhere, \u03c9g, \u03b6gand \u03c3\u03c9 are the resonant frequency, damping, and RMS of the \nacceleration input of the filter, respectively. These are also considered \nuncertain variables with mean values of [2\u03c0rad/s, 0.5, 0.2g] and a c.o.v \nequal to 0.20. The non-stationarity of the excitation is modeled by \nmultiplying the filter output with the envelope function as: \ne(t) = \u03bb3t\u03bb1exp( \u2212 \u03bb2t),\n(28)  \nwith parameters \u03bb1 = 1.25, \u03bb2 = 0.2 and \u03bb3 = 0.353 chosen to simulate \nstrong earthquake excitation for a duration of 40 s with a sampling time \nof 0.02 s. The base-isolation system considered is a lead\u2013rubber bilinear \nisolator with an additional viscous damper. The base has a 247-ton mass. \nThe design parameters \u03c6 for the base isolation structure system are the \nstiffness before yielding Kprand after yielding Kp, the yield force is Fy, \nand the damping coefficient cd. The reader may refer to [39,50] for \nadditional details regarding the base isolation structure system adopted \nin this study. \nFailure is indicated when any of the normalized base displacements \nor inter-story drifts exceeds unity. The normalization constants are 0.5 m \nand 0.033 m respectively. The design interval for each variable is \nspecified as Kpr = [50, 600] MN/m, Fy = [1, 8] MN, Kp = [5, 60] MN/m, \nand cd = [0.1, 10]MNs/m. In this example, iSSO and SSO are imple-\nmented with six number of iterations. \nTable 4 shows the optimization results for the best 10 independent \nsimulation runs. The comparison of the results obtained using SSO, SAA \n(with a sample size of 103), and iSSO shows that the optimal design \nobtained using the proposed approach iSSO is in good agreement. The \nfailure probability of the structure is reduced from 0.95 (without the \nbase isolation system) to 0.0326 after installing the optimally designed \nbase isolation system. \n5. Conclusion \nThis study attempts to provide an optimization approach called \n\"iSSO\", which is an improved version of SSO, primarily for stochastic \noptimization problems while it retains utility for deterministic optimi-\nzation problems as well. Two novel ideas are introduced in this study: \nfirst, a better characterization of the design space is offered by parti-\ntioning the design space into non-overlapping subregions using Voronoi \nFig. 11. (left) 10-story base isolated shear model, and (right) force-deformation of bilinear isolator [49].  \nTable 4 \nBase isolation structure system optimization results (best of 10 independent \nruns).  \nMethod \nDesign parameters (\u03c6*) \nFailure \nprobability \nPF(\u03c6*) \nKpr (MN/ \nm) \nFy \n(MN) \nKp (MN/ \nm) \ncd (MNs/ \nm) \nSSO  \n[39] \n425.33 \n1.20 \n15.52 \n6.54 \n0.0340 \nSAA \n414.68 \n1.16 \n16.15 \n6.26 \n0.0324 \niSSO \n418.34 \n1.11 \n15.88 \n7.08 \n0.0366  \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n12\ntessellation which improves the effectiveness and efficiency of the pro-\nposed iSSO considerably in comparison to SSO. Second, a novel \"double \nsort\" approach is proposed, eliminating the need for optimization to \nidentify the subregions for the optimal design at each iSSO iteration. \nSeveral mathematical and engineering design examples, including TMD, \n120 bars truss structure, and base-isolated structure, are included in this \nstudy to demonstrate the efficacy of the proposed iSSO. The results show \nthat the proposed iSSO effectively identifies the reduced design space for \ncomplex design problems with multiple global and local minima. This is \nattributable to the Voronoi tessellation, which eliminates the require-\nment of the presumed admissible design space form to resemble the \ncontour of the original design. Voronoi tessellation enabled better \ndesign space exploration, allowing multiple global minima scattered \nthroughout the design pace to be effectively identified. Due to the dis-\ncretization of the design space via Voronoi tessellation, computation \ndemand is significantly reduced as the number of function evaluations \nfor all examples is lower vis-a-vis the original SSO. Moreover, the novel \nidea of the double sort approach achieves the requisite precision in \nidentifying the subregions for optimal solutions and makes iSSO \nimplementation simple and effective. \nThe applicability of the approach is dependent on the creation of the \nVoronoi cells. At present the methods available in the literation for \ncreating the Voronoi tessellation are computationally demanding when \nconsidering problems of very high dimension. Future work will focus on \ndeveloping a method for creating the Voronoi tessellation in higher di-\nmensions, particularly those greater than ten. \nCRediT authorship contribution statement \nMohd Aman Khalid: Investigation, Methodology, Formal analysis, \nSoftware, Visualization, Writing \u2013 original draft. Sahil Bansal: \nConceptualization, Methodology, Supervision. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nData availability \nNo data was used for the research described in the article.  \nAppendix-A: Voronoi Tessellation \nVoronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or \nDirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with \neach polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each \npolygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more \npoints in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space.\nFig. 12. Voronoi diagram in 2-dimensional space.  \nThere are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the \nVoronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm [51], which generates a \nDelaunay triangulation in any number of dimensions, can be applied while creating a Voronoi diagram. The Delaunay triangulation is a triangulation \nof the point in which no point falls within the circumcircle of any triangle. The polygon generated by the intersection of the half-planes defined by the \nedges of the Delaunay triangles enclosing the point is therefore obtained as the Voronoi cell of a point. \nIt can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to \na set of points. Voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and \nmicrostructure study [52]. The MATLAB command \"Voronoin\" from the \"Parallel Computing Toolbox\" [54] has been used in this study to create the \nVoronoi cells. \nReferences \n[1] Marti K. Stochastic optimization methods. Berlin: Springer; 2008. \n[2] Tsompanakis Y, Lagaros ND, Papadrakakis M. Structural design optimization \nconsidering uncertainties. CRC Press; 2008. \n[3] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design \noptimization of structures using stochastic simulation method. Res Eng Des 2023; \n34:179\u2013200. https://doi.org/10.1007/s00163-022-00405-z. \n[4] Meng Z, Li G, Wang X, Sait SM, R\u0131za A. A comparative study of metaheuristic \nalgorithms for reliability \u2011 based design optimization problems. Arch Comput \nMethods Eng 2021;28:1853\u201369. https://doi.org/10.1007/s11831-020-09443-z. \n[5] Abualigah L, Elaziz MA, Khasawneh AM, Alshinwan M. Meta-heuristic \noptimization algorithms for solving real-world mechanical engineering design \nproblems : a comprehensive survey, applications, comparative analysis, and \nresults. Neural Comput Appl 2022;34:4081\u2013110. https://doi.org/10.1007/s00521- \n021-06747-4. \n[6] Katebi J, Shoaei M, Nguyen S, Trung T, Khorami M. Developed comparative \nanalysis of metaheuristic optimization algorithms for optimal active control of \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \nAdvances in Engineering Software 188 (2024) 103568\n13\nstructures. Eng Comput 2020;36:1539\u201358. https://doi.org/10.1007/s00366-019- \n00780-7. \n[7] Alorf A. Engineering applications of artificial intelligence a survey of recently \ndeveloped metaheuristics and their comparative analysis. Eng Appl Artif Intell \n2023;117:105622. https://doi.org/10.1016/j.engappai.2022.105622. \n[8] Kirsch U. Structural optimization: fundamentals and applications. Springer-Verlag; \n2012. \n[9] Floudas CA, Pardalos PA. Encyclopedia of optimization. Springer; 2008. \n[10] Kiureghian AD, Ditlevsen O. Aleatory or epistemic? Does it matter? Struct Saf \n2009;31:105\u201312. https://doi.org/10.1016/j.strusafe.2008.06.020. \n[11] Schu\u00a8eller GI, Jensen HA. Computational methods in optimization considering \nuncertainties - an overview. Comput Methods Appl Mech Eng 2008;198:2\u201313. \nhttps://doi.org/10.1016/j.cma.2008.05.004. \n[12] Schneider J, Kirkpatrick S. Stochastic optimization. Springer; 2007. \n[13] Do B, Ohsaki M. A random search for discrete robust design optimization of linear- \nelastic steel frames under interval parametric uncertainty. Comput Struct 2021; \n249:106506. https://doi.org/10.1016/j.compstruc.2021.106506. \n[14] Asadpoure A, Tootkaboni M, Guest JK. Robust topology optimization of structures \nwith uncertainties in stiffness - application to truss structures. Comput Struct 2011. \nhttps://doi.org/10.1016/j.compstruc.2010.11.004. \n[15] Doltsinis I, Kang Z. Robust design of structures using optimization methods. \nComput Methods Appl Mech Eng 2004;193:2221\u201337. https://doi.org/10.1016/j. \ncma.2003.12.055. \n[16] Carneiro G, das N, Ant\u00b4onio CC. Dimensional reduction applied to the reliability- \nbased robust design optimization of composite structures. Compos Struct 2021; \n255. https://doi.org/10.1016/j.compstruct.2020.112937. \n[17] An H, Youn BD, Kim HS. Reliability-based design optimization of laminated \ncomposite structures under delamination and material property uncertainties. Int J \nMech Sci 2021. https://doi.org/10.1016/j.ijmecsci.2021.106561. \n[18] Li Z, Duan LB, Cheng AG, Yao ZP, Chen T, Yao W. Lightweight and crashworthiness \ndesign of an electric vehicle using a six-sigma robust design optimization method. \nEng Optim 2019. https://doi.org/10.1080/0305215X.2018.1521396. \n[19] Gholinezhad H, Torabi SH. Reliability-based multidisciplinary design optimization \nof an underwater vehicle including cost analysis. J Mar Sci Technol 2021. https:// \ndoi.org/10.1007/s00773-021-00804-2. \n[20] Lee KH, Park GJ. Robust optimization considering tolerances of design variables. \nComput Struct 2001;79:77\u201386. https://doi.org/10.1016/S0045-7949(00)00117-6. \n[21] Anderson TV, Mattson CA. Propagating skewness and kurtosis through engineering \nmodels for low-cost, meaningful, nondeterministic design. J Mech Des Trans \nASME. 2012. https://doi.org/10.1115/1.4007389. \n[22] Zhou Q, Wang Y, Choi SK, Jiang P, Shao X, Hu J, Shu L. A robust optimization \napproach based on multi-fidelity metamodel. Struct Multidiscip Optim 2018. \nhttps://doi.org/10.1007/s00158-017-1783-4. \n[23] Wang GG, Shan S. Review of metamodeling techniques in support of engineering \ndesign optimization. J Mech Des Trans ASME. 2007;129:370\u201380. https://doi.org/ \n10.1115/1.2429697. \n[24] Chatterjee T, Chakraborty S, Chowdhury R. A critical review of surrogate assisted \nrobust design optimization. Arch Comput Methods Eng 2019;26:245\u201374. https:// \ndoi.org/10.1007/s11831-017-9240-5. \n[25] Chatterjee T, Friswell MI, Adhikari S, Chowdhury R. A global two-layer meta- \nmodel for response statistics in robust design optimization. Eng Optim 2021. \nhttps://doi.org/10.1080/0305215X.2020.1861262. \n[26] Guo X, Zhao X, Zhang W, Yan J, Sun G. Multi-scale robust design and optimization \nconsidering load uncertainties. Comput Methods Appl Mech Eng 2015;283: \n994\u20131009. https://doi.org/10.1016/j.cma.2014.10.014. \n[27] Jerez DJ, Jensen HA, Beer M. Reliability-based design optimization of structural \nsystems under stochastic excitation: an overview. Mech Syst Signal Process 2022. \nhttps://doi.org/10.1016/j.ymssp.2021.108397. \n[28] Li W, Gao L, Xiao M. Multidisciplinary robust design optimization under parameter \nand model uncertainties. Eng Optim 2020;52:426\u201345. https://doi.org/10.1080/ \n0305215X.2019.1590564. \n[29] Beyer HG, Sendhoff B. Robust optimization - a comprehensive survey. Comput \nMethods Appl Mech Eng 2007;196:3190\u2013218. https://doi.org/10.1016/j. \ncma.2007.03.003. \n[30] Motta R, de S, Afonso SMB. An efficient procedure for structural reliability-based \nrobust design optimization. Struct Multidiscip Optim 2016;54:511\u201330. https://doi. \norg/10.1007/s00158-016-1418-1. \n[31] Yildiz AR. Comparison of evolutionary-based optimization algorithms for \nstructural design optimization. Eng Appl Artif Intell 2013;26:327\u201333. https://doi. \norg/10.1016/j.engappai.2012.05.014. \n[32] Beck AT, Gomes WJDS. A comparison of deterministic, reliability-based and risk- \nbased structural optimization under uncertainty. Probab Eng Mech 2012;28:18\u201329. \nhttps://doi.org/10.1016/j.probengmech.2011.08.007. \n[33] Acar, E., Bayrak, G., Jung, Y., Lee, I., Ramu, P., Ravichandran, S.S.: Modeling, \nanalysis, and optimization under uncertainties: a review, (2021). 10.1007/s001 \n58-021-03026-7. \n[34] Georghiou A, Kuhn D, Wiesemann W. The decision rule approach to optimization \nunder uncertainty: methodology and applications. Comput Manag Sci 2019. \nhttps://doi.org/10.1007/s10287-018-0338-5. \n[35] Braydi O, Lafon P, Younes R. Study of uncertainties and objective function \nmodeling effects on probabilistic optimization results. ASCE ASME J Risk \nUncertain Eng Syst Part B Mech Eng 2019. https://doi.org/10.1115/1.4044152. \n[36] Liu WS, Cheung SH. Reliability based design optimization with approximate failure \nprobability function in partitioned design space. Reliab Eng Syst Saf 2017;167: \n602\u201311. https://doi.org/10.1016/j.ress.2017.07.007. \n[37] Chiralaksanakul A, Mahadevan S. First-order approximation methods in reliability- \nbased design optimization. J Mech Des Trans ASME 2005. https://doi.org/ \n10.1115/1.1899691. \n[38] Doltsinis I, Kang Z, Cheng G. Robust design of non-linear structures using \noptimization methods. Comput Methods Appl Mech Eng 2005;194:1779\u201395. \nhttps://doi.org/10.1016/j.cma.2004.02.027. \n[39] Taflanidis AA, Beck JL. Stochastic Subset Optimization for optimal reliability \nproblems. Probab Eng Mech 2008. https://doi.org/10.1016/j. \nprobengmech.2007.12.011. \n[40] Au SK, Beck JL. Estimation of small failure probabilities in high dimensions by \nsubset simulation. Probab Eng Mech 2001;16:263\u201377. https://doi.org/10.1016/ \nS0266-8920(01)00019-4. \n[41] Jia GF, Taflanidis AA. Non-parametric stochastic subset optimization for optimal- \nreliability design problems. Comput Struct 2013;126:86\u201399. https://doi.org/ \n10.1016/j.compstruc.2012.12.009. \n[42] Taflanidis AA. Stochastic subset optimization incorporating moving least squares \nresponse surface methodologies for stochastic sampling. Adv Eng Softw 2012;44: \n3\u201314. https://doi.org/10.1016/j.advengsoft.2011.07.009. \n[43] Khalid MA, Bansal S. Framework for robust design optimization of tuned mass \ndampers by stochastic subset optimization. Int J Struct Stab Dyn 2023;23. https:// \ndoi.org/10.1142/S0219455423501559. \n[44] Au SK. Reliability-based design sensitivity by efficient simulation. Comput Struct \n2005;83:1048\u201361. \n[45] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system \ndesign using stochastic simulation. Comput Methods Appl Mech Eng 2008. https:// \ndoi.org/10.1016/j.cma.2008.03.029. \n[46] Robert CP, Casella G. Monte carlo statistical methods. New York, NY: Springer; \n2004. \n[47] Li HS. Subset simulation for unconstrained global optimization. Appl Math Model \n2011;35:5108\u201320. https://doi.org/10.1016/j.apm.2011.04.023. \n[48] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design \noptimization of structures using stochastic simulation method. Res Eng Des 2022. \nhttps://doi.org/10.1007/s00163-022-00405-z. \n[49] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system \ndesign using stochastic simulation. Comput Methods Appl Mech Eng 2008;198: \n88\u2013101. https://doi.org/10.1016/j.cma.2008.03.029. \n[50] Kandemir EC, Mortazavi A. Optimization of seismic base isolation system using a \nfuzzy reinforced swarm intelligence. Adv Eng Softw 2022;174:103323. https://doi. \norg/10.1016/j.advengsoft.2022.103323. \n[51] Rebay S. Efficient unstructured mesh generation by means of delaunay \ntriangulation and Bowyer-Watson algorithm. J Comput Phys 1993;106:125\u201338. \n[52] Wade N, Graham-Brady L. Estimating microstructural feature distributions from \nimage data using a Bayesian framework. J Microsc 2023:1\u201316. https://doi.org/ \n10.1111/jmi.13184. \n[53] Duan X, Li L, Ge Y, Liu B. Exact Voronoi diagram for topographic spatial analysis. \nGIScience Remote Sens 2023;60. https://doi.org/10.1080/ \n15481603.2023.2171703. \n[54] MATLAB and parallel computing toolbox release. Natick, Massachusetts, United \nStates: The Mathworks, Inc.; 2021. \nM.A. Khalid and S. Bansal                                                                                                                                                                                                                   \n",
    "pdf_url": "",
    "references": [
      "[1] Marti K. Stochastic optimization methods. Berlin: Springer; 2008. ",
      "[2] Tsompanakis Y, Lagaros ND, Papadrakakis M. Structural design optimization ",
      "considering uncertainties. CRC Press; 2008. ",
      "[3] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design"
    ],
    "publication_date": "12-12-2023"
  }
]