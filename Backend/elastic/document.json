[
  {
    "titre": "Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model",
    "resume": "The emergence of novel types of communication, such as email, hasbeen brought on by the development of the internet, which radicallyconcentrated the way in that individuals communicate socially andwith one another. It is now establishing itself as a crucial aspect ofthe communication network which has been adopted by a varietyof commercial enterprises such as retail outlets. So in this researchpaper, we have built a unique spam-detection methodology basedon email-body sentiment analysis. The proposed hybrid model isput into practice and preprocessing the data, extracting the proper-ties, and categorizing data are all steps in the process. To examinethe emotive and sequential aspects of texts, we use word embed-ding and a bi-directional LSTM network. this model frequentlyshortens the training period, then utilizes the Convolution Layer toextract text features at a higher level for the Bi LSTM network. Ourmodel performs better than previous versions, with an accuracyrate of 9798%. In addition, we show that our model beats not justsome well-known machine learning classifiers but also cutting-edgemethods for identifying spam communications, demonstrating itssuperiority on its own. Suggested Ensemble models results areexamined in terms of recall, accuracy, and precision",
    "auteurs": [
      "Khushbu Doulani",
      "GRU",
      "Shivangi Sachan",
      "Khushbu Doulani",
      "Mainak Adhikari",
      "Noida",
      "Decision Trees",
      "Bi-LSTM",
      "GRU",
      "BiLSTM+GRU"
    ],
    "institutions": [
      "Department of CSEIIIT Lucknow Lucknow, UP, ",
      "Mainak Adhikari Department of CSEIIIT LucknowUP, "
    ],
    "mots_cles": [
      " Dataset",
      " KNN",
      " Gaussian Naive Bayes",
      " LSTM",
      " SVM",
      " Bidirectional "
    ],
    "texte_integral": "Semantic Analysis and Classification of Emails through\nInformative Selection of Features and Ensemble AI Model\nShivangi Sachan\u2217\nDepartment of CSE\nIIIT Lucknow\nLucknow, UP, India\nmcs21025@iiitl.ac.in\nKhushbu Doulani\nVardhaman College of Engineering\nHyderabad, India\nkhushidoulani@gmail.com\nMainak Adhikari\nDepartment of CSE\nIIIT Lucknow\nUP, India\nmainak.ism@gmail.com\nABSTRACT\nThe emergence of novel types of communication, such as email, has\nbeen brought on by the development of the internet, which radically\nconcentrated the way in that individuals communicate socially and\nwith one another. It is now establishing itself as a crucial aspect of\nthe communication network which has been adopted by a variety\nof commercial enterprises such as retail outlets. So in this research\npaper, we have built a unique spam-detection methodology based\non email-body sentiment analysis. The proposed hybrid model is\nput into practice and preprocessing the data, extracting the proper-\nties, and categorizing data are all steps in the process. To examine\nthe emotive and sequential aspects of texts, we use word embed-\nding and a bi-directional LSTM network. this model frequently\nshortens the training period, then utilizes the Convolution Layer to\nextract text features at a higher level for the BiLSTM network. Our\nmodel performs better than previous versions, with an accuracy\nrate of 97\u201398%. In addition, we show that our model beats not just\nsome well-known machine learning classifiers but also cutting-edge\nmethods for identifying spam communications, demonstrating its\nsuperiority on its own. Suggested Ensemble model\u2019s results are\nexamined in terms of recall, accuracy, and precision\nCCS CONCEPTS\n\u2022 Computer systems organization \u2192 Embedded systems; Re-\ndundancy; Robotics; \u2022 Networks \u2192 Network reliability.\nKEYWORDS\nDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional\nLSTM, GRU, Word-Embeddings, CNN\nACM Reference Format:\nShivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. Semantic\nAnalysis and Classification of Emails through Informative Selection of\nFeatures and Ensemble AI Model. In 2023 Fifteenth International Conference\non Contemporary Computing (IC3-2023) (IC3 2023), August 03\u201305, 2023, Noida,\nIndia. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.\n3607979\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nIC3 2023, August 03\u201305, 2023, Noida, India\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0022-4/23/08...$15.00\nhttps://doi.org/10.1145/3607947.3607979\n1\nINTRODUCTION\nOver the past few years, a clear surge of both the amount of spam-\nmers as well as spam emails. This is likely due to a fact that the\ninvestment necessary for engaging in the spamming industry is\nrelatively low. As a result of this, we currently have a system that\nidentifies every email as suspicious, which has caused major expen-\nditures in the investment of defense systems [12]. Emails are used\nfor online crimes like fraud, hacking, phishing, E-mail bombing, bul-\nlying, and spamming. [16]. Algorithms that are based on machine\nlearning (ML) are now the most effective and often used approach to\nthe recognition of spam. Phishing, which is defined as a fraudulent\nattempt to acquire private information by masquerading as a trust-\nworthy party in electronic communication, has rapidly advanced\npast use of simple techniques and the tactic of casting a wide net;\ninstead, spear phishing uses a variety of sophisticated techniques\nto target a single high-value individual. Other researchers used NB,\nDecision Trees, and SVM to compare the performance of supervised\nML algorithms for spam identification [6]. Spam emails clog up re-\ncipients\u2019 inboxes with unsolicited communications, which frustrate\nthem and push them into the attacker\u2019s planned traps [7]. As a re-\nsult, spam messages unquestionably pose a risk to both email users\nand the Internet community. In addition, Users may occasionally\nread the entire text of an unsolicited message that is delivered to\nthe target users\u2019 inboxes without realizing that the message is junk\nand then choosing to avoid it. Building a framework for email spam\ndetection is the aim of this project. In this approach, we combine the\nWord-Embedding Network with the CNN layer, Bi-LSTM, and GRU\n(BiLSTM+GRU). CNN layers are used to speed up training time\nbefore the Bi-LSTM network, and more advanced textual character-\nistics are extracted with the use of this network in comparison to\nthe straight LSTM network, in less time. Gated recurrent neural net-\nworks (GRUs) are then added because they train more quickly and\nperform better for language modeling. To evaluate and investigate\nvarious machine learning algorithms for predicting email spam,\nand develop a hybrid classification algorithm to filter email spam\nbefore employing an ensemble classification algorithm to forecast\nit. To put an innovative technique into practice and compare it to\nthe current method in terms of various metrics. Ensemble learn-\ning, a successful machine learning paradigm, combines a group of\nlearners rather than a single learner to forecast unknown target\nattributes. Bagging, boosting, voting, and stacking are the four main\ntypes of ensemble learning techniques. To increase performance,\nan integrated method and the combining of two or three algorithms\nare also suggested. Extraction of text-based features takes a long\ntime. Furthermore, it can be challenging to extract all of the crucial\ninformation from a short text. Over the span associated with this\n181\nIC3 2023, August 03\u201305, 2023, Noida, India\nSachan et al.\nresearch, we utilize Bidirectional Large Short-Term Memories (Bi-\nLSTM) in conjunction with Convolutional Neural Networks (CNN)\nto come up with an innovative method to the detection of spam.\nBagging and boosting approaches were widely preferred in this\nstudy. Contribution and paper organization is as follows: section 1.1\ndescribes literature study, section 1.2 describe motivation for this\nresearch work, section 2 sketches procedure of details implemen-\ntation, Section 3 present experimental setup, dataset description\nand evaluation metrics, and section 4 summarizing outcomes of the\nexperiment.\n1.1\nRelated Work\nEmail is indeed the second most frequently utilized Internet appli-\ncation as well as the third most common method of cyberbullying,\nclaims one study. Cybercriminals exploit it in a number of ways,\nincluding as sending obscene or abusive messages, adding viruses\nto emails, snatching the private information of victims, and ex-\nposing it to a broad audience. Spam letters made up 53.95% of all\nemail traffic in March 2020. We examine three main types of un-\nlawful emails in our study. First are fake emails, which are sent\nto manipulate recipients to submit sensitive information. The sec-\nond as being cyberbullying\u2019s use of harassing emails to threaten\nindividuals. Suspicious emails that describe illegal activities belong\nto the third category. Many researchers have earlier contributed\nmassively to this subject. The researcher claims there is some proof\nthat suspicious emails were sent before to the events of 9/11. [14].\nWhen it comes to data labeling, there are also convinced rule-based\napproaches and technologies ( like VADER) that are used, even\nthough their efficiency of the are together is adversely affected. A\nhidden layer, which itself is essential for vectorization, is the top\nlayer of the model. We use oversampling methods for this minority\nclass because of the absence of data. Sampling techniques can help\nwith multicollinearity, but they have an impact on simulation re-\nsults. Oversampling causes data to be randomly repeated, which\naffects test data because dividing data may result in duplicates. Un-\ndersampling may result in the loss of some strong information. In\norder to advance email research, it is crucial to provide datasets on\ncriminal activity. P. Garg et al. (2021) [5], which revealed that spam\nin an email was detected in 70 percent of business emails, spam was\nestablished as an obstacle for email administrators. Recognizing\nspam and getting rid of it were the primary concerns, as spam can\nbe offensive, may lead to other internet sites being tricked, which\ncan offer harmful data, and can feature those who are not particu-\nlar with their content using NLP. To select the best-trained model,\neach mail transmission protocol requires precise and effective email\nclassification, a machine learning comparison is done. Our study\nhas suggested that innovative deep learning outperforms learning\nalgorithms like SVM and RF. Current studies on the classification\nof emails use a variety of machine learning (ML) techniques, with\na few of them focusing on the study of the sentiments consisted of\nwithin email databases. The lack of datasets is a significant obstacle\nto email classification. There are few publicly accessible E-mail\ndatasets, thus researchers must use these datasets to test their hy-\npotheses or gather data on their own. Authors[15] describe supplied\ntwo-phased outlier detection models to enhance the IIOT network\u2019s\ndependability. Artificial Neural Network, SVM, Gaussian NB, and\nRF (random forest) ensemble techniques were performed to forecast\nclass labels, and the outputs were input into a classifying unit to\nincrease accuracy. A method for content-based phishing detection\nwas presented by the authors in [2], to classify phishing emails,\nthey employed RF. They categorize spam and phishing emails. They\nenhanced phishing email classifiers with more accurate predictions\nby extracting features. They showed some effective Machine learn-\ning spam filtering techniques. When the PCA method is used, it will\nlower the number of features in the dataset. The collected features\ngo through the PCA algorithm to reduce the number of features.\nThe PCA method is used to make a straightforward representation\nof the information which illustrates the amount of variability there\nis in the data. The authors of [20] presented the Fuzzy C-means\nmethod for classifying spam email. To stop spam, they implemented\na membership threshold value. A methodology to identify unla-\nbeled data was put forth by the authors of [1] and applied motive\nanalysis to the Enron data collection. They divided the data into\ncategories that were favorable, negative, and neutral. They grouped\nthe data using k-means clustering, an unsupervised ML technique\nand then classified it using the supervised ML techniques SVM and\nNB. Hina, Maryam, and colleagues (2021) implemented Sefaced:\nDeep learning-based semantic analysis and categorization of e-mail\ndata using a forensic technique. For multiclass email classification,\nSeFACED employs a Gated Recurrent Neural Network (GRU) based\non Long Short-Term Memory (LSTM). Different random weight ini-\ntializations affect LSTMs [9]. Zhang, Yan, et al.(2019) Experiments\non three-way game-theoretic rough set (GTRS) email spam filter-\ning show that it is feasible to significantly boost coverage without\ndecreasing accuracy [23]. According to Xia et al. [22], SMS spam\nhas been identified using machine learning model such as naive\nbayes , vector-space modeling, support vector machines (SVM),\nlong selective memory machines (LSTM), and convolutional neural\nnetworks including every instance of a method for categorizing\ndata. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic\ngradient descent (sgd) algorithms for e-mail filtering with R and\norange software spam [3]. Orange software was used to create the\nclassifications, which included Adaboost and SGD. The majority of\nresearchers focused on text-based email spam classification meth-\nods because image-based spam can be filtered in the early stages\nof pre-processing. There are widely used word bag (BoW) model,\nwhich believes that documents are merely unordered collections\nof words, is the foundation for these techniques. Kumaresan [11]\nexplains SVM with a cuckoo search algorithm was used to extract\ntextual features for spam detection. Renuka and Visalakshi made\nuse of svm [17] spam email identification, followed by selecting\nfeatures using Latent Semantic Indexing (LSI). Here we have used\nlabeled dataset to train the hybrid classifier. We used TF-IDF for\nfeature extraction [20] and Textual features for spam detection\nwere extracted using SVM and a cuckoo search algorithm. [4] for\nfiltering out the spam email. Combining the integrated strategy to\nthe pure SVM and NB methods, overall accuracy is really improved.\nMoreover, accurate detection for spam email has been proposed\nusing the Negative Selection Algorithm (NSA) and Particle Swarm\nOptimization\u2019s (PSO) algorithm. PSO is used in this instance to\nimprove the effectiveness of the classifier.\n182\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\nIC3 2023, August 03\u201305, 2023, Noida, India\n1.2\nMotivation and Novelty\nEmail is most common form of communication between people\nin this digital age. Many users have been victims of spam emails,\nand their personal information has been compromised. The email\nClassification technique is employed to identify and filter junk\nmail, junk, and virus-infected emails prior to reach a user\u2019s inbox.\nExisting email classification methods result in irrelevant emails\nand/or the loss of valuable information. Keeping these constraints\nin mind, the following contributions are made in this paper:\n\u2022 Text-based feature extraction is a lengthy process. Further-\nmore, extracting every important feature from text is difficult.\nIn this paper, we show how to employ GRU with Convo-\nlutional Neural Networks and Bidirectional-LSTM to find\nspam.\n\u2022 Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-\nral Networks to examine the relationships, sentimental con-\ntent, and sequential way of email contents.\n\u2022 Applied CNN before the Bi-LSTM network, training time can\nbe sped up. This network can also extract more advanced\ntextual features faster than the Bi-LSTM network alone when\ncombined with the GRU network.\n\u2022 We use Enorn Corpora datasets and compute precision, re-\ncall, and f-score to assess how well the suggested technique\nperforms. Our model outperforms several well-known ma-\nchine learning techniques as well as more contemporary\nmethods for spam message detection.\n2\nPROPOSED SYSTEM ARCHITECTURE AND\nMODEL\nE-mail is a valuable tool for communicating with other users. Email\nallows the sender to efficiently forward millions of advertisements\nat no cost. Unfortunately, this scheme is now being used in a variety\nof organizations. As a result, a massive amount of redundant emails\nis known as spam or junk mail, many people are confused about the\nemails in their E- Mailboxes. Each learning sequence is given for-\nward as well as backward to two different LSTM networks that are\nattached to the same outputs layer in order for bidirectional Lstms\nto function. This indicates that the Bi-LSTM has detailed sequential\ninformation about all points before and following each point in a\nspecific sequence. In other words, we concatenate the outputs from\nboth the forward and the backward LSTM at each time step rather\nthan just encoding the sequence in the forward direction. Each\nword\u2019s encoded form now comprehends the words that come before\nand after it. This is a problem for the Internet community. The di-\nagram depicts various stages that aid in the prediction of email spam:\nBecause real-world data is messy and contains unnecessary infor-\nmation and duplication, data preprocessing is critical in natural\nlanguage processing (NLP). The major preprocessing steps are de-\npicted below.\n2.1\nNLP Tokenization\nTokenization of documents into words follows predefined rules.\nThe tokenization step is carried out in Python with spacy library.\n2.2\nStop Words Removal\nStop words appear infrequently or frequently in the document, but\nthey are less significant in terms of importance. As a result, these\nare removed to improve data processing.\n2.3\nText Normalization\nA word\u2019s lexicon form or order may differ. Thus, they must all be\nchanged to their root word to be correctly analyzed. Lemmatization\nand stemming are the two methods that can be used for normal-\nization. When a word\u2019s final few characters are removed to create\na shorter form, even if that form has no meaning, the procedure\nis known as stemming. lemmatization [21] is a mixture of corpus-\nbased an rule-based methods, and it retains the context of a term\nwhile changing it back to its root.\n2.4\nFeature Extraction\nfeature extraction which transforms the initial text into its features\nso that it may be used for modeling after being cleaned up and\nnormalized. Before predicting them, we use a specific way to give\nweights to specific terms in our document. While it is simple for a\ncomputer to process numbers, we choose to represent individual\nwords numerically. In such cases, we choose word embeddings. IDF\nis the count of documents containing the term divided by the total\nnumber of documents, and occurrence is the amount of instances a\nword appears in a document. We derive characteristics based on\nequations. 1,2,3,4,5, and 6. We use equations to derive properties.\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51\ud835\udc53 = \ud835\udc61\ud835\udc53 \u2217\n\ufffd 1\n\ud835\udc51\ud835\udc53\n\ufffd\n(1)\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51\ud835\udc53 = \ud835\udc61\ud835\udc53 \u2217 Inverse(\ud835\udc51\ud835\udc53 )\n(2)\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51\ud835\udc53 (\ud835\udc61,\ud835\udc51, \ud835\udc37) = \ud835\udc47 \ud835\udc53 (\ud835\udc61,\ud835\udc51).\ud835\udc3c\ud835\udc51\ud835\udc53 (\ud835\udc61, \ud835\udc37)\n(3)\n\ud835\udc47\ud835\udc3c\ud835\udc51\ud835\udc53 (\ud835\udc61,\ud835\udc51) = log\n\ud835\udc41\n|\ud835\udc51\ud835\udf16\ud835\udc37\ud835\udc61\ud835\udf16\ud835\udc37|\n(4)\nA word2vec neural network-based approach is the method that is\nutilized for this goal as the tool. The following equation, referred\nto as 5, shows how word2vec handles word context through the\nuse of probability-accurate measurements. Here letter D stands for\nthe paired-wise display of a set of words, while the letters w and c0\nor c1 represent paired word context that originated from a larger\ncollection of set D.\n\ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64,\ud835\udc5011:\ud835\udc58) =\n1\n1 + \ud835\udc52\u2212(\ud835\udc64\u00b7\ud835\udc5011+\ud835\udc64\u00b7\ud835\udc5012+...+\ud835\udc64\u00b7\ud835\udc501\ud835\udc58)\n(5)\n\ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64,\ud835\udc501:\ud835\udc58) =\n1\n1 + \ud835\udc52\u2212(\ud835\udc64\u00b7\ud835\udc500)\n(6)\n183\nIC3 2023, August 03\u201305, 2023, Noida, India\nSachan et al.\n2.5\nWord-Embeddings\nWord-Embedding helps to improve on the typical \"bag-of-words\"\nworldview, which requires a massive sparse feature vector to score\nevery word individually to represent this same entire vocabulary.\nThis perception is sparse because the vocabulary is large, and each\nword or document is defined by a massive vector. Using a word\nmap-based dictionary, word embedding needs to be converted terms\n(words) into real value feature vectors. There are two basic issues\nwith standard feature engineering techniques for deep learning.\nData is represented using sparse vectors, and the second is that\nsome of the meanings of words are not taken into consideration.\nSimilar phrases will have values in embedding vectors that are\nalmost real-valued. The Input length in our proposed study is set\nto 700 for our suggested model. If the texts seemed to be integer\nencoded with value systems between 10 and 20, the vocabulary\ndistance would be 11. Our data is encoded as integers, and the input\nand output dimensions are both set to 50,000. The embedding layer\noutcome will be used in successive layers and for BiLSTM and GRU\nlayers.\n2.6\nMachine Learning Model\nWithin the scope of the research, we are using the subsequent ma-\nchine learning techniques, to examine and compare the overall\nefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-\nchine, Gaussian NB, Logistic Regression, K - nearest neighbors, and\nRandom Forest (RF).\n2.7\nConvolution Network\nThe popular RNN model generally performs well but takes too\nlong to train the model incorporating the textual sequential data.\nWhen a layer is added after the RNN layer, the model\u2019s learning\nduration is considerably decreased. Higher-level feature extraction\nis another benefit. [19] additionally possible using the convolutional\nlayer. In essence, the convolution layer looks for combinations of\nthe various words or paragraphs in the document that involve the\nfilters. We use features with 128 dimensions and a size 10 for each.\nFor this task, the Relu activation function is utilized. After that, the\none-dimensional largest pooling layers with a pooling size of 4 are\nput on the data in order to obtain higher-level features.\n2.8\nBiLSTM Network with GRU\nRecurrent Neural Network (RNN) technique of text sentiment anal-\nysis is particularly well-liked and frequently applied. Recurrent\nneural networks (RNN) surpass conventional neural networks. be-\ncause it can remember the information from earlier time steps\nthanks to its memory. A state vector is combined with an RNN\u2019s\ndata to create a new state vector. The resulting state vector uses the\npresent to recollect past knowledge. The RNN is straightforward\nand is based on the following equations:\n\u210e\ud835\udc61 = tanh (\ud835\udc4a\u210e\u210e\u210e\ud835\udc61\u22121 +\ud835\udc4a\ud835\udf0b\u210e\ud835\udc65\ud835\udc61)\n(7)\n\ud835\udc66\ud835\udc61 = \ud835\udc4a\u210e\ud835\udc66\u210e\ud835\udc61\n(8)\nThe vanilla RNN[18]is not very good at remembering previous\nsequences. In addition to that, RNN struggles with diminishing\ngradient descent. A kind of RNN is a long short-term recall network\n(LSTM), solves a vanishing gradient descent problem and learns\nlong-term dependencies[10]. LSTM was actually created to address\nthe problem of long-term reliance. LSTM has the unique ability to\nrecall. The cell state is the LSTM model\u2019s central concept. With\nonly a small amount of linear interaction, the cell state follows the\nsequence essentially unmodified from beginning to end. gate of\nan LSTM is also significant. Under the command of these gates,\ninformation is safely inserted to or eliminated from the cell stated.\nThe following equations are used by the LSTM model to update\neach cell:\n\ud835\udc53\ud835\udc61 = \ud835\udf0e\n\ufffd\n\ud835\udc4a\ud835\udc53 \u00b7 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc53\n\ufffd\n(9)\nIn this case, Xt denotes input, and ht is the hidden state at the t\ntime step. The following is the revised cell state Ct:\n\ud835\udc56t = \ud835\udf0e (\ud835\udc4a\ud835\udc56 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc56)\n(10)\n\ud835\udc36\ud835\udc47 = tanh (\ud835\udc4a\ud835\udc50 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc50\ud835\udc61)\n(11)\n\ud835\udc36\ud835\udc61 = \ud835\udc53\ud835\udc61 \u2217 \ud835\udc36\ud835\udc61\u22121 + \ud835\udc56\ud835\udc61 \u2217 \ud835\udc36\ud835\udc47\n(12)\nHere, we may compute the output and hidden state at t time steps\nusing the point-wise multiplication operator *.\n\ud835\udc5c\ud835\udc61 = \ud835\udf0e (\ud835\udc4a\ud835\udc5c \u00b7 [\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61] + \ud835\udc4f\ud835\udc5c)\n(13)\n\u210e\ud835\udc61 = \ud835\udc5c\ud835\udc61 \u2217 tanh (\ud835\udc36\ud835\udc61)\n(14)\nDue to the reality it only considers all prior contexts from the\npresent one, LSTM does have a few drawbacks. As a result of this,\nit may accept data from preceding time steps through LSTM as well\nas RNN. Therefore, in order to avoid this issue, further improve-\nments are carried out with the help of a bidirectional recurrent\nneural network(Bi-RNN). BiRNN [13] can handle two pieces of in-\nformation from both the front and the back. Bi-LSTM is created\nby combining the Bi-RNN and LSTM. As a result, operating LSTM\nhas advantages such as cell state storage so that BiRNN have way\nto acknowledge from the context before and after. As a conse-\nquence of this, it provides the Bi-LSTM with the advantages of an\nLSTM with feedback for the next layer. Remembering long-term\ndependencies is a significant new benefit of Bi-LSTM. The output,\nwhich is a feature vector, will be based on the call state. Finally,\nwe forecast the probability of email content as Normal, Fraudu-\nlent, Harassment, and Suspicious Emails using as an input to the\nsoftmax activation function, which is a weighted sum of the dense\nlayer\u2019s outputs. To regulate the information flow, GRU employs\nthe point-wise multiplying function and logistic sigmoid activation.\nThe GRU has hidden states of storage memory and does not have\ndistinct memory cells or units for state control. The W, U, and b\nvectors, which stand for weights, gates, and biases, respectively, are\ncrucial variables that must be calculated during the creation of the\nGRU model. For training reasons, the pre-trained word embedding\nknown as the Glove vector is used. They made it clear that GRU\nis the superior model when there is a large amount of training\ndata for textual groups and word embedding is available. BiLSTM,\nCNN, and GRU is required so as to compensate for the deletion\nof the document\u2019s long-term and short-term connections. In our\ncase, the embedding dimension, maximum sequence length, and\nlexicon size were used to start the LSTM embedding layer in three\nseparate LSTM models. The input vector was modified to make it\nappropriate for such a Conv1D layer, prior situations\u2019 sequences are\nreturned by LSTM layer. The \"return sequences\" of the LSTM layer\nmust be set to False when the subsequent state is free of the gated\n184\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\nIC3 2023, August 03\u201305, 2023, Noida, India\narchitecture. Quantity of learning parameters must be taken into\nconsideration. A 350-unit LSTM layer was set - up, and different\nLSTM unit combinations were tested. More importantly, because\nit has more parts, the model made with BiLSTM will take longer\nto train. Bidirectional LSTM is the name of a particular kind of\nrecurrent neural network that is primarily used for the processing\nof natural languages. (BiLSTM). It is able to use data from both\nsides, and, in contrast to regular LSTM, it enables input flow in\nboth directions. It is an effective instrument for demonstrating the\nlogical relationships between words and phrases, and this involves\nboth the forward and backward directions of the sequence. In con-\nclusion, BiLSTM works by adding one extra layer of LSTM, causing\nthe information flow to travel in the other direction. It only denotes\nthat the input sequence runs in reverse at the next LSTM layer. Mul-\ntiple operations, including averaging, summation, multiplication,\nand concatenation, are then applied to the results of the two LSTM\nlayers. The gated design of Bi-LSTM and GRU networks solves\nthe disappearing gradient and exploding problems. A good way to\nhandle more long sequences is to use Bi-LSMT and GRU together.\nGRU works well with datasets that don\u2019t have text. In two to three\nrounds, the complicated CNN+BiLSTM+GRU model learns the long\nsequence of email text well. We have used word embedding, cnn,\nbidirectional lstm and gru networks as our three building blocks\nto separate email messages based on their sentiment and text\u2019s\nsequential features. Also, we succinctly demonstrate below why\nthese blocks help identify email spam:\n\u2022 First, We have used the Sequence - to - sequence Lstm as the\ncurrent block in the networks since it can retrieve both the\nprevious and next sequences from the current. More so than\na straightforward LSTM network, it can also recognize and\nextract text sentiment and sequential properties.\n\u2022 Second, we extract the more complex and advanced charac-\nteristics for Bi-LSTM network using Convolutional Network\nblock, which is the network\u2019s second block after the Bi-LSTM\nblock. Bi-LSTM takes a long time to extract text-based fea-\ntures, hence one of the reasons for using this block is to\nreduce the network\u2019s overall training time.\n3\nEXPERIMENTAL EVALUATION\n3.1\nExperimental Setup\nWe divided the information into training and testing groups of\n80/20. We divided the remaining 20% of the 80 percent training\ndata into test data for the model. Construct, compute, and evaluate\nthe efficacy of the suggested method using the Pythonic packages\nKeras, as TensorFlow and Scikit learn.\n3.2\nDataset Description\nEmail spam detection is the foundation of this research project. The\ndataset includes normal emails from the Enron corpora, deceptive\nemails from phished email corpora, harassment emails chosen from\nhate speech, and the offensive dataset. Only the content of the email\nbody is used for analysis; all header information, including sender,\ntopic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word\nEmbedding are used to extract characteristics from the email mes-\nsage and classify them. This dataset[8] is publicly available. The\npresented model is implemented using Python, and several metrics,\nincluding accuracy, precision, and recall, are used to examine the\noutcomes.\n3.3\nEvaluation Metrics and Results\nClassifier performance is assessed Using metrics such as accuracy,\nprecision, and recall. Four terms make up a confusion matrix that\nis used to calculate these metrics.\n\u2022 True positives (TP) are positive values that have been accu-\nrately assigned the positive label.\n\u2022 The negative values that are accurately identified as negative\nare known as True Negatives (TN).\n\u2022 True Negative values are those that can be accurately identi-\nfied as being negative (TN).\n\u2022 Positive readings that have been mistakenly labeled as nega-\ntive are known as False Negatives (FN).\nAssess the efficacy of the suggested model is listed below:\n3.3.1\nAccuracy. Accuracy reveals how frequently the ML model\nwas overall correct.\nAccuracy =\n\ud835\udc47\ud835\udc43 +\ud835\udc47\ud835\udc41\n\ud835\udc47\ud835\udc43 +\ud835\udc47\ud835\udc41 + \ud835\udc39\ud835\udc43 + \ud835\udc39\ud835\udc41\n(15)\n3.3.2\nPrecision. The accuracy of the model gauges how effectively\nit can predict a specific category.\nPrecision =\n\ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc43\n(16)\n3.3.3\nRecall. Recall tells us how often the model was able to rec-\nognize a specific category.\nRecall =\n\ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41\n(17)\nModel\nAccuracy\nPrecision\nRecall\nGaussian NB\n91.3\n90.1\n91.8\nRandom Forest\n88.41\n90\n88\nKNN\n86.6\n89\n87\nSVM\n92.4\n91\n92\nLSTM\n95.2\n95\n95.7\nProposed\nEnsemble\n(CNN,BiLSTM+GRU)\n97.32\n95.6\n95.3\nTable 1: Differet Model\u2019s Score on Test Data\nAccuracy, Precision, and Recall metrics are computed. In the\ngiven Table 1 where six different classifiers are Gaussian NB, Ran-\ndom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid\nModel (CNN+BiLSTM+GRU) have been used in this work. In the\nCNN, Bi-LSTM, and GRU architectures which enable sequence pre-\ndiction, CNN strands for feature extraction on data input which are\ncombined with LSTM. It requires less time training and a higher\nexpandable model. Any bottlenecks are created by predictions and\nthe increasing number of distinct units of information. This model\nis useful for dealing with issue-related classifications that consist\nof two or more than two classes. So suggested Ensemble model, out\nof these six classifiers, produces more accurate findings.\n185\nIC3 2023, August 03\u201305, 2023, Noida, India\nSachan et al.\nFigure 1: Performance Analysis\n3.4\nComparative Analysis\nA model\u2019s ability to fit new data is measured by the validation\nloss, whereas its ability to fit training data is determined by the\ntraining loss. The two main variables that decide whether in which\nlearning is efficient or not are validation loss and training loss.\nLSTM and Suggested Ensemble hybrid Models have equivalent loss\nand accuracy. In this context, we are contrasting the LSTM with the\nproposed model (CNN, Bilstm, and GRU) in terms of their respective\nvalidation accuracies and losses. The model\u2019s accuracy was at its\nhighest after 14 epochs of operation when it achieved an accuracy\nof roughly 97-98% while minimizing model loss.\nFigure 2: LSTM Model Training and Validation Accuracy\nFigure 3: LSTM Model Training and Validation Loss\nFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Training\nand Validation Accuracy\nFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Training\nand Validation Loss\n186\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\nIC3 2023, August 03\u201305, 2023, Noida, India\nIn this Proposed ensemble hybrid model\u2019s train accuracy is 98.7%\nValidation accuracy is 97.32% and LSTM has train accuracy of 97.41%\nand validation accuracy is 95.2%. So based on figures 3 and 5 indicate\nthe validation loss for LSTM and the proposed ensemble hybrid\nmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show the\nvalidation accuracy to be 95.2% and 97.3%, respectively. LSTM and\nthe proposed hybrid model used ensemble artificial intelligence,\nwith the proposed hybrid model outperforming the LSTM. We\ndecide on dense architecture as the final model for identifying the\ntext messages as spam or nonspam based on loss, accuracy, and the\naforementioned charts. The loss and accuracy over epochs are more\nstable than LSTM, and the Proposed classifier has a straightforward\nstructure.\n4\nCONCLUSION\nThe model is composed of four networks Word-Embeddings, CNN,\nBi-LSTM, and GRU. We may train the model more quickly by using\nthe convolutional layer first, followed by the word-embedding layer,\nand then the BiLSTM network. The Bidirectional LSTM network\nalso has higher-level properties that we can extract. We have used\na bidirectional LSTM(BiLSTM)and GRU network to memorize a\nsentence\u2019s contextual meaning and sequential structure, which im-\nproves the model\u2019s performance accuracy to roughly 97.32 percent.\nREFERENCES\n[1] Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-\nbeled email data. In 2019 International Conference on Computational Intelligence\nand Knowledge Economy (ICCIKE). IEEE, 328\u2013333.\n[2] Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-\nrithm to filter spam using machine learning techniques. Pacific Science Review A:\nNatural Science and Engineering 18, 2 (2016), 145\u2013149.\n[3] Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic\ngradient descent (sgd) algorithms with R and orange software for filtering e-mail\nspam. In 2019 11th Computer Science and Electronic Engineering (CEEC). IEEE,\n41\u201346.\n[4] Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. A\nsupport vector machine based naive Bayes algorithm for spam filtering. In 2016\nIEEE 35th International Performance Computing and Communications Conference\n(IPCCC). IEEE, 1\u20138.\n[5] Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam Filtering\nTechniques based on Natural Language Processing Framework. In 2021 11th Inter-\nnational Conference on Cloud Computing, Data Science & Engineering (Confluence).\nIEEE, 30\u201335.\n[6] Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-\nlyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316\u2013342.\n[7] Radicati Group et al. 2015. Email Statistics Report 2015\u20132019. Radicati Group.\nAccessed August 13 (2015), 2019.\n[8] Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensic\nanalysis and classification of e-mail data using deep learning. IEEE Access 9\n(2021), 98398\u201398411.\n[9] Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali\nKhan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis and\nclassification of e-mail data using deep learning. IEEE Access 9 (2021), 98398\u2013\n98411.\n[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and Yuan\nZhang. 2017. Short-term residential load forecasting based on LSTM recurrent\nneural network. IEEE transactions on smart grid 10, 1 (2017), 841\u2013851.\n[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoo\nsearch and support vector machine. International Journal of Bio-Inspired Compu-\ntation 9, 3 (2017), 142\u2013156.\n[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification of\nspam emails using deep learning. In 2021 1st Babylon International Conference on\nInformation Technology and Science (BICITS). IEEE, 63\u201368.\n[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural\nnetwork language model. In 2012 IEEE Spoken Language Technology Workshop\n(SLT). IEEE, 234\u2013239.\n[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.\n2014. Detection of fraudulent emails by employing advanced feature abundance.\nEgyptian Informatics Journal 15, 3 (2014), 169\u2013174.\n[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,\nand Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT using\nensemble classifier. arXiv preprint arXiv:2102.01515 (2021).\n[16] Justinas Rastenis, Simona Ramanauskait\u02d9e, Justinas Janulevi\u010dius, Antanas \u010cenys,\nAsta Slotkien\u02d9e, and K\u0119stutis Pakrijauskas. 2020. E-mail-based phishing attack\ntaxonomy. Applied Sciences 10, 7 (2020), 2363.\n[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVM\nmodel for email spam classification. (2014).\n[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequence\nmodeling for intelligent typing assistant with Bangla and English keyboard. In\n2018 International Conference on Innovation in Engineering and Technology (ICIET).\nIEEE, 1\u20136.\n[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Ha\u015fim Sak. 2015. Convolu-\ntional, long short-term memory, fully connected deep neural networks. In 2015\nIEEE international conference on acoustics, speech and signal processing (ICASSP).\nIeee, 4580\u20134584.\n[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spam\nmessages and mails using fuzzy C means algorithm. In 2019 4th International\nConference on Internet of Things: Smart Innovation and Usages (IoT-SIU). IEEE,\n1\u20135.\n[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-\nzation and part-of-speech prediction. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP. 486\u2013494.\n[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boosting\nthroughput on rule-based filtering systems. IEEE Access 8 (2020), 82653\u201382661.\n[23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filtering\nwith game-theoretic rough sets. In 2019 International conference on computing,\nnetworking and communications (ICNC). IEEE, 552\u2013556.\nReceived 15 April 2023\n187\n",
    "pdf_url": "/media/Article_02_3ac80585055b4ad8a0f2f4441c34d582.pdf",
    "references": [
      "[1] Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-",
      "beled email data. In 2019 International Conference on Computational Intelligence",
      "and Knowledge Economy (ICCIKE). IEEE, 328\u2013333.",
      "[2] Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-",
      "rithm to filter spam using machine learning techniques. Pacific Science Review A:",
      "Natural Science and Engineering 18, 2 (2016), 145\u2013149.",
      "[3] Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic"
    ],
    "publication_date": "2023-11-06T00:00:00",
    "corrected": 0,
    "_id": "l_pca40B7ffmqzjIzInG"
  },
  {
    "titre": "Large Language Model Augmented Narrative Driven Recommendations",
    "resume": "Narrative-driven recommendation (NDR) presents an informationaccess problem where users solicit recommendations with verbosedescriptions of their preferences and context, for example, travelerssoliciting recommendations for points of interest while describ-ing their likes/dislikes and travel circumstances. These requestsare increasingly important with the rise of natural language-basedconversational interfaces for search and recommendation systems.However, NDR lacks abundant training data for models, and currentplatforms commonly do not support these requests. Fortunately,classical user-item interaction datasets contain rich textual data,e.g., reviews, which often describe user preferences and context this may be used to bootstrap training for NDR models. In thiswork, we explore using large language models (LL Ms) for dataaugmentation to train NDR models. We use LLMs for authoringsynthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on syntheticqueries and user-item interaction data. Our experiments demon-strate that this is an effective strategy for training small-parameterretrieval models that outperform other retrieval and LLM baselinesfor narrative-driven recommendation.",
    "auteurs": [
      "Andrew McCallum",
      "Recommender",
      "Andrew McCallum",
      "Hamed Zamani",
      "Recommender",
      "subreddits1",
      "\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62"
    ],
    "institutions": [
      "Hamed  University of Massachusetts AmherstUSA",
      "Sheshera  University of Massachusetts AmherstUSA",
      "Andrew Mc  of Massachusetts AmherstUSA"
    ],
    "mots_cles": [
      "queries",
      "models",
      "user",
      "narrative",
      "retrieval",
      "item",
      "mint",
      "items",
      "model",
      "language"
    ],
    "texte_integral": "Large Language Model Augmented Narrative Driven\nRecommendations\nSheshera Mysore\nsmysore@cs.umass.edu\nUniversity of Massachusetts Amherst\nUSA\nAndrew McCallum\nmccallum@cs.umass.edu\nUniversity of Massachusetts Amherst\nUSA\nHamed Zamani\nhzamani@cs.umass.edu\nUniversity of Massachusetts Amherst\nUSA\nABSTRACT\nNarrative-driven recommendation (NDR) presents an information\naccess problem where users solicit recommendations with verbose\ndescriptions of their preferences and context, for example, travelers\nsoliciting recommendations for points of interest while describ-\ning their likes/dislikes and travel circumstances. These requests\nare increasingly important with the rise of natural language-based\nconversational interfaces for search and recommendation systems.\nHowever, NDR lacks abundant training data for models, and current\nplatforms commonly do not support these requests. Fortunately,\nclassical user-item interaction datasets contain rich textual data,\ne.g., reviews, which often describe user preferences and context\n\u2013 this may be used to bootstrap training for NDR models. In this\nwork, we explore using large language models (LLMs) for data\naugmentation to train NDR models. We use LLMs for authoring\nsynthetic narrative queries from user-item interactions with few-\nshot prompting and train retrieval models for NDR on synthetic\nqueries and user-item interaction data. Our experiments demon-\nstrate that this is an effective strategy for training small-parameter\nretrieval models that outperform other retrieval and LLM baselines\nfor narrative-driven recommendation.\nCCS CONCEPTS\n\u2022 Information systems \u2192 Recommender systems; Users and inter-\nactive retrieval; \u2022 Computing methodologies \u2192 Natural language\ngeneration.\nACM Reference Format:\nSheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large\nLanguage Model Augmented Narrative Driven Recommendations. In Sev-\nenteenth ACM Conference on Recommender Systems (RecSys \u201923), Septem-\nber 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages.\nhttps://doi.org/10.1145/3604915.3608829\n1\nINTRODUCTION\nRecommender systems personalized to users are an important com-\nponent of several industry-scale platforms [16, 17, 46]. These sys-\ntems function by inferring users\u2019 interests from their prior inter-\nactions on the platform and making recommendations based on\nthese inferred interests. While recommendations based on historical\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0241-9/23/09...$15.00\nhttps://doi.org/10.1145/3604915.3608829\ninteractions are effective, users soliciting recommendations often\nstart with a vague idea about their desired target items or may\ndesire recommendations depending on the context of use, often\nmissing in historical interaction data (Figure 1). In these scenarios,\nit is common for users to solicit recommendations through long-\nform narrative queries describing their broad interests and context.\nInformation access tasks like these have been studied as narrative-\ndriven recommendations (NDR) for items ranging from books [5]\nand movies [18], to points of interest [1]. Bogers and Koolen [5]\nnote these narrative requests to be common on discussion forums\nand several subreddits1, but, there is a lack of support for these\ncomplex natural language queries in current recommenders.\nHowever, with the emergence of conversational interfaces for\ninformation access tasks, support for complex NDR tasks is likely\nto become necessary. In this context, recent work has noted an\nincrease in complex and subjective natural language requests com-\npared to more conventional search interfaces [13, 34]. Furthermore,\nthe emergence of large language models (LLM) with strong lan-\nguage understanding capabilities presents the potential for fulfilling\nsuch complex requests [9, 33]. This work explores the potential for\nre-purposing historical user-item recommendation datasets, tra-\nditionally used for training collaborative filtering recommenders,\nwith LLMs to support NDR.\nSpecifically, given a user\u2019s interactions, \ud835\udc37\ud835\udc62, with items and\ntheir accompanying text documents (e.g., reviews, descriptions)\n\ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1, selected from a user-item interaction dataset I, we\nprompt InstructGPT, a 175B parameter LLM, to author a synthetic\nnarrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the\nquery \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user\nreviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based\non a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user doc-\nument, \ud835\udc51\ud835\udc56. Then, a pre-trained LM based retrieval model (110M\nparameters) is fine-tuned for retrieval on the synthetic queries and\nfiltered reviews.\nOur approach, which we refer to as Mint2, follows from the\nobservation that while narrative queries and suggestions are often\nmade in online discussion forums, and could serve as training data,\nthe number of these posts and the diversity of domains for which\nthey are available is significantly smaller than the size and diversity\nof passively gathered user-item interaction datasets. E.g. while\nBogers and Koolen [5] note nearly 25,000 narrative requests for\nbooks on the LibraryThing discussion forum, a publicly available\nuser-item interaction dataset for Goodreads contains interactions\nwith nearly 2.2M books by 460k users [43] .\nWe empirically evaluate Mint in a publicly available test collec-\ntion for point of interest recommendation: pointrec [1]. To train\n1r/MovieSuggestions, r/booksuggestions, r/Animesuggest\n2Mint: Data augMentation with INteraction narraTives.\n777\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nMysore, McCallum, Zamani\nFigure 1: An example narrative query soliciting point of\ninterest recommendations. The query describes the users\npreferences and the context of their request.\nFigure 2: The format of the prompt used in Mint for\ngenerating synthetic narrative queries from user-item\ninteraction with a large language model.\nour NDR models, we generate synthetic training data based on\nuser-item interaction datasets from Yelp. Models (110M parameters)\ntrained with Mint significantly outperform several baseline models\nand match the performance of significantly larger LLM baselines\nautoregressively generating recommendations. Code and synthetic\ndatasets are available:3\n2\nRELATED WORK\nData Augmentation for Information Access. A line of recent\nwork has explored using language models to generate synthetic\nqueries for data augmentation to train models for information re-\ntrieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of\ninterest, a pre-trained language model is used to create synthetic\nqueries for the document collection. An optional filtering step ex-\ncludes noisy queries, and finally, a bi-encoder or a cross-encoder is\ntrained for the retrieval task. While earlier work of Ma et al. [31]\ntrain a custom query generation model on web-text datasets, more\nrecent work has leveraged large language models for zero/few-shot\nquestion generation [7, 8, 15, 23]. In generating synthetic queries,\nthis work indicates the effectiveness of smaller parameter LLMs\n(up to 6B parameters) for generating synthetic queries in simpler\ninformation-retrieval tasks [7, 8, 23], and finds larger models (100B\nparameters and above) to be necessary for harder tasks such as\nargument retrieval [15, 23]. Similar to this work, we explore the\ngeneration of synthetic queries with LLMs for a retrieval task. Un-\nlike this work, we demonstrate a data augmentation method for\ncreating effective training data from sets of user documents found in\nrecommendation datasets rather than individual documents. Other\nwork in this space has also explored training more efficient multi-\nvector models from synthetic queries instead of more expensive\ncross-encoder models [39] and generating queries with a diverse\nrange of intents than the ones available in implicit feedback datasets\nto enhance item retrievability [35].\n3https://github.com/iesl/narrative-driven-rec-mint/\nBesides creating queries for ad-hoc retrieval tasks, concurrent\nwork of Leszczynski et al. [25] has also explored the creation of syn-\nthetic conversational search datasets from music recommendation\ndatasets with LLMs. The synthetic queries and user documents are\nthen used to train bi-encoder retrieval models for conversational\nsearch. Our work resembles this in creating synthetic queries from\nsets of user items found in recommendation interaction datasets.\nHowever, it differs in the task of focus, creating long-form narra-\ntive queries for NDR. Finally, our work also builds on the recent\nperspective of Radlinski et al. [36] who make a case for natural\nlanguage user profiles driving recommenders \u2013 narrative requests\ntie closely to natural language user profiles. Our work presents a\nstep toward these systems.\nFinally, while our work explores data augmentation from user-\nitem interactions for a retrieval-oriented NDR task, prior work has\nalso explored data augmentation of the user-item graph for training\ncollaborative filtering models. This work has often explored aug-\nmentation to improve recommendation performance for minority\n[12, 47] or cold-start users [11, 28, 45]. And has leveraged genera-\ntive models [11, 45] and text similarity models [28] for augmenting\nthe user-item graph.\nComplex Queries in Information Access. With the advent\nof performant models for text understanding, focus on complex\nand interactive information access tasks has seen a resurgence\n[2, 29, 32, 48]. NDR presents an example of this \u2013 NDR was first\nformalized in Bogers and Koolen [5] for the case of book recommen-\ndation and subsequently studied in other domains [3, 4, 6]. Bogers\nand Koolen [5] systematically examined narrative requests posted\nby users on discussion forums. They defined NDR as a task requir-\ning item recommendation based on a long-form narrative query\nand prior-user item interactions. While this formulation resembles\npersonalized search [42] and query-driven recommendation [20],\nthe length and complexity of requests differentiate these from NDR.\nOther work has also demonstrated the effectiveness of re-ranking\ninitial recommendations from collaborative filtering approaches\n778\nLarge Language Model Augmented Narrative Driven Recommendations\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nFigure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering\nmodels for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with\na large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic\nqueries and user items.\nbased on the narrative query [18]. More recent work of Afzali et al.\n[1] formulate the NDR task without access to the prior interactions\nof a user while also noting the value of contextual cues contained\nin the narrative request. In our work, we focus on this latter for-\nmulation of NDR, given the lack of focus on effectively using the\nrich narrative queries in most prior work. Further, we demonstrate\nthe usefulness of data augmentation from LLMs and user-item\ninteraction datasets lacking narrative queries.\nBesides this, a range of work has explored more complex, long-\nform, and interactive query formulations for information access;\nthese resemble queries in NDR. Arguello et al. [2] define the tip of\ntongue retrieval task, a known-item search task where user queries\ndescribe the rich context of items while being unable to recall item\nmetadata itself. Mysore et al. [32] formulate an aspect conditional\nquery-by example task where results must match specific aspects of\na long natural language query. And finally, a vibrant body of work\nhas explored conversational critiquing of recommenders where nat-\nural language feedback helps tune the recommendations received\nby users [30, 44, 49].\n3\nMETHOD\n3.1\nProblem Setup\nIn our work, we define narrative-driven recommendation (NDR) to\nbe a ranking task, where given a narrative query \ud835\udc5e made by a user\n\ud835\udc62, a ranking system \ud835\udc53 must generate a ranking \ud835\udc45 over a collection\nof items C. Further, we assume access to a user-item interaction\ndataset I consisting of user interactions with items (\ud835\udc62, {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1). We\nassume the items \ud835\udc51\ud835\udc56 to be textual documents like reviews or item\ndescriptions. While we don\u2019t assume there to be any overlap in the\nusers making narrative queries or the collection of items C and the\nuser-items interaction dataset I, we assume them to be from the\nsame broad domain, e.g., books, movies, points-of-interest.\n3.2\nProposed Method\nOur proposed method, Mint, for NDR, re-purposes a dataset of\nabundantly available user-item interactions, I = {(\ud835\udc62, {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1)} into\ntraining data for retrieval models by using LLMs as query gener-\nation models to author narrative queries \ud835\udc5e\ud835\udc62: D = {(\ud835\udc5e\ud835\udc62, {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1)}.\nThen, retrieval models are trained on the synthetic dataset D (Fig-\nure 3).\n3.2.1\nNarrative Queries from LLMs. To author a narrative query \ud835\udc5e\ud835\udc62\nfor a user in I, we make use of the 175B parameter InstructGPT4\nmodel as our query generation model QGen. We include the text\nof interacted items {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1 in the prompt for QGen, and instruct it\nto author a narrative query (Figure 2). To improve the coherence\nof generated queries and obtain correctly formatted outputs, we\nmanually author narrative queries for 3 topically diverse users\nbased on their interacted items and include it in the prompt for\nQGen. The same three few shot examples are used for the whole\ndataset I, and the three users were chosen from I. Generating\nnarrative queries based on user interactions may also be considered\na form of multi-document summarization for generating a natural\nlanguage user profile [36].\n3.2.2\nFiltering Items for Synthetic Queries. Since we expect user\nitems to capture multiple aspects of their interests and generated\nqueries to only capture a subset of these interests, we only retain\nsome of the items present in {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1 before using it for training re-\ntrieval models. For this, we use a pre-trained language model to com-\npute the likelihood of the query given each user item, \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62|\ud835\udc51\ud835\udc56),\nand only retain the top \ud835\udc40 highly scoring item for \ud835\udc5e\ud835\udc62, this re-\nsults in \ud835\udc40 training samples per user for our NDR retrieval models:\n{(\ud835\udc5e\ud835\udc62,\ud835\udc51\ud835\udc56)\ud835\udc40\n\ud835\udc56=1}. In our experiments, we use FlanT5 with 3B parame-\nters [14] for computing and follow Sachan et al. [40] for computing\n\ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62|\ud835\udc51\ud835\udc56). Note that our use of \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62|\ud835\udc51\ud835\udc56) represents a query-\nlikelihood model classically used for ad-hoc search and recently\nshown to be an effective unsupervised re-ranking method when\nused with large pre-trained language models [40].\n3.2.3\nTraining Retrieval Models. We train bi-encoder and cross-\nencoder models for NDR on the generated synthetic dataset \u2013 com-\nmonly used models in search tasks. Bi-encoders are commonly used\nas scalable first-stage rankers from a large collection of items. On the\nother hand, cross-encoders allow a richer interaction between query\nand item and are used as second-stage re-ranking models. For both\nmodels, we use a pre-trained transformer language model architec-\nture with 110M parameters, MPnet, a model similar to Bert [41].\nBi-encoder models embed the query and item independently into\nhigh dimensional vectors: q\ud835\udc62 = MPNet(\ud835\udc5e\ud835\udc62), d\ud835\udc56 = MPNet(\ud835\udc51\ud835\udc56) and\nrank items for the user based on the minimum L2 distance between\n4https://platform.openai.com/docs/models/gpt-3, text-davinci-003\n779\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nMysore, McCallum, Zamani\nq\ud835\udc62 and d\ud835\udc56. Embeddings are obtained by averaging token embeddings\nfrom the final layer of MPNet, and the same model is used for both\nqueries and items. Cross-encoder models input both the query and\nitem and output a score to be used for ranking \ud835\udc60 = \ud835\udc53Cr([\ud835\udc5e\ud835\udc62;\ud835\udc51\ud835\udc56]),\nwhere \ud835\udc53Cr is parameterized as w\ud835\udc47 dropout\n\ufffd\nW\ud835\udc47 MPNet(\u00b7)\n\ufffd\n. We\ntrain our bi-encoder model with a margin ranking loss: L\ud835\udc35\ud835\udc56 =\n\ufffd\n\ud835\udc62\n\ufffd\ud835\udc40\n\ud835\udc56=1 max[\ud835\udc3f2(q\ud835\udc62, d\ud835\udc56) \u2212 \ud835\udc3f2(q\ud835\udc62, d\n\u2032\n\ud835\udc56) + \ud835\udeff, 0] with randomly sam-\npled negatives \ud835\udc51\n\u2032 and \ud835\udeff = 1. Our cross-encoders are trained with\na cross-entropy loss: L\ud835\udc36\ud835\udc5f = \ufffd\n\ud835\udc62\n\ufffd\ud835\udc40\n\ud835\udc56=1 log(\n\ufffd \ud835\udc52\ud835\udc60\n\ud835\udc51\u2032 \ud835\udc52\ud835\udc60\u2032 ). For training, 4\nnegative example items \ud835\udc51\u2032 are randomly sampled from ranks 100-\n300 from our trained bi-encoder. At test time, we retrieve the top\n200 items with our trained bi-encoder and re-rank them with the\ncross-encoder - we evaluate both these components in experiments\nand refer to them as BiEnc-Mint and CrEnc-Mint.\n4\nEXPERIMENTS AND RESULTS\nNext, we evaluate Mint on a publicly available test collection for\nNDR and present a series of ablations.\n4.1\nExperimental Setup\n4.1.1\nDatasets. We perform evaluations on an NDR dataset for\npoint-of-interest (POI) recommendation Pointrec [1]. Pointrec\ncontains 112 realistic narrative queries (130 words long) obtained\nfrom discussion forums on Reddit and items pooled from baseline\nrankers. The items are annotated on a graded relevance scale by\ncrowd-workers and/or discussion forum members and further vali-\ndated by the dataset authors. The item collection C in Pointrec\ncontains 700k POIs with metadata (category, city) and noisy text\nsnippets describing the POI obtained from the Bing search engine.\nFor test time ranking, we only rank the candidate items in the city\nand request category (e.g., \u201cRestaurants\u201d) of the query available in\nPointrec - this follows prior practice to exclude clearly irrelevant\nitems [1, 26]. We use user-item interaction datasets from Yelp to\ngenerate synthetic queries for training.5 Note also that we limit our\nevaluations to Pointrec since it presents the only publicly avail-\nable, manually annotated, and candidate pooled test collection for\nNDR, to our knowledge. Other datasets for NDR use document col-\nlections that are no longer publicly accessible [24], contain sparse\nand noisy relevance judgments due to them being determined with\nautomatic rules applied to discussion threads [18, 24], lack pooling\nto gather candidates for judging relevance [18, 24], or lack realistic\nnarrative queries [21]. We leave the development of more robust\ntest collections and evaluation methods for NDR to future work.\n4.1.2\nImplementation Details. Next, we describe important details\nfor Mint and leave finer details of the model and training to our\ncode release. To sample user interactions for generating synthetic\nqueries from the Yelp dataset, we exclude POIs and users with\nfewer than ten reviews to ensure that users were regular users of\nthe site with well represented interests. This follows common prior\npractice in preparing user-item interaction datasets for use [27].\nThen we retain users who deliver an average rating greater than\n3/5 and with 10-30 above-average reviews. This desirably biases\nour data to users who commonly describe their likings (rather than\n5https://www.yelp.com/dataset\ndislikes). It also retains the users whose interests are summarizable\nby QGen. In the Yelp dataset, this results in 45,193 retained users.\nNow, 10,000 randomly selected users are chosen for generating syn-\nthetic narrative queries. For these users, a single randomly selected\nsentence from 10 of their reviews is included in the prompt (Figure\n2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some\nitems are filtered out (\u00a73.2.2). Here, we exclude 40% of the items\nfor a user. This results in about 60,000 training samples for training\nBiEnc-Mint and CrEnc-Mint. These decisions were made manu-\nally by examining the resulting datasets and the cost of authoring\nqueries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230.\n4.1.3\nBaselines. We compare BiEnc-Mint and CrEnc-Mint mod-\nels against several standard and performant retrieval model base-\nlines. These span zero-shot/unsupervised rankers, supervised bi-\nencoders, unsupervised cross-encoders, and LLM baselines. BM25:\nA standard unsupervised sparse retrieval baseline based on term\noverlap between query and document, with strong generalization\nperformance across tasks and domains [38]. Contriver: A BERT-base\nbi-encoder model pre-trained for zero-shot retrieval with weakly su-\npervised query-document pairs [22]. MPNet-1B: A strong Sentence-\nBert bi-encoder model initialized with MPNet-base and trained on\n1 billion supervised query-document pairs aggregated from numer-\nous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned\non supervised question-passage pairs from MSMarco. UPR: A two-\nstage approach that retrieves items with a Contriver bi-encoder\nand re-ranks the top 200 items with a query-likelihood model using\na FlanT5 model with 3B parameters [14, 40]. This may be seen\nas an unsupervised \u201ccross-encoder\u201d model. Grounded LLM: A re-\ncently proposed two-stage approach which autoregressively gener-\nates ten pseudo-relevant items using an LLM (175B InstructGPT)\nprompted with the narrative query and generates recommenda-\ntions grounded in C by retrieving the nearest neighbors for each\ngenerated item using a bi-encoder [19]. We include one few-shot\nexample of a narrative query and recommended items in the prompt\nto the LLM. We run this baseline three times and report average\nperformance across runs. We report NDCG at 5 and 10, MAP, MRR,\nand Recall at 100 and 200. Finally, our reported results should be\nconsidered lower bounds on realistic performance due to the un-\njudged documents (about 70% at \ud835\udc58 = 10) in our test collections\n[10].\n4.2\nResults\nTable 1 presents the performance of the proposed method compared\nagainst baselines. Here, bold numbers indicate the best-performing\nmodel, and superscripts indicate statistical significance computed\nwith two-sided t-tests at \ud835\udc5d < 0.05.\nHere, we first note the performance of baseline approaches. We\nsee BM25 outperformed by Contriver, a transformer bi-encoder\nmodel trained for zero-shot retrieval; this mirrors prior work [22].\nNext, we see supervised bi-encoder models trained on similar pas-\nsage (MPNet-1B) and question-answer (BERT-MSM) pairs outper-\nform a weakly supervised model (Contriver) by smaller margins.\nFinally, the Grounded LLM outperforms all bi-encoder baselines, in-\ndicating strong few-shot generalization and mirroring prior results\n[19]. Examining the Mint models, we first note that the BiEnc-\nMint sees statistically significant improvement compared to BM25\n780\nLarge Language Model Augmented Narrative Driven Recommendations\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nTable 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts\ndenote statistically significant improvements compared to specific baseline models.\nPointrec\nModel\nParameters\nNDCG@5\nNDCG@10\nMAP\nMRR\nRecall@100\nRecall@200\n1BM25\n-\n0.2682\n0.2464\n0.1182\n0.2685\n0.4194\n0.5429\n2Contriver\n110M\n0.2924\n0.2776\n0.1660\n0.3355\n0.4455\n0.5552\n3MPNet-1B\n110M\n0.3038\n0.2842\n0.1621\n0.3566\n0.4439\n0.5657\n4BERT-MSM\n110M\n0.3117\n0.2886\n0.1528\n0.3320\n0.4679\n0.5816\n5Grounded LLM\n175B+110M\n0.3558\n0.3251\n0.1808\n0.3861\n0.4797\n0.5797\n6UPR\n110M+3B\n0.3586\n0.3242\n0.1712\n0.4013\n0.4489\n0.5552\nBiEnc-Mint\n110M\n0.34891\n0.32631\n0.18901\n0.39821\n0.49141\n0.6221\nCrEnc-Mint\n2\u00d7110M\n0.372512\n0.348912\n0.219214\n0.43171\n0.5448123\n0.6221\nand outperforms the best bi-encoder baselines by 11-13% on preci-\nsion measures and 5-7% on recall measures. Specifically, we see a\nmodel trained for question-answering (BERT-MSM) underperform\nBiEnc-Mint, indicating the challenge of the NDR task. Further,\nBiEnc-Mint, trained on 5 orders of magnitude lesser data than\nMPNet-1B, sees improved performance \u2013 indicating the quality of\ndata obtained from Mint. Furthermore, BiEnc-Mint also performs\nat par with a 175B LLM while offering the inference efficiency of a\nsmall-parameter bi-encoder. Next, we see CrEnc-Mint outperform\nthe baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM\nby 4-21% on precision measures and 7-13% on recall measures \u2013\ndemonstrating the value of Mint for training NDR models.\n4.3\nAblations\nIn Table 2, we ablate various design choices in Mint. Different\nchoices result in different training sets for the BiEnc and CrEnc\nmodels. Also, note that in reporting ablation performance for CrEnc,\nwe still use the performant BiEnc-Mint model for obtaining nega-\ntive examples for training and first-stage ranking. Without high-\nquality negative examples, we found CrEnc to result in much poorer\nperformance.\nNo item filtering. Since synthetic queries are unlikely to rep-\nresent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1\nwhich have a low likelihood of being generated from the document\n(\u00a73.2.2). Without this step, we expect the training set for training\nretrieval models to be larger and noisier. In Table 2, we see that\nexcluding this step leads to a lower performance for BiEnc and\nCrEnc, indicating that the quality of data obtained is important for\nperformance.\n6B LLM for QGen. Mint relies on using an expensive 175B pa-\nrameter InstructGPT model for QGen. Here, we investigate the\nefficacy for generating\ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1 with a 6B parameter Instruct-\nGPT model (text-curie-001). We use an identical setup to the\n175B LLM for this. In Table 2, we see that training on the synthetic\nnarrative queries of the smaller LLM results in worse models \u2013 of-\nten underperforming the baselines in Table 1. This indicates the\ninability of a smaller model to generate complex narrative queries\nwhile conditioning on a set of user items. This necessity of a larger\nLLM for generating queries in complex retrieval tasks has been\nobserved in prior work [15, 23].\n6B LLM for Item Queries. We find a smaller 6B LLM to result\nin poor quality data when used to generate narrative queries con-\nditioned on {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1. Here we simplify the text generation task \u2013\nusing a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56. This\nexperiment also mirrors the setup for generating synthetic queries\nfor search tasks [7, 15]. Here, we use 3-few shot examples and sam-\nple one item per user for generating \ud835\udc5e\ud835\udc62. Given the lower cost of\nusing a smaller LLM, we use all 45,193 users in our Yelp dataset\nrather than a smaller random sample. From Table 2, we see that this\nresults in higher quality queries than using smaller LLMs for gen-\nerating narrative queries from {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1. The resulting BiEnc model\nunderperforms the BiEnc-Mint, indicating the value of generating\ncomplex queries conditioned on multiple items as in Mint for NDR.\nWe see that CrEnc approaches the performance of CrEnc-Mint\u2013\nnote, however, that this approach uses the performant BiEnc-Mint\nfor sampling negatives and first stage ranking. We leave further\nexploration of using small parameter LLMs for data augmentation\nfor NDR models to future work.\n5\nCONCLUSIONS\nIn this paper, we present Mint, a data augmentation method for the\nnarrative-driven recommendation (NDR) task. Mint re-purposes\nhistorical user-item interaction datasets for NDR by using a 175B pa-\nrameter large language model to author long-form narrative queries\nwhile conditioning on the text of items liked by users. We evaluate\nbi-encoder and cross-encoder models trained on data from Mint on\nthe publicly available Pointrec test collection for narrative-driven\npoint of interest recommendation. We demonstrate that the result-\ning models outperform several strong baselines and ablated models\nand match or outperform a 175B LLM directly used for NDR in a\n1-shot setup.\nHowever, Mint also presents some limitations. Given our use of\nhistorical interaction datasets for generating synthetic training data\nand the prevalence of popular interests in these datasets longer,\ntailed interests are unlikely to be present in the generated syn-\nthetic datasets. In turn, causing retrieval models to likely see poorer\nperformance on these requests. Our use of LLMs to generate syn-\nthetic queries also causes the queries to be repetitive in structure,\nlikely causing novel longer-tail queries to be poorly served. These\nlimitations may be addressed in future work.\n781\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nMysore, McCallum, Zamani\nTable 2: Mint ablated for different design choices on Pointrec.\nPointrec\nAblation\nNDCG@5\nNDCG@10\nMAP\nMRR\nRecall@100\nRecall@200\nBiEnc-Mint\n0.3489\n0.3263\n0.1890\n0.3982\n0.5263\n0.6221\n\u2212 No item filtering\n0.2949\n0.2766\n0.1634\n0.3505\n0.4979\n0.5951\n\u2212 6B LLM for QGen\n0.2336\n0.2293\n0.1125\n0.2287\n0.426\n0.5435\n\u2212 6B LLM for Item Queries\n0.3012\n0.2875\n0.1721\n0.3384\n0.4800\n0.5909\nCrEnc-Mint\n0.3725\n0.3489\n0.2192\n0.4317\n0.5448\n0.6221\n\u2212 No item filtering\n0.3570\n0.3379\n0.2071\n0.4063\n0.5366\n0.6221\n\u2212 6B LLM for QGen\n0.2618\n0.2421\n0.1341\n0.3118\n0.4841\n0.6221\n\u2212 6B LLM for Item Queries\n0.3792\n0.3451\n0.2128\n0.4098\n0.5546\n0.6221\nBesides this, other avenues also present rich future work. While\nMint leverages a 175B LLM for generating synthetic queries, smaller\nparameter LLMs may be explored for this purpose - perhaps by\ntraining dedicated QGen models. Mint may also be expanded to\nexplore more active strategies for sampling items and users for\nwhom narrative queries are authored - this may allow more effi-\ncient use of large parameter LLMs while ensuring higher quality\ntraining datasets. Next, the generation of synthetic queries from\nsets of documents may be explored for a broader range of retrieval\ntasks beyond NDR given its promise to generate larger training\nsets \u2013 a currently underexplored direction. Finally, given the lack of\nlarger-scale test collections for NDR and the effectiveness of LLMs\nfor authoring narrative queries from user-item interaction, fruitful\nfuture work may also explore the creation of larger-scale datasets\nin a mixed-initiative setup to robustly evaluate models for NDR.\nACKNOWLEDGMENTS\nWe thank anonymous reviewers for their invaluable feedback. This\nwork was partly supported by the Center for Intelligent Informa-\ntion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of\nNaval Research contract number N000142212688, an Amazon Alexa\nPrize grant, and the Chan Zuckerberg Initiative under the project\nScientific Knowledge Base Construction. Any opinions, findings\nand conclusions or recommendations expressed here are those of\nthe authors and do not necessarily reflect those of the sponsors.\nREFERENCES\n[1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:\nA Test Collection for Narrative-Driven Point of Interest Recommendation. In\nProceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As-\nsociation for Computing Machinery, New York, NY, USA, 2478\u20132484.\nhttps:\n//doi.org/10.1145/3404835.3463243\n[2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and\nFernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in\nMovie Identification. In Proceedings of the 6th international ACM SIGIR Conference\non Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/\ndoi/10.1145/3406522.3446021\n[3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018.\n\u201cWhat was this Movie About this Chick?\u201d A Comparative Study of Relevance\nAspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter-\nnational Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings\n13. Springer, 323\u2013334.\n[4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019.\n\u201cLooking for an amazing game I can relax and sink hours into...\u201d: A Study of\nRelevance Aspects in Video Game Discovery. In Information in Contemporary\nSociety: 14th International Conference, iConference 2019, Washington, DC, USA,\nMarch 31\u2013April 3, 2019, Proceedings 14. Springer, 503\u2013515.\n[5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven\nRecommendation. In Proceedings of the Eleventh ACM Conference on Recommender\nSystems (Como, Italy) (RecSys \u201917). Association for Computing Machinery, New\nYork, NY, USA, 238\u2013242. https://doi.org/10.1145/3109859.3109893\n[6] Toine Bogers and Marijn Koolen. 2018. \u201cI\u2019m looking for something like...\u201d:\nCombining Narratives and Example Items for Narrative-driven Book Recommen-\ndation. In Knowledge-aware and Conversational Recommender Systems Workshop.\nCEUR Workshop Proceedings.\n[7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nInPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings\nof the 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing\nMachinery, New York, NY, USA, 2387\u20132392.\nhttps://doi.org/10.1145/3477495.\n3531863\n[8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,\nRamya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-\npervised Training of Efficient Rankers. arXiv:2301.02998\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-\nzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nInc., 1877\u20131901.\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nInformation. In Proceedings of the 27th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (Sheffield, United Kingdom)\n(SIGIR \u201904). Association for Computing Machinery, New York, NY, USA, 25\u201332.\nhttps://doi.org/10.1145/1008992.1009000\n[11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-\nCF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing\nCold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con-\nference on Research and Development in Information Retrieval (Virtual Event,\nChina) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA,\n1251\u20131260. https://doi.org/10.1145/3397271.3401038\n[12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun\nZhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data\nAugmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\nUSA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n1012\u20131020. https://doi.org/10.1145/3543507.3583341\n[13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study\nfor Understanding Users\u2019 Attitudes Towards a Conversational Agent for News\nRecommendation. In Proceedings of the 4th Conference on Conversational User\nInterfaces (Glasgow, United Kingdom) (CUI \u201922). Association for Computing\nMachinery, New York, NY, USA, Article 36, 6 pages.\nhttps://doi.org/10.1145/\n3543829.3544530\n[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\ninstruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n[15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot\n782\nLarge Language Model Augmented Narrative Driven Recommendations\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nDense Retrieval From 8 Examples. In The Eleventh International Conference on\nLearning Representations. https://openreview.net/forum?id=gmL46YMpu2J\n[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.\nGoogle News Personalization: Scalable Online Collaborative Filtering. In Pro-\nceedings of the 16th International Conference on World Wide Web (Banff, Alberta,\nCanada) (WWW \u201907). Association for Computing Machinery, New York, NY, USA,\n271\u2013280. https://doi.org/10.1145/1242572.1242610\n[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,\nUllas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi\nSampath. 2010. The YouTube Video Recommendation System. In Proceedings of\nthe Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys\n\u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https:\n//doi.org/10.1145/1864708.1864770\n[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating\nNarrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th\nInternational Conference on Intelligent User Interfaces (Marina del Ray, California)\n(IUI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https:\n//doi.org/10.1145/3301275.3302287\n[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot\nDense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).\n[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context\nAware Recommendation. In Proceedings of the 7th ACM Conference on Recom-\nmender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing\nMachinery, New York, NY, USA, 9\u201316. https://doi.org/10.1145/2507157.2507187\n[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M\nVoorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In\nTREC.\n[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-\nmation Retrieval with Contrastive Learning. Transactions on Machine Learning\nResearch (2022). https://openreview.net/forum?id=jKN1pXi7b0\n[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,\nJakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as\nEfficient Dataset Generators for Information Retrieval. arXiv:2301.01820\n[24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo\nHuurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016.\nOverview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul-\ntilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa\nGon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato,\nand Nicola Ferro (Eds.). Springer International Publishing, Cham, 351\u2013370.\n[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski,\nFernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data\nfor Conversational Music Recommendation Using Random Walks and Language\nModels. arXiv:2301.11489\n[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of-\nInterest Recommendation by Mining Users\u2019 Preference Transition. In Proceedings\nof the 22nd ACM International Conference on Information & Knowledge Manage-\nment (San Francisco, California, USA) (CIKM \u201913). Association for Computing Ma-\nchinery, New York, NY, USA, 733\u2013738. https://doi.org/10.1145/2505515.2505639\n[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An\nExperimental Evaluation of Point-of-Interest Recommendation in Location-Based\nSocial Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010\u20131021. https://doi.\norg/10.14778/3115404.3115407\n[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and\nLucas Dixon. 2021. Augmenting the user-item graph with textual similarity\nmodels. arXiv preprint arXiv:2109.09358 (2021).\n[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue\nDataset: Retrieving Data Tables through Conversations with Genuine Intents. In\nProceedings of the 17th Conference of the European Chapter of the Association for\nComputational Linguistics. Association for Computational Linguistics, Dubrovnik,\nCroatia, 2799\u20132829. https://aclanthology.org/2023.eacl-main.206\n[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear\nCritiquing for Conversational Recommender Systems. In The Web Conference.\n[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot\nNeural Passage Retrieval via Domain-targeted Synthetic Question Generation.\nIn Proceedings of the 16th Conference of the European Chapter of the Associa-\ntion for Computational Linguistics: Main Volume. Association for Computational\nLinguistics, Online, 1075\u20131088. https://doi.org/10.18653/v1/2021.eacl-main.92\n[32] Sheshera Mysore, Tim O\u2019Gorman, Andrew McCallum, and Hamed Zamani. 2021.\nCSFCube - A Test Collection of Computer Science Research Articles for Faceted\nQuery by Example. In Thirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv.\n2103.12906\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with human feedback. In\nAdvances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\nInc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf\n[34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,\nand Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces\nThat Elicit Natural Language Queries. In Proceedings of the 2021 Conference on\nHuman Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR\n\u201921). Association for Computing Machinery, New York, NY, USA, 261\u2013265. https:\n//doi.org/10.1145/3406522.3446035\n[35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues\nBouchard. 2023. Improving Content Retrievability in Search with Controllable\nQuery Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\nUSA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n3182\u20133192. https://doi.org/10.1145/3543507.3583261\n[36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin.\n2022. On Natural Language User Profiles for Transparent and Scrutable Rec-\nommendation. In Proceedings of the 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval (Madrid, Spain) (SIGIR\n\u201922). Association for Computing Machinery, New York, NY, USA, 2863\u20132874.\nhttps://doi.org/10.1145/3477495.3531873\n[37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing. Association for Computational\nLinguistics. https://arxiv.org/abs/1908.10084\n[38] Stephen Robertson and Hugo Zaragoza. 2009.\nThe Probabilistic Relevance\nFramework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389.\nhttps://doi.org/10.1561/1500000019\n[39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin\nFranz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.\nUDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation\nof Rerankers. arXiv:2303.00807 [cs.IR]\n[40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\nYih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\nwith Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, 3781\u20133797. https://aclanthology.\norg/2022.emnlp-main.249\n[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked\nand Permuted Pre-training for Language Understanding. In Advances in Neural\nInformation Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf\n[42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via\nAutomated Analysis of Interests and Activities. In Proceedings of the 28th Annual\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (Salvador, Brazil) (SIGIR \u201905). Association for Computing Machinery,\nNew York, NY, USA, 449\u2013456. https://doi.org/10.1145/1076034.1076111\n[43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic\nBehavior Chains. In Proceedings of the 12th ACM Conference on Recommender\nSystems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association for\nComputing Machinery, New York, NY, USA, 86\u201394.\nhttps://doi.org/10.1145/\n3240323.3240369\n[44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.\nControllable Gradient Item Retrieval. In Web Conference.\n[45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\nand Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug-\nmentation. In Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). As-\nsociation for Computing Machinery, New York, NY, USA, 548\u2013556.\nhttps:\n//doi.org/10.1145/3292500.3330873\n[46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized\nRanking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM\nConference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association\nfor Computing Machinery, New York, NY, USA, 502\u2013505.\nhttps://doi.org/10.\n1145/3523227.3547394\n[47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng.\n2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users\nin Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\nUSA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n1396\u20131404. https://doi.org/10.1145/3543507.3583538\n[48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con-\nversational information seeking. arXiv preprint arXiv:2201.08808 (2022).\n[49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based\nRecommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval (Virtual Event, China)\n(SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 881\u2013890.\nhttps://doi.org/10.1145/3397271.3401180\n783\n",
    "pdf_url": "/media/Article_07.pdf",
    "references": [
      "[1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:",
      "A Test Collection for Narrative-Driven Point of Interest Recommendation. In",
      "Proceedings of the 44th International ACM SIGIR Conference on Research and",
      "Development in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As-",
      "sociation for Computing Machinery, New York, NY, USA, 2478\u20132484.",
      "https:",
      "//doi.org/10.1145/3404835.3463243",
      "[2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and",
      "Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in",
      "Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference",
      "on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/",
      "doi/10.1145/3406522.3446021",
      "[3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018."
    ],
    "publication_date": "2023-11-07T00:00:00",
    "corrected": 0,
    "_id": "_KLJa40B3PLCePsgEjvX"
  },
  {
    "titre": "Advances in Engineering Software Research paper SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical andmixed variables Gaussian processes",
    "resume": "The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogatemodeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a majornew release of SMT that introduces significant upgrades and new features to the toolbox. This release addsthe capability to handle mixed-variable surrogate models and hierarchical variables. These types of variablesare becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SM Tby extending sampling methods, adding new surrogate models, and computing variance and kernel derivativesfor Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best ofour knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchicaland mixed inputs. This open-source software is distributed under the New BSD license.2",
    "auteurs": [
      "Paul Saves",
      "b,\u2217,1",
      "R\u00e9mi Lafage a,1",
      "Nathalie Bartoli",
      "Youssef Diouane",
      "Jasper Bussemaker",
      "Thierry Lefebvre a,1",
      "John T. Hwang",
      "Joseph Morlier f,1",
      "Toulouse",
      "Kriging",
      "Kriging",
      "P. Saves",
      "Y. Diouane",
      "J. Bussemaker",
      "J.T. Hwang",
      "J. Morlier",
      "UQLab"
    ],
    "institutions": [
      "Research paper SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical andmixed variables Gaussian processes",
      "y ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of Michigan. Now, both Polytechnique Montr\u00e9al and the University ofCalifornia San Diego are also contributors. SMT 2.0 updates and ex-tends the original SMT repository capabilities among which the originalpublication [5] focuses on different types of derivatives for surrogatemodels detailed hereafter.",
      " ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, Franceb ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, Francec Polytechnique Montr\u00e9al, Montreal, QC, Canadad German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germanye University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, US Af ICA, Universit\u00e9 de Toulouse, ISAESUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, Franceg University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA",
      " Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France.E-mail addresses:  (P. Saves),  (R. Lafage),  (N. Bartoli), "
    ],
    "mots_cles": [
      ": Surrogate modeling "
    ],
    "texte_integral": "Advances in Engineering Software 188 (2024) 103571\nAvailable online 7 December 2023\n0965-9978/\u00a9 2023 Elsevier Ltd. All rights reserved.\nContents lists available at ScienceDirect\nAdvances in Engineering Software\njournal homepage: www.elsevier.com/locate/advengsoft\nResearch paper\nSMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and\nmixed variables Gaussian processes\nPaul Saves a,b,\u2217,1, R\u00e9mi Lafage a,1, Nathalie Bartoli a,1, Youssef Diouane c,1, Jasper Bussemaker d,1,\nThierry Lefebvre a,1, John T. Hwang e,1, Joseph Morlier f,1, Joaquim R.R.A. Martins g,1\na ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France\nb ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, France\nc Polytechnique Montr\u00e9al, Montreal, QC, Canada\nd German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany\ne University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA\nf ICA, Universit\u00e9 de Toulouse, ISAE\u2013SUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France\ng University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA\nA R T I C L E\nI N F O\nDataset link: https://colab.research.google.com\n/github/SMTorg/smt/blob/master/tutorial/No\ntebookRunTestCases_Paper_SMT_v2.ipynb\nKeywords:\nSurrogate modeling\nGaussian process\nKriging\nHierarchical problems\nHierarchical and mixed-categorical inputs\nMeta variables\nA B S T R A C T\nThe Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate\nmodeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major\nnew release of SMT that introduces significant upgrades and new features to the toolbox. This release adds\nthe capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables\nare becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT\nby extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives\nfor Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of\nour knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical\nand mixed inputs. This open-source software is distributed under the New BSD license.2\n1. Motivation and significance\nWith the increasing complexity and accuracy of numerical models, it\nhas become more challenging to run complex simulations and computer\ncodes [1,2]. As a consequence, surrogate models have been recognized\nas a key tool for engineering tasks such as design space exploration,\nuncertainty quantification, and optimization [3]. In practice, surrogate\nmodels are used to reduce the computational effort of these tasks by\nreplacing expensive numerical simulations with closed-form approxi-\nmations [4, Ch. 10]. To build such a model, we start by evaluating\nthe original expensive simulation at a set of points through a Design\nof Experiments (DoE). Then, the corresponding evaluations are used to\nbuild the surrogate model according to the chosen approximation, such\nas Kriging, quadratic interpolation, or least squares regression.\nThe Surrogate Modeling Toolbox (SMT) is an open-source frame-\nwork that provides functions to efficiently build surrogate models [5].\n\u2217 Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France.\nE-mail addresses: paul.saves@onera.fr (P. Saves), remi.lafage@onera.fr (R. Lafage), nathalie.bartoli@onera.fr (N. Bartoli), youssef.diouane@polymtl.ca\n(Y. Diouane), jasper.bussemaker@dlr.de (J. Bussemaker), thierry.lefebvre@onera.fr (T. Lefebvre), jhwang@eng.ucsd.edu (J.T. Hwang),\njoseph.morlier@isae-supaero.fr (J. Morlier), jrram@umich.edu (J.R.R.A. Martins).\n1 All authors contributed to this work, research and manuscript.\n2 https://github.com/SMTorg/SMT\nKriging models (also known as Gaussian processes) that take advantage\nof derivative information are one of SMT\u2019s key features [6]. Numerical\nexperiments have shown that SMT achieved lower prediction error\nand computational cost than Scikit-learn [7] and UQLab [8] for a\nfixed number of points [9]. SMT has been applied to rocket engine\ncoaxial-injector optimization [10], aircraft engine consumption mod-\neling [11], numerical integration [12], multi-fidelity sensitivity analy-\nsis [13], high-order robust finite elements methods [14,15], planning\nfor photovoltaic solar energy [16], wind turbines design optimiza-\ntion [17], porous material optimization for a high pressure turbine\nvane [18], chemical process design [19] and many other applications.\nIn systems engineering, architecture-level choices significantly in-\nfluence the final system performance, and therefore, it is desirable to\nconsider such choices in the early design phases [20]. Architectural\nchoices are parameterized with discrete design variables; examples in-\nclude the selection of technologies, materials, component connections,\nhttps://doi.org/10.1016/j.advengsoft.2023.103571\nReceived 22 August 2023; Received in revised form 23 October 2023; Accepted 26 November 2023\nAdvances in Engineering Software 188 (2024) 103571\n2\nP. Saves et al.\nTable 1\nComparison of software packages for hierarchical and mixed Kriging models. \u2713= implemented. * = user-defined.\nPackage\nBOTorch\nDakota\nDiceKriging\nKerGP\nLVGP\nParmoo\nSpearmint\nSMT 2.0\nReference\n[25]\n[26]\n[27]\n[32]\n[28]\n[29]\n[30]\nThis paper\nLicense\nMIT\nEPL\nGPL\nGPL\nGPL\nBSD\nGNU\nBSD\nLanguage\nPython\nC\nR\nR\nR\nPython\nPython\nPython\nMixed var.\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nGD kernel\n\u2713\n\u2713\n\u2713\n*\n\u2713\nCR kernel\n\u2713\n\u2713\n\u2713\n\u2713\nHH kernel\n\u2713\n\u2713\nEHH kernel\n*\n\u2713\nHierarchical var.\n\u2713\nand number of instantiated elements. When design problems include\nboth discrete variables and continuous variables, they are said to have\nmixed variables.\nWhen architectural choices lead to different sets of design variables,\nwe have hierarchical variables [21,22]. For example, consider differ-\nent aircraft propulsion architectures [23]. A conventional gas turbine\nwould not require a variable to represent a choice in the electrical\npower source, while hybrid or pure electric propulsion would require\nsuch a variable. The relationship between the choices and the sets of\nvariables can be represented by a hierarchy.\nHandling hierarchical and mixed variables requires specialized sur-\nrogate modeling techniques [24]. To address these needs, SMT 2.0\nis offering researchers and practitioners a collection of cutting-edge\ntools to build surrogate models with continuous, mixed and hierarchical\nvariables. The main objective of this paper is to detail the new enhance-\nments that have been added in this release compared to the original\nSMT 0.2 release [5].\nThere are two new major capabilities in SMT 2.0: the ability\nto build surrogate models involving mixed variables and the support\nfor hierarchical variables within Kriging models. To handle mixed\nvariables in Kriging models, existing libraries such as BoTorch [25],\nDakota [26], DiceKriging [27], LVGP [28], Parmoo [29], and Spearmint\n[30] implement simple mixed models by using either continuous relax-\nation (CR), also known as one-hot encoding [30], or a Gower distance\n(GD) based correlation kernel [31]. KerGP [32] (developed in R) imple-\nments more general kernels but there is no Python open-source toolbox\nthat implements more general kernels to deal with mixed variables,\nsuch as the homoscedastic hypersphere (HH) [33] and exponential\nhomoscedastic hypersphere (EHH) [34] kernels. Such kernels require\nthe tuning of a large number of hyperparameters but lead to more\naccurate Kriging surrogates than simpler mixed kernels [34,35]. SMT\n2.0 implements all these kernels (CR, GD, HH, and EHH) through a\nunified framework and implementation. To handle hierarchical vari-\nables, no library in the literature can build peculiar surrogate models\nexcept SMT 2.0, which implements two Kriging methods for these\nvariables. Notwithstanding, most softwares are compatible with a na\u00efve\nstrategy called the imputation method [24] but this method lacks depth\nand depends on arbitrary choices. This is why Hutter and Osborne\n[21] proposed a first kernel, called Arc-Kernel which in turn was\ngeneralized by Horn et al. [36] with a new kernel called the Wedge-\nKernel [37]. None of these kernels are available in any open-source\nmodeling software. Furthermore, thanks to the framework introduced\nin Audet et al. [38], our proposed kernels are sufficiently general so\nthat all existing hierarchical kernels are included within it. Section 4\ndescribes the two kernels implemented in SMT 2.0 that are referred\nas SMT Arc-Kernel and SMT Alg-Kernel. In particular, Alg-\nKernel is a novel hierarchical kernel introduced in this paper. Table 1\noutlines the main features of the state-of-the-art modeling software that\ncan handle hierarchical and mixed variables.\nSMT 2.0 introduces other enhancements, such as additional sam-\npling procedures, new surrogate models, new Kriging kernels (and their\nderivatives), Kriging variance derivatives, and an adaptive criterion for\nhigh-dimensional problems. SMT 2.0 adds applications of Bayesian\noptimization (BO) with hierarchical and mixed variables or noisy co-\nKriging that have been successfully applied to aircraft design [39], data\nfusion [40], and structural design [41]. The SMT 2.0 interface is more\nuser-friendly and offers an improved and more detailed documentation\nfor users and developers.3 SMT 2.0 is hosted publicly4 and can be\ndirectly imported within Python scripts. It is released under the New\nBSD License and runs on Linux, MacOS, and Windows operating sys-\ntems. Regression tests are run automatically for each operating system\nwhenever a change is committed to the repository. In short, SMT 2.0\nbuilds on the strengths of the original SMT package while adding new\nfeatures. On one hand, the emphasis on derivatives (including predic-\ntion, training and output derivatives) is maintained and improved in\nSMT 2.0. On the other hand, this new release includes support for\nhierarchical and mixed variables Kriging based models. For the sake\nof reproducibility, an open-source notebook is available that gathers\nall the methods and results presented on this paper.5\nThe remainder of the paper is organized as follows. First, we in-\ntroduce the organization and the main implemented features of the\nrelease in Section 2. Then, we describe the mixed-variable Kriging\nmodel with an example in Section 3. Similarly, we describe and provide\nan example for a hierarchical-variable Kriging model in Section 4.\nThe Bayesian optimization models and applications are described in\nSection 5. Finally, we describe the other relevant contributions in\nSection 6 and conclude in Section 7.\n2. SMT 2.0 : an improved surrogate modeling toolbox\nFrom a software point of view, SMT 2.0 maintains and improves\nthe modularity and generality of the original SMT version [5]. In this\nsection, we describe the software as follows. Section 2.1 describes the\nlegacy of SMT 0.2. Then, Section 2.2 describes the organization of the\nrepository. Finally, Section 2.3 shows the new capabilities implemented\nin the SMT 2.0 update.\n2.1. Background on SMT former version: SMT 0.2\nSMT [5] is an open-source collaborative work originally developed\nby ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\nMichigan. Now, both Polytechnique Montr\u00e9al and the University of\nCalifornia San Diego are also contributors. SMT 2.0 updates and ex-\ntends the original SMT repository capabilities among which the original\npublication [5] focuses on different types of derivatives for surrogate\nmodels detailed hereafter.\n3 http://smt.readthedocs.io/en/latest\n4 https://github.com/SMTorg/smt\n5 https://github.com/SMTorg/smt/tree/master/tutorial/\nNotebookRunTestCases_Paper_SMT_v2.ipynb\nAdvances in Engineering Software 188 (2024) 103571\n3\nP. Saves et al.\nTable 2\nImpact of using Numba on training time of the hierarchical Goldstein problem. Speedup\nis calculated excluding the JIT compilation table, as this step is only needed once after\nSMT installation.\nTraining set\nWithout numba\nNumba\nSpeedup\nJIT overhead\n15 points\n1.3 s\n1.1 s\n15%\n24 s\n150 points\n38 s\n7.4 s\n80%\n23 s\nA Python surrogate modeling framework with derivatives. One of the\noriginal main motivations for SMT was derivative support. In fact, none\nof the existing packages for surrogate modeling such as Scikit-learn in\nPython [7], SUMO in Matlab [42] or GPML in Matlab and Octave [43]\nfocuses on derivatives. Three types of derivatives are distinguished:\nprediction derivatives, training derivatives, and output derivatives.\nSMT also includes new models with derivatives such as Kriging with\nPartial Least Squares (KPLS) [44] and Regularized Minimal-energy\nTensor-product Spline (RMTS) [3]. These developed derivatives were\neven used in a novel algorithm called Gradient-Enhanced Kriging with\nPartial Least Squares (GEKPLS) [6] to use with adjoint methods, for\nexample [45].\nSoftware architecture, documentation, and automatic testing. SMT is orga-\nnized along three main sub-modules that implement a set of sampling\ntechniques (sampling_methods), benchmarking functions (problems),\nand surrogate modeling techniques (surrogate_models). The toolbox\ndocumentation6 is created using reStructuredText and Sphinx, a doc-\numentation generation package for Python, with custom extensions.\nCode snippets in the documentation pages are taken directly from\nactual tests in the source code and are automatically updated. The\noutput from these code snippets and tables of options are generated\ndynamically by custom Sphinx extensions. This leads to high-quality\ndocumentation with minimal effort. Along with user documentation,\ndeveloper documentation is also provided to explain how to contribute\nto SMT. This includes a list of API methods for the SurrogateModel,\nSamplingMethod, and Problem classes, that must be implemented\nto create a new surrogate modeling method, sampling technique, or\nbenchmarking problem. When a developer submits a pull request, it is\nmerged only after passing the automated tests and receiving approval\nfrom at least one reviewer. The repository on GitHub7 is linked to\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\nMacOS, to a coverage test on coveralls.io and to a dependency version\ncheck for Python with DependaBot. Various parts of the source code\nhave been accelerated using Numba [46], an LLVM-based just-in-time\n(JIT) compiler for numpy-heavy Python code. Numba is applied to con-\nventional Python code using function decorators, thereby minimizing\nits impact on the development process and not requiring an additional\nbuild step. For a mixed Kriging surrogate with 150 training points, a\nspeedup of up to 80% is observed, see Table 2. The JIT compilation\nstep only needs to be done once when installing or upgrading SMT\nand adds an overhead of approximately 24 s on a typical workstation\nIn this paper, all results are obtained using an Intel\u00ae Xeon\u00ae CPU\nE5-2650 v4 @ 2.20 GHz core and 128 GB of memory with a Broadwell-\ngeneration processor front-end and a compute node of a peak power of\n844 GFlops.\n2.2. Organization of SMT 2.0\nThe main features of the open-source repository SMT 2.0 are\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems\nand Surrogate models are kept from SMT 0.2 and two new\nsections Models applications and Interactive notebooks\nhave been added to the architecture of the code. These sections are\n6 https://smt.readthedocs.org\n7 https://github.com/SMTorg/smt\nhighlighted in blue and detailed on Fig. 1. The new major features\nimplemented in SMT 2.0 are highlighted in lavender whereas the\nlegacy features that were already in present in the original publication\nfor SMT 0.2 [5] are in black.\n2.3. New features within SMT 2.0\nThe main objective of this new release is to enable Kriging surrogate\nmodels for use with both hierarchical and mixed variables. Moreover,\nfor each of these five sub-modules described in Section 2.2, several\nimprovements have been made between the original version and the\nSMT 2.0 release.\nHierarchical and mixed design space. A new design space definition\nclass DesignSpace has been added that implements hierarchical\nand mixed functionalities. Design variables can either be continu-\nous (FloatVariable), ordered (OrdinalVariable) or categorical\n(CategoricalVariable). The integer type (IntegerVariable) rep-\nresents a special case of the ordered variable, specified by bounds\n(inclusive) rather than a list of possible values. The hierarchical struc-\nture of the design space can be defined using declare_decreed_var:\nthis function declares that a variable is a decreed variable that is\nactivated when the associated meta variable takes one of a set of\nspecified values, see Section 4 for background. The DesignSpace\nclass also implements mechanisms for sampling valid design vectors\n(i.e. design vectors that adhere to the hierarchical structure of the\ndesign space) using any of the below-mentioned samplers, for cor-\nrecting and imputing design vectors, and for requesting which design\nvariables are acting in a given design vector. Correction ensures that\nvariables have valid values (e.g. integers for discrete variables) [24],\nand imputation replaces non-acting variables by some default value\n(0 for discrete variables, mid-way between the bounds for continuous\nvariables in SMT 2.0) [47].\nSampling. SMT implements three methods for sampling. The first one\nis a na\u00efve approach, called Random that draws uniformly points along\nevery dimension. The second sampling method is called Full Fac-\ntorial and draws a point for every cross combination of variables,\nto have an \u2018\u2018exhaustive\u2019\u2019 design of experiments. The last one is the\nLatin Hypercube Sampling (LHS) [48] that draws a point in\nevery Latin square parameterized by a certain criterion. For LHS, a\nnew criterion to manage the randomness has been implemented and\nthe sampling method was adapted for multi-fidelity and mixed or\nhierarchical variables. More details about the new sampling techniques\nare given in Section 6.1.\nProblems. SMT implements two new engineering problems: a mixed\nvariant of a cantilever beam described in Section 3 and a hierarchical\nneural network described in Section 4.\nSurrogate models. In order to keep up with state-of-art, several re-\nleases done from the original version developed new options for the\nalready existing surrogates. In particular, compared to the original\npublication [5], SMT 2.0 adds gradient-enhanced neural networks [45]\nand marginal Gaussian process [49] models to the list of available\nsurrogates. More details about the new models are given in Section 6.2.\nApplications. Several applications have been added to the toolbox to\ndemonstrate the surrogate models capabilities. The most relevant ap-\nplication is efficient global optimization (EGO), a Bayesian optimiza-\ntion algorithm [50,51]. EGO optimizes expensive-to-evaluate black-box\nproblems with a chosen surrogate model and a chosen optimization\ncriterion [52]. The usage of EGO with hierarchical and mixed variables\nis described in Section 5.\nAdvances in Engineering Software 188 (2024) 103571\n4\nP. Saves et al.\nFig. 1. Functionalities of SMT 2.0. The new major features implemented in SMT 2.0 compared to SMT 0.2 are highlighted with the lavender color.\nInteractive notebooks. These tutorials introduce and explain how to use\nthe toolbox for different surrogate models and applications.8 Every\ntutorial is available both as a .ipynb file and directly on Google\ncolab.9 In particular, a hierarchical and mixed variables dedicated\nnotebook is available to reproduce the results presented on this paper.10\nIn the following, Section 3 details the Kriging based surrogate\nmodels for mixed variables, and Section 4 presents our new Kriging\nsurrogate for hierarchical variables. Section 5 details the EGO applica-\ntion and the other new relevant features aforementioned are described\nsuccinctly in Section 6.\n8 https://github.com/SMTorg/smt/tree/master/tutorial\n9 https://colab.research.google.com/github/SMTorg/smt/\n10 https://github.com/SMTorg/smt/tree/master/tutorial/\nNotebookRunTestCases_Paper_SMT_v2.ipynb\n3. Surrogate models with mixed variables in SMT 2.0\nAs mentioned in Section 1, design variables can be either of continu-\nous or discrete type, and a problem with both types is a mixed-variable\nproblem. Discrete variables can be ordinal or categorical. A discrete\nvariable is ordinal if there is an order relation within the set of possible\nvalues. An example of an ordinal design variable is the number of\nengines in an aircraft. A possible set of values in this case could be\n2, 4, 8. A discrete variable is categorical if no order relation is known\nbetween the possible choices the variable can take. One example of a\ncategorical variable is the color of a surface. A possible example of a\nset of choices could be blue, red, green. The possible choices are called\nthe levels of the variable.\nSeveral methods have been proposed to address the recent increase\ninterest in mixed Kriging based models [30\u201333,35,39,53,54]. The main\ndifference from a continuous Kriging model is in the estimation of\nAdvances in Engineering Software 188 (2024) 103571\n5\nP. Saves et al.\nTable 3\nCategorical kernels implemented in SMT 2.0.\nName\n\ud835\udf05(\ud835\udf19)\n\ud835\udef7(\ud835\udee9\ud835\udc56)\n# of hyperparam.\nSMT GD\nexp(\u2212\ud835\udf19)\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 1\n2 \ud835\udf03\ud835\udc56 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= 0\n1\nSMT CR\nexp(\u2212\ud835\udf19)\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= [\ud835\udee9\ud835\udc56]\ud835\udc57,\ud835\udc57 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= 0\n\ud835\udc3f\ud835\udc56\nSMT EHH\nexp(\u2212\ud835\udf19)\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 0 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= log \ud835\udf16\n2 ([\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)\u22a4]\ud835\udc57,\ud835\udc57\u2032 \u2212 1)\n1\n2 (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 \u2212 1)\nSMT HH\n\ud835\udf19\n[\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 1 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= [\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)\u22a4]\ud835\udc57,\ud835\udc57\u2032\n1\n2 (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 \u2212 1)\nthe categorical correlation matrix, which is critical to determine the\nmean and variance predictions. As mentioned in Section 1, approaches\nsuch as CR [30,39], continuous latent variables [54], and GD [31]\nuse a kernel-based method to estimate the correlation matrix. Other\nmethods estimate the correlation matrix by modeling the correlation\nentries directly [32,35,53], such as HH [33] and EHH [34]. The HH\ncorrelation kernel is of particular interest because it generalizes simpler\nkernels such as CR and GD [34]. In SMT 2.0, the correlation kernel is\nan option that can be set to either CR (CONT_RELAX_KERNEL), GD\n(GOWER_KERNEL),\nHH\n(HOMO_HSPHERE_KERNEL)\nor\nEHH\n(EXP_HOMO_HSPHERE_KERNEL).\n3.1. Mixed Gaussian processes\nThe continuous and ordinal variables are both treated similarly\nin SMT 2.0 with a continuous kernel, where the ordinal values are\nconverted to continuous through relaxation. For categorical variables,\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\nspecified by the API. This is why we developed a unified mathematical\nformulation that allows a unique implementation for any model.\nDenote \ud835\udc59 the number of categorical variables. For a given \ud835\udc56 \u2208\n{1, \u2026 , \ud835\udc59}, the \ud835\udc56th categorical variable is denoted \ud835\udc50\ud835\udc56 and its number\nof levels is denoted \ud835\udc3f\ud835\udc56. The hyperparameter matrix peculiar to this\nvariable \ud835\udc50\ud835\udc56 is\n\ud835\udee9\ud835\udc56 =\n\u23a1\n\u23a2\n\u23a2\n\u23a2\n\u23a2\u23a3\n[\ud835\udee9\ud835\udc56]1,1\n\ud835\udc7a\ud835\udc9a\ud835\udc8e.\n[\ud835\udee9\ud835\udc56]1,2\n[\ud835\udee9\ud835\udc56]2,2\n\u22ee\n\u22f1\n\u22f1\n[\ud835\udee9\ud835\udc56]1,\ud835\udc3f\ud835\udc56\n\u2026\n[\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56\u22121,\ud835\udc3f\ud835\udc56\n[\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56,\ud835\udc3f\ud835\udc56\n\u23a4\n\u23a5\n\u23a5\n\u23a5\n\u23a5\u23a6\n,\nand the categorical parameters are defined as \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61 = {\ud835\udee91, \u2026 , \ud835\udee9\ud835\udc59}. For\ntwo given inputs in the DoE, for example, the \ud835\udc5fth and \ud835\udc60th points, let\n\ud835\udc50\ud835\udc5f\n\ud835\udc56 and \ud835\udc50\ud835\udc60\n\ud835\udc56 be the associated categorical variables taking respectively\nthe \ud835\udcc1\ud835\udc56\n\ud835\udc5f and the \ud835\udcc1\ud835\udc56\n\ud835\udc60 level on the categorical variable \ud835\udc50\ud835\udc56. The categorical\ncorrelation kernel is defined by\n\ud835\udc58\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc50\ud835\udc5f, \ud835\udc50\ud835\udc60, \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61) =\n\ud835\udc59\u220f\n\ud835\udc56=1\n\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f\n\ud835\udc56 ,\ud835\udcc1\ud835\udc60\n\ud835\udc56 ) \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60\n\ud835\udc56 ,\ud835\udcc1\ud835\udc5f\n\ud835\udc56 )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f\n\ud835\udc56 ,\ud835\udcc1\ud835\udc5f\n\ud835\udc56 )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60\n\ud835\udc56 ,\ud835\udcc1\ud835\udc60\n\ud835\udc56 )\n(1)\nwhere \ud835\udf05 is either a positive definite kernel or identity and \ud835\udef7(.) is a\nsymmetric positive definite (SPD) function such that the matrix \ud835\udef7(\ud835\udee9\ud835\udc56)\nis SPD if \ud835\udee9\ud835\udc56 is SPD. For an exponential kernel, Table 3 gives the\nparameterizations of \ud835\udef7 and \ud835\udf05 that correspond to GD, CR, HH, and\nEHH kernels. The complexity of these different kernels depends on\nthe number of hyperparameters that characterizes them. As defined\nby Saves et al. [34], for every categorical variable \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc59}, the\nmatrix \ud835\udc36(\ud835\udee9\ud835\udc56) \u2208 R\ud835\udc3f\ud835\udc56\u00d7\ud835\udc3f\ud835\udc56 is lower triangular and built using a hypersphere\ndecomposition [55,56] from the symmetric matrix \ud835\udee9\ud835\udc56 \u2208 R\ud835\udc3f\ud835\udc56\u00d7\ud835\udc3f\ud835\udc56 of\nhyperparameters. The variable \ud835\udf16 is a small positive constant and the\nvariable \ud835\udf03\ud835\udc56 denotes the only positive hyperparameter that is used for\nthe Gower distance kernel.\nAnother Kriging based model that can use mixed variables is Kriging\nwith partial least squares (KPLS) [57]. KPLS adapts Kriging to high\ndimensional problems by using a reduced number of hyperparameters\nthanks to a projection into a smaller space. Also, for a general surrogate,\nnot necessarily Kriging, SMT 2.0 uses continuous relaxation to allow\nwhatever model to handle mixed variables. For example, we can use\nmixed variables with least squares (LS) or quadratic polynomial (QP)\nmodels. We now illustrate the abilities of the toolbox in terms of mixed\nmodeling over an engineering test case.\nTable 4\nResults of the cantilever beam models [34, Table 4].\nCategorical\nkernel\nDisplacement\nerror (cm)\nLikelihood\n# of\nhyperparam.\nSMT GD\n1.3861\n111.13\n3\nSMT CR\n1.1671\n155.32\n14\nSMT EHH\n0.1613\n236.25\n68\nSMT HH\n0.2033\n235.66\n68\n3.2. An engineering design test-case\nA classic engineering problem commonly used for model validation\nis the beam bending problem [32,58]. This problem is illustrated\non Fig. 2(a) and consists of a cantilever beam in its linear range loaded\nat its free end with a force \ud835\udc39. As in Cheng et al. [58], the Young\nmodulus is \ud835\udc38 = 200 GPa and the chosen load is \ud835\udc39 = 50 kN. Also, as\nin Roustant et al. [32], 12 possible cross-sections can be used. These\n12 sections consist of 4 possible shapes that can be either hollow, thick\nor full as illustrated in Fig. 2(b).\nTo compare the mixed Kriging models of SMT 2.0, we draw a 98\npoint LHS as training set and the validation set is a grid of 12 \u00d7 30 \u00d7\n30 = 10800 points. For the four implemented methods, displacement\nerror (computed with a root-mean-square error criterion), likelihood,\nnumber of hyperparameters and computational time for every model\nare shown in Table 4. For the continuous variables, we use the square\nexponential kernel. More details are found in [34]. As expected, the\ncomplex EHH and HH models lead to a lower displacement error and a\nhigher likelihood value, but use more hyperparameters and increase\nthe computational cost compared to GD and CR. On this test case,\nthe kernel EHH is easier to optimize than HH but in general, they\nare similar in terms of performance. Also, by default SMT 2.0 uses\nCR as it is known to be a good trade-off between complexity and\nperformance [59].\n4. Surrogate models with hierarchical variables in SMT 2.0\nTo introduce the newly developed Kriging model for hierarchical\nvariables implemented in SMT 2.0, we present the general mathe-\nmatical framework for hierarchical and mixed variables established\nby Audet et al. [38]. In SMT 2.0, two variants of our new method\nare implemented, namely SMT Alg-Kernel and SMT Arc-Kernel.\nIn particular, the SMT Alg-Kernel is a novel correlation kernel\nintroduced in this paper.\n4.1. The hierarchical variables framework\nA problem structure is classified as hierarchical when the sets of\nactive variables depend on architectural choices. This occurs frequently\nin industrial design problems. In hierarchical problems, we can classify\nvariables as neutral, meta (also known as dimensional) or decreed\n(also known as conditionally active) as detailed in Audet et al. [38].\nNeutral variables are the variables that are not affected by the hierarchy\nwhereas the value assigned to meta variables determines which decreed\nvariables are activated. For example, a meta variable could be the\nnumber of engines. If the number of engines changes, the number of\ndecreed bypass ratios that every engine should specify also changes.\nAdvances in Engineering Software 188 (2024) 103571\n6\nP. Saves et al.\nFig. 2. Cantilever beam problem [34, Figure 6].\nFig. 3. Variables classification as used in SMT 2.0.\nHowever, the wing aspect ratio being neutral, it is not affected by this\nhierarchy.\nProblems involving hierarchical variables are generally dependant\non discrete architectures and as such involve mixed variables. Hence,\nin addition to their role (neutral, meta or decreed), each variable also\nhas a variable type amongst categorical, ordinal or continuous. For the\nsake of simplicity and because both continuous and ordinal variables\nare treated similarly [34], we chose to regroup them as quantitative\nvariables. For instance, the neutral variables \ud835\udc65neu may be partitioned\ninto different variable types, such that \ud835\udc65neu = (\ud835\udc65cat\nneu, \ud835\udc65qnt\nneu) where \ud835\udc65cat\nneu\nrepresents the categorical variables and \ud835\udc65qnt\nneu are the quantitative ones.\nThe variable classification scheme in SMT 2.0 is detailed in Fig. 3.\nTo explain the framework and the new Kriging model, we illustrate\nthe inputs variables of the model using a classical machine learn-\ning problem related to the hyperparameters optimization of a fully-\nconnected Multi-Layer Perceptron (MLP) [38] on Fig. 4. In\nTable 5,\nwe detail the input variables of the model related to the MLP problem\n(i.e., the hyperparameters of the neural network, together with their\ntypes and roles). To keep things clear and concise, the chosen problem\nis a simplification of the original problem developed by Audet et al.\n[38]. Regarding the MLP problem of Fig. 4 and following the classi-\nfication scheme of Fig. 3, we start by separating the input variables\naccording to their role. In fact,\n1. changing the number of hidden layers modifies the number\nof inputs variables. Therefore, \u2018\u2018# of hidden layers\u2019\u2019 is a meta\nvariable.\n2. The number of neurons in the hidden layer number \ud835\udc58 is either\nincluded or excluded. For example, the \u2018\u2018# of neurons in the 3rd\nlayer\u2019\u2019 would be excluded for an input that only has 2 hidden\nlayers. Therefore, \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are decreed\nvariables.\n3. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and\n\u2018\u2018Batch size\u2019\u2019 are not affected by the hierarchy choice. Therefore,\nthey are neutral variables.\nAccording to their types, the MLP input variables can be classified as\nfollows:\n4. The meta variable \u2018\u2018# of hidden layers\u2019\u2019 is an integer and, as\nsuch, is represented by the component \ud835\udc65qnt\nmet.\n5. The decreed variables \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are integers\nand, as such, are represented by the component \ud835\udc65qnt\ndec.\n6. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and\n\u2018\u2018Batch size\u2019\u2019 are, respectively, continuous, for the first two (ev-\nery value between two bounds), categorical (qualitative between\nthree choices) and integer (quantitative between 6 choices).\nTherefore, the \u2018\u2018Activation function\u2019\u2019 and the \u2018\u2018Momentum\u2019\u2019 are\nrepresented by the component \ud835\udc65cat\nneu. The \u2018\u2018Learning rate\u2019\u2019 and the\n\u2018\u2018Batch size\u2019\u2019 are represented by the component \ud835\udc65qnt\nneu.\nTo model hierarchical variables, as proposed in [38], we separate\nthe input space \ue244 as (\ue244neu, \ue244met, \ue244dec) where \ue244dec =\n\u22c3\n\ud835\udc65met\u2208\ue244met\n\ue244inc(\ud835\udc65met).\nAdvances in Engineering Software 188 (2024) 103571\n7\nP. Saves et al.\nFig. 4. The Multi-Layer Perceptron (MLP) problem.\nSource: Figure adapted from [38, Figure 1].\nTable 5\nA detailed description of the variables in the MLP problem.\nMLP\nHyperparameters\nVariable\nDomain\nType\nRole\nLearning rate\n\ud835\udc5f\n[10\u22125, 10\u22122]\nFLOAT\nNEUTRAL\nMomentum\n\ud835\udefc\n[0, 1]\nFLOAT\nNEUTRAL\nActivation\nfunction\n\ud835\udc4e\n{\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48, \ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51, \ud835\udc47 \ud835\udc4e\ud835\udc5b\u210e}\nENUM\nNEUTRAL\nBatch size\n\ud835\udc4f\n{8, 16, \u2026 , 128, 256}\nORD\nNEUTRAL\n# of hidden\nlayers\n\ud835\udc59\n{1, 2, 3}\nORD\nMETA\n# of neurons\nhidden layer \ud835\udc58\n\ud835\udc5b\ud835\udc58\n{50, 51, \u2026 , 55}\nORD\nDECREED\nHence, for a given point \ud835\udc65 \u2208 \ue244, one has \ud835\udc65 = (\ud835\udc65neu, \ud835\udc65met, \ud835\udc65inc(\ud835\udc65met)), where\n\ud835\udc65neu \u2208 \ue244neu, \ud835\udc65met \u2208 \ue244met and \ud835\udc65inc(\ud835\udc62met) \u2208 \ue244inc(\ud835\udc62met) are defined as follows:\n\u2022 The components \ud835\udc65neu \u2208 \ue244neu gather all neutral variables that\nare not impacted by the meta variables but needed. For ex-\nample, in the MLP problem, \ue244neu gathers the possible learning\nrates, momentum, activation functions and batch sizes. Namely,\nfrom Table 5, \ue244neu = [10\u22125, 10\u22122]\u00d7[0, 1] \u00d7 {ReLu, Sigmoid, Tanh}\u00d7\n{8, 16, \u2026 , 256}.\n\u2022 The components \ud835\udc65met gather the meta (also known as dimen-\nsional) variables that determine the inclusion or exclusion of\nother variables. For example, in the MLP problem, \ue244met represents\nthe possible numbers of layers in the MLP. Namely, from Table 5,\n\ue244met = {1, 2, 3}.\n\u2022 The components \ud835\udc65inc(\ud835\udc65met), contain the decreed variables whose\ninclusion (decreed-included) or exclusion (decreed-excluded) is\ndetermined by the values of the meta components \ud835\udc65met. For exam-\nple, in the MLP problem, \ue244dec represents the number of neurons\nin the decreed layers. Namely, from Table 5, \ue244inc(\ud835\udc65met = 3) =\n[50, 55]3, \ue244inc(\ud835\udc65met = 2) = [50, 55]2 and \ue244inc(\ud835\udc65met = 1) = [50, 55].\n4.2. A Kriging model for hierarchical variables\nIn this section, a new method to build a Kriging model with hierar-\nchical variables is introduced based on the framework aforementioned.\nThe proposed methods are included in SMT 2.0.\n4.2.1. Motivation and state-of-the-art\nAssuming that the decreed variables are quantitative, Hutter and\nOsborne [21] proposed several kernels for the hierarchical context. A\nclassic approach, called the imputation method (Imp-Kernel) leads\nto an efficient paradigm in practice that can be easily integrated into\na more general framework as proposed by Bussemaker et al. [24].\nHowever, this kernel lacks depth and depends on arbitrary choices.\nTherefore, Hutter and Osborne [21] also proposed a more general\nkernel, called Arc-Kernel and Horn et al. [36] generalized this\nkernel even more and proposed a new formulation called the Wedge-\nKernel [37]. The drawbacks of these two methods are that they add\nsome extra hyperparameters for every decreed dimension (respectively\none extra hyperparameter for the Arc-Kernel and two hyperparam-\neters for the Wedge-Kernel) and that they need a normalization\naccording to the bounds. More recently, Pelamatti et al. [60] developed\na hierarchical kernel for Bayesian optimization. However, our work\nis also more general thanks to the framework introduced earlier [38]\nthat considers variable-wise formulation and is more flexible as we are\nallowing sub-problems to be intersecting.\nIn the following, we describe our new method to build a correlation\nkernel for hierarchical variables. In particular, we introduce a new alge-\nbraic kernel called Alg-Kernel that behaves like the Arc-Kernel\nwhilst correcting most of its drawbacks. In particular, our kernel does\nnot add any hyperparameters, and the normalization is handled in a\nnatural way.\n4.2.2. A new hierarchical correlation kernel\nFor modeling purposes, we assume that the decreed space is quan-\ntitative, i.e., \ue244dec = \ue244qnt\ndec. Let \ud835\udc62 \u2208 \ue244 be an input point partitioned as\nAdvances in Engineering Software 188 (2024) 103571\n8\nP. Saves et al.\n\ud835\udc62 = (\ud835\udc62neu, \ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)) and, similarly, \ud835\udc63 \u2208 \ue244 is another input such\nthat \ud835\udc63 = (\ud835\udc63neu, \ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)). The new kernel \ud835\udc58 that we propose for\nhierarchical variables is given by\n\ud835\udc58(\ud835\udc62, \ud835\udc63) = \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) \u00d7 \ud835\udc58met(\ud835\udc62met, \ud835\udc63met)\n\u00d7 \ud835\udc58met,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)]),\n(2)\nwhere \ud835\udc58neu, \ud835\udc58met and \ud835\udc58met,dec are as follows:\n\u2022 \ud835\udc58neu represents the neutral kernel that encompasses both categor-\nical and quantitative neutral variables, i.e., \ud835\udc58neu can be decom-\nposed into two parts \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat(\ud835\udc62cat\nneu, \ud835\udc63cat\nneu)\ud835\udc58qnt(\ud835\udc62qnt\nneu, \ud835\udc63qnt\nneu).\nThe categorical kernel, denoted \ud835\udc58cat, could be any Symmetric\nPositive Definite (SPD) [34] mixed kernel (see Section 3). For\nthe quantitative (integer or continuous) variables, a distance-\nbased kernel is used. The chosen quantitative kernel (Exponential,\nMat\u00e9rn, ...), always depends on a given distance \ud835\udc51. For example,\nthe \ud835\udc5b-dimensional exponential kernel gives\n\ud835\udc58qnt(\ud835\udc62qnt, \ud835\udc63qnt) =\n\ud835\udc5b\n\u220f\n\ud835\udc56=1\nexp(\u2212\ud835\udc51(\ud835\udc62qnt\n\ud835\udc56\n, \ud835\udc63qnt\n\ud835\udc56\n)).\n(3)\n\u2022 \ud835\udc58met is the meta variables related kernel. It is also separated into\ntwo parts: \ud835\udc58met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc58cat(\ud835\udc62cat\nmet, \ud835\udc63cat\nmet)\ud835\udc58qnt(\ud835\udc62qnt\nmet, \ud835\udc63qnt\nmet) where the\nquantitative kernel is ordered and not continuous because meta\nvariables take value in a finite set.\n\u2022 \ud835\udc58met,dec is an SPD kernel that models the correlations between the\nmeta levels (all the possible subspaces) and the decreed variables.\nIn what comes next, we detailed this kernel.\n4.2.3. Towards an algebraic meta-decreed kernel\nMeta-decreed\nkernels\nlike\nthe\nimputation\nkernel\nor\nthe\nArc-Kernel were first proposed in [21,47] and the distance-based\nkernels such as Arc-Kernel or Wedge-Kernel [37] were proven\nto be SPD. Nevertheless, to guarantee this SPD property, the same\nhyperparameters are used to model the correlations between the meta\nlevels and between the decreed variables [47]. For this reason, the\nArc-Kernel includes additional continuous hyperparameters which\nmakes the training of the GP models more expensive and introduces\nmore numerical stability issues. In this context, we are proposing a\nnew algebraic meta-decreed kernel (denoted as Alg-Kernel) that\nenjoys similar properties as Arc-Kernel but without using additional\ncontinuous hyperparameters nor rescaling. In the SMT 2.0 release, we\nincluded Alg-Kernel and a simpler version of Arc-Kernel that do\nnot relies on additional hyperparameters.\nOur proposed Alg-Kernel kernel is given by\n\ud835\udc58alg\nmet,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)])\n= \ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) \u00d7 \ud835\udc58alg\ndec(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)).\n(4)\nMathematically, we could consider that there is only one meta variable\nwhose levels correspond to every possible included subspace. Let \ud835\udc3csub\ndenotes the components indices of possible subspaces, the subspaces\nparameterized by the meta component \ud835\udc62met are defined as \ue244inc(\ud835\udc62met =\n\ud835\udc59), \ud835\udc59 \u2208 \ud835\udc3csub. It follows that the fully extended continuous decreed\nspace writes as \ue244dec = \u22c3\n\ud835\udc59\u2208\ud835\udc3csub \ue244inc(\ud835\udc62met = \ud835\udc59) and \ud835\udc3cdec is the set of the\nassociated indices. Let \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63\ndenotes the set of components related to\nthe space \ue244inc(\ud835\udc62met, \ud835\udc63met) containing the variables decreed-included in\nboth \ue244inc(\ud835\udc62met) and \ue244inc(\ud835\udc63met).\nSince the decreed variables are quantitative, one has\n\ud835\udc58alg\ndec(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)) = \ud835\udc58qnt(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met))\n=\n\u220f\n\ud835\udc56\u2208\ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63\n\ud835\udc58qnt([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56)\n(5)\nThe construction of the quantitative kernel \ud835\udc58qnt depends on a given\ndistance denoted \ud835\udc51alg. The kernel \ud835\udc58alg\nmet is an induced meta kernel that\ndepends on the same distance \ud835\udc51alg to preserve the SPD property of\n\ud835\udc58alg\nmet,dec. For every \ud835\udc56 \u2208 \ud835\udc3cdec, if \ud835\udc56 \u2208 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63 , the new algebraic distance is\ngiven by\n\ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) =\n\u239b\n\u239c\n\u239c\n\u239c\u239d\n2|[\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56 \u2212 [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56|\n\u221a\n[\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56\n2 + 1\n\u221a\n[\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56\n2 + 1\n\u239e\n\u239f\n\u239f\n\u239f\u23a0\n\ud835\udf03\ud835\udc56,\n(6)\nwhere \ud835\udf03\ud835\udc56 \u2208 R+ is a continuous hyperparameter. Otherwise, if \ud835\udc56 \u2208 \ud835\udc3cdec\nbut \ud835\udc56 \u2209 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63 , there should be a non-zero residual distance between the\ntwo different subspaces \ue244inc(\ud835\udc62met) and \ue244inc(\ud835\udc63met) to ensure the kernel\nSPD property. To have a residual not depending on the decreed values,\nour model considers that there is a unit distance\n\ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) = 1.0 \ud835\udf03\ud835\udc56, \u2200\ud835\udc56 \u2208 \ud835\udc3cdec \u29f5 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n\ud835\udc62,\ud835\udc63 .\nThe induced meta kernel \ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) to preserve the SPD property\nof \ud835\udc58alg is defined as:\n\ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) =\n\u220f\n\ud835\udc56\u2208\ud835\udc3cmet\n\ud835\udc58qnt(1.0 \ud835\udf03\ud835\udc56).\n(7)\nNot only our kernel of Eq. (2) uses less hyperparameters than the Arc-\nKernel (as we cut off its extra parameters) but it is also a more flexible\nkernel as it allows different kernels for meta and decreed variables.\nMoreover, another advantage of our kernel is that it is numerically\nmore stable thanks to the new non-stationary [61] algebraic distance\ndefined in Eq. (7) [62]. Our proposed distance also does not need any\nrescaling by the bounds to have values between 0 and 1.\nIn what comes next, we will refer to the implementation of the\nkernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\nSMT Alg-Kernel. We note also that the implementation of SMT\nArc-Kernel differs slightly from the original Arc-Kernel as we\nfixed some hyperparameters to 1 in order to avoid adding extra hy-\nperparameters and use the formulation of Eq. (2) and rescaling of the\ndata.\n4.2.4. Illustration on the MLP problem\nIn this section, we illustrate the hierarchical Arc-Kernel on the\nMLP example. For that sake, we consider two design variables \ud835\udc62 and\n\ud835\udc63 such that \ud835\udc62 = (2.10\u22124, 0.9, ReLU, 16, 2, 55, 51) and \ud835\udc63 = (5.10\u22123, 0.8,\nSigmoid, 64, 3, 50, 54, 53). Since the value of \ud835\udc62met (i.e., the number of\nhidden layers) differs from one point to another (namely, 2 for \ud835\udc62 and 3\nfor \ud835\udc63), the associated variables \ud835\udc62inc(\ud835\udc62met) have either 2 or 3 variables\nfor the number of neurons in each layer (namely 55 and 51 for \ud835\udc62,\nand 50, 54 and 53 for the point \ud835\udc63). In our case, 8 hyperparame-\nters ([\ud835\udc451]1,2, \ud835\udf031, \u2026 , \ud835\udf037) will have to be optimized where \ud835\udc58 is given by\nEq. (2). These 7 hyperparameters can be described using our proposed\nframework as follows:\n\u2022 For the neutral components, we have \ud835\udc62neu = (2.10\u22124, 0.9, ReLU, 16)\nand \ud835\udc63neu = (5.10\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical\nmatrix kernel \ud835\udc451 and a square exponential quantitative kernel,\n\ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat(\ud835\udc62cat\nneu, \ud835\udc63cat\nneu)\ud835\udc58qnt(\ud835\udc62qnt\nneu, \ud835\udc63qnt\nneu)\n= [\ud835\udc451]1,2 exp [\u2212\ud835\udf031(2.10\u22124 \u2212 5.10\u22123)2]\nexp [\u2212\ud835\udf032(0.9 \u2212 0.8)2] exp [\u2212\ud835\udf033(16 \u2212 64)2].\nThe values [\ud835\udc451]1,2, \ud835\udf031, \ud835\udf032 and \ud835\udf033 need to be optimized. Here,\n[\ud835\udc451]1,2 is the correlation between \"ReLU\" and \"Sigmoid\".\n\u2022 For the meta components, we have \ud835\udc62met\n= 2 and \ud835\udc63met\n= 3.\nTherefore, for a square exponential quantitative kernel,\n\ud835\udc58met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc58cat(\ud835\udc62cat\nmet, \ud835\udc63cat\nmet)\ud835\udc58qnt(\ud835\udc62qnt\nmet, \ud835\udc63qnt\nmet)\n= exp [\u2212\ud835\udf034(3 \u2212 2)2].\nThe value \ud835\udf034 needs to be optimized.\nAdvances in Engineering Software 188 (2024) 103571\n9\nP. Saves et al.\n\u2022 For the meta-decreed kernel, we have [\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)] = [2, (55, 51)]\nand [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)] = [3, (50, 54, 53)] which gives\n\ud835\udc58alg\nmet,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)])\n= \ud835\udc58alg\nmet(2, 3) \ud835\udc58alg\ndec((55, 51), (50, 54, 53)).\nThe distance \ud835\udc51alg(51, 54) =\n(\n2\u00d7|51\u221254|\n\u221a\n512+1\n\u221a\n542+1\n)\n\ud835\udf036 = 2.178.10\u22123 \ud835\udf036. In\ngeneral, for surrogate models, and in particular in SMT 2.0, the\ninput data are normalized. With a unit normalization from [50, 55]\nto [0, 1], we would have \ud835\udc51alg(0.2, 0.8) =\n(\n2\u00d70.6\n\u221a\n0.22+1\n\u221a\n0.62+1\n)\n\ud835\udf036 =\n0.919 \ud835\udf036. Similarly, we have, between 55 and 50, \ud835\udc51alg(0, 1) =\n1.414 \ud835\udf035. Hence, for a square exponential quantitative kernel, one\ngets\n\ud835\udc58alg\nmet,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)])\n= exp [\u2212\ud835\udf037] \u00d7 exp [\u22121.414 \ud835\udf035] \u00d7 exp [\u22120.919 \ud835\udf036],\nwhere the meta induced component is \ud835\udc58alg\nmet(\ud835\udc62met, \ud835\udc63met) = exp [\u2212\ud835\udf037]\nbecause the decreed value 53 in \ud835\udc63 has nothing to be compared\nwith in \ud835\udc62 as in Eq. (7). The values \ud835\udf035, \ud835\udf036 and \ud835\udf037 need to be opti-\nmized which complete the description of the hyperparameters.\nWe note that for the MLP problem, Alg-Kernel models use\n10 hyperparameters whereas the Arc-Kernel would require\n12 hyperparameters without the meta kernel (\ud835\udf034) but with 3\nextra decreed hyperparameters and the Wedge-Kernel would\nrequire 15 hyperparameters. For deep learning applications, a\nmore complex perceptron with up to 10 hidden layers would\nrequire 17 hyperparameters with SMT 2.0 models against 26\nfor Arc-Kernel and 36 for Wedge-Kernel. The next section\nillustrates the interest of our method to build a surrogate model\nfor this neural network engineering problem.\n4.3. A neural network test-case using SMT 2.0\nIn this section, we apply our models to the hyperparameters opti-\nmization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\nan example illustrates this MLP problem. For the sake of showing the\nKriging surrogate abilities, we implemented a dummy function with no\nsignificance to replace the real black-box that would require training\na whole Neural Network (NN) with big data. This function requires a\nnumber of variables that depends on the value of the meta variable,\ni.e the number of hidden layers. To simplify, we have chosen only\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables\nbut deep neural networks could also be investigated as our model can\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN)\nshows that our model SMT Alg-Kernel interpolates the data prop-\nerly, checks that the data dimension is correct and also asserts that\nthe inactive decreed variables have no influence over the prediction.\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical\nand mixed variables based on the implementation of SMT 2.0 for\ntest_hierarchical_variables_NN.\nTo compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel\nand SMT Arc-Kernel) with the state-of-the-art imputation method\npreviously used on industrial application (Imp-Kernel) [24], we\ndraw a 99 point LHS (33 points by meta level) as a training set and the\nvalidation set is a LHS of 3\u00d71000 = 3000 points. For the Imp-Kernel,\nthe decreed-excluded values are replaced by 52 because the mean value\n52.5 is not an integer (by default, SMT rounds to the floor value).\nFor the three methods, the precision (computed with a root-mean-\nsquare error RMSE criterion), the likelihood and the computational\ntime are shown in Table 6. For this problem, we can see that SMT Alg-\nkernel gives better performance than the imputation method or SMT\nArc-kernel. Also, as all methods use the same number of hyperpa-\nrameters, they have similar time performances. A direct application of\nTable 6\nResults on the neural network model.\nHierarchical method\nPrediction\nerror (RMSE)\nLikelihood\n# of\nhyperparam.\nSMT Alg-kernel\n3.7610\n176.11\n10\nSMT Arc-kernel\n4.9208\n162.01\n10\nImp-Kernel\n4.5455\n170.64\n10\nour modeling method is Bayesian optimization to perform quickly the\nhyperparameter optimization of a neural network [63].\n5. Bayesian optimization within SMT 2.0\nEfficient global optimization (EGO) is a sequential Bayesian op-\ntimization algorithm designed to find the optimum of a black-box\nfunction that may be expensive to evaluate [52]. EGO starts by fitting\na Kriging model to an initial DoE, and then uses an acquisition function\nto select the next point to evaluate. The most used acquisition function\nis the expected improvement. Once a new point has been evaluated, the\nKriging model is updated. Successive updates increase the model accu-\nracy over iterations. This enrichment process repeats until a stopping\ncriterion is met.\nBecause SMT 2.0 implements Kriging models that handle mixed\nand hierarchical variables, we can use EGO to solve problems in-\nvolving such design variables. Other Bayesian optimization algorithms\noften adopt approaches based on solving subproblems with contin-\nuous or non-hierarchical Kriging. This subproblem approach is less\nefficient and scales poorly, but it can only solve simple problems.\nSeveral Bayesian optimization software packages can handle mixed or\nhierarchical variables with such a subproblem approach. The pack-\nages include BoTorch [25], SMAC [65], Trieste [66], HEBO [67],\nOpenBox [68], and Dragonfly [69].\n5.1. A mixed optimization problem\nFig. 6 compares the four EGO methods implemented in SMT 2.0:\nSMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\nillustrates Bayesian optimization is a toy test case [64] detailed in Ap-\npendix A. This test case has two variables, one continuous and one\ncategorical with 10 levels. To assess the performance of our algorithm,\nwe performed 20 runs with different initial DoE sampled by LHS.\nEvery DoE consists of 5 points and we chose a budget of 55 infill\npoints. Fig. 6(a) plots the convergence curves for the four methods. In\nparticular, the median is the solid line, and the first and third quantiles\nare plotted in dotted lines. To visualize better the data dispersion,\nthe boxplots of the 20 best solutions after 20 evaluations are plotted\nin Fig. 6(b). As expected, the more a method is complex, the better\nthe optimization. Both SMT HH and SMT EHH converged in around 18\nevaluations whereas SMT CR and SMT GD take around 26 iterations as\nshown on Fig. 6(a). Also, the more complex the model, the closer the\noptimum is to the real value as shown on Fig. 6(b).\nIn Fig. 7 we illustrate how to use EGO with mixed variables based\non the implementation of SMT 2.0. The illustrated problem is a mixed\nvariant of the Branin function [70].\nNote that a dedicated notebook is available to reproduce the results\npresented in this paper and the mixed integer notebook also includes\nan extra mechanical application with composite materials [41].11\n11 https://colab.research.google.com/github/SMTorg/smt/blob/master/\ntutorial/SMT_MixedInteger_application.ipynb\nAdvances in Engineering Software 188 (2024) 103571\n10\nP. Saves et al.\nFig. 5. Example of usage of Hierarchical and Mixed Kriging surrogate.\nAdvances in Engineering Software 188 (2024) 103571\n11\nP. Saves et al.\nFig. 6. Optimization results for the Toy function [64].\nFig. 7. Example of usage of mixed surrogates for Bayesian optimization.\nAdvances in Engineering Software 188 (2024) 103571\n12\nP. Saves et al.\nFig. 8. Optimization results for the hierarchical Goldstein function.\n5.2. A hierarchical optimization problem\nThe hierarchical test case considered in this paper to illustrate\nBayesian optimization is a modified Goldstein function [60] detailed\nin Appendix B. The resulting optimization problem involves 11 vari-\nables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical.\nThese variables consist in 6 neutral variables, 1 dimensional (or meta)\nvariable and 4 decreed variables. Depending on the meta variable\nvalues, 4 different sub-problems can be identified. The optimization\nproblem is given by:\nmin \ud835\udc53(\ud835\udc65cat\nneu, \ud835\udc65qnt\nneu, \ud835\udc65cat\n\ud835\udc5a , \ud835\udc65qnt\ndec)\nw.r.t. \ud835\udc65cat\nneu = \ud835\udc642 \u2208 {0, 1}\n\ud835\udc65qnt\nneu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) \u2208 {0, 100}3 \u00d7 {0, 1, 2}2\n\ud835\udc65cat\n\ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3}\n\ud835\udc65qnt\ndec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) \u2208 {0, 100}2 \u00d7 {0, 1, 2}2\n(8)\nCompared to the model choice of Pelamatti et al. [60], we chose to\nmodel \ud835\udc655 and \ud835\udc642 as neutral variables even if \ud835\udc53 does not depend on\n\ud835\udc655 when \ud835\udc642 = 0. Other modeling choices are kept; for example, \ud835\udc642 is\na so-called \u2018\u2018binary variable\u2019\u2019 and not a categorical one [71]. Similarly,\nwe also keep the formulation of \ud835\udc641 as a categorical variable but a better\nmodel would be to model it as a \u2018\u2018cyclic variable\u2019\u2019 [72]. The resulting\nproblem is described in Appendix B. To assess the performance of our\nalgorithm, we performed 20 runs with different initial DoE sampled by\nLHS. Every DoE consists of \ud835\udc5b + 1 = 12 points and we chose a budget\nof 5\ud835\udc5b = 55 infill points. To compare our method with a baseline, we\nalso tested the random search method thanks to the expand_lhs\nnew method [40] described in Section 6.1 and we also optimized the\nGoldstein function using EGO with a classic Kriging model based on\nimputation method (Imp-Kernel). This method replaces the decreed-\nexcluded variables by their mean values: 50 or 1 respectively for (\ud835\udc653, \ud835\udc654)\nand (\ud835\udc671, \ud835\udc672). Fig. 8(a) plots the convergence curves for the four methods.\nIn particular, the median is the solid line and the first and third\nquantiles are plotted in dotted lines. To visualize better the correspond-\ning data dispersion, the boxplots of the 20 best solutions are plotted\nin Fig. 8(b). The results in Fig. 8 show that the hierarchical Kriging\nmodels of SMT 2.0 lead to better results than the imputation method\nor the random search both in terms of final objective value and variance\nover the 20 runs and in term of convergence rate. More precisely, SMT\nArc-Kernel and SMT Alg-Kernel Kriging model gave the best\nEGO results and managed to converge correctly as shown in Fig. 8(b).\nMore precisely, the 20 sampled DoEs led to similar performance and\nfrom one DoE, the method SMT Alg-Kernel managed to find the true\nminimum. However, this result has not been reproduced in other runs\nand is therefore not statistically significant. The variance between the\nruns is of similar magnitude regardless of the considered methods.\n6. Other relevant contributions in SMT 2.0\nThe new release SMT 2.0 introduces several improvements be-\nsides Kriging for hierarchical and mixed variables. This section details\nthe most important new contributions. Recall from Section 2.2 that\nfive sub-modules are present in the code: Sampling, Problems,\nSurrogate Models, Applications and Notebooks.\n6.1. Contributions to Sampling\nPseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\nstochastic sampling technique to generate quasi-random sampling dis-\ntributions. It is among the most popular sampling method in computer\nexperiments thanks to its simplicity and projection properties with\nhigh-dimensional problems. The LHS method uses the pyDOE package\n(Design Of Experiments for Python). Five criteria for the construction\nof LHS are implemented in SMT. The first four criteria (center,\nmaximin, centermaximin, correlation) are the same as in\npyDOE.12 The last criterion ese, is implemented by the authors of\nSMT [48]. In SMT 2.0 a new LHS method was developed for the\nNested design of experiments (NestedLHS) [73] to use with multi-\nfidelity surrogates. A new mathematical method (expand_lhs) [40]\nwas developed in SMT 2.0 to increase the size of a design of exper-\niments while maintaining the ese property. Moreover, we proposed\na sampling method for mixed variables, and the aforementioned LHS\nmethod was applied to hierarchical variables in Fig. 8.\n6.2. Contributions to Surrogate models\nNew kernels and their derivatives for Kriging. Kriging surrogates are\nbased on hyperparameters and on a correlation kernel. Four correla-\ntion kernels are now implemented in SMT 2.0 [74]. In SMT, these\ncorrelation functions are absolute exponential (abs_exp), Gaussian\n(squar_exp), Matern 5/2 (matern52) and Matern 3/2 (matern32).\nIn addition, the implementation of gradient and Hessian for each kernel\nmakes it possible to calculate both the first and second derivatives of\nthe GP likelihood with respect to the hyperparameters [5].\nVariance derivatives for Kriging. To perform uncertainty quantification\nfor system analysis purposes, it could be interesting to know more\nabout the variance derivatives of a model [75\u201377]. For that purpose\nand also to pursue the original publication about derivatives [5], SMT\n2.0 extends the derivative support to Kriging variances and kernels.\n12 https://pythonhosted.org/pyDOE/index.html\nAdvances in Engineering Software 188 (2024) 103571\n13\nP. Saves et al.\nNoisy Kriging. In engineering and in big data contexts with real exper-\niments, surrogate models for noisy data are of significant interest. In\nparticular, there is a growing need for techniques like noisy Kriging\nand noisy Multi-Fidelity Kriging (MFK) for data fusion [78]. For that\npurpose, SMT 2.0 has been designed to accommodate Kriging and MFK\nto noisy data including the option to incorporate heteroscedastic noise\n(using the use_het_noise option) and to account for different noise\nlevels for each data source [40].\nKriging with partial least squares. Beside MGP, for high-dimensional\nproblems, the toolbox implements Kriging with partial least squares\n(KPLS) [57] and its extension KPLSK [44]. The PLS information is\ncomputed by projecting the data into a smaller space spanned by the\nprincipal components. By integrating this PLS information into the\nKriging correlation matrix, the number of inputs can be scaled down,\nthereby reducing the number of hyperparameters required. The result-\ning number of hyperparameters \ud835\udc51\ud835\udc52 is indeed much smaller than the\noriginal problem dimension \ud835\udc51. Recently, in SMT 2.0, we extended the\nKPLS method for multi-fidelity Kriging (MFKPLS and MFKPLSK) [73,79,\n80]. We also proposed an automatic criterion to choose automatically\nthe reduced dimension \ud835\udc51\ud835\udc52 based on Wold\u2019s R criterion [81]. This\ncriterion has been applied to aircraft optimization with EGO where the\nnumber \ud835\udc51\ud835\udc52 (\ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code) for the model is automatically selected\nat every iteration [39]. Special efforts have been made to accommodate\nKPLS for multi-fidelity and mixed integer data [79,80].\nMarginal Gaussian process. SMT 2.0 implements Marginal Gaussian\nProcess (MGP) surrogate models for high dimensional problems [82].\nMGP are Gaussian processes taking into account hyperparameters un-\ncertainty defined as a density probability function. Especially we sup-\npose that the function to model \ud835\udc53 \u2236 \ud835\udefa \u21a6 R, where \ud835\udefa \u2282 R\ud835\udc51 and \ud835\udc51 is\nthe number of design variables, lies in a linear embedding \ue22d such as\n\ue22d = {\ud835\udc62 = \ud835\udc34\ud835\udc65, \ud835\udc65 \u2208 \ud835\udefa}, \ud835\udc34 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc52 and \ud835\udc53(\ud835\udc65) = \ud835\udc53\ue22d(\ud835\udc34\ud835\udc65) with \ud835\udc53(\ud835\udc65) = \ud835\udc53\ue22d \u2236\n\ue22d \u21a6 R and \ud835\udc51\ud835\udc52 \u226a \ud835\udc51. Then, we must use a kernel \ud835\udc58(\ud835\udc65, \ud835\udc65\u2032) = \ud835\udc58\ue22d(\ud835\udc34\ud835\udc65, \ud835\udc34\ud835\udc65\u2032)\nwhose each component of the transfer matrix \ud835\udc34 is an hyperparameter.\nThus we have \ud835\udc51\ud835\udc52 \u00d7 \ud835\udc51 hyperparameters to find. Note that \ud835\udc51\ud835\udc52 is defined\nas \ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code [49].\nGradient-enhanced neural network. The new release SMT 2.0 imple-\nments Gradient-Enhanced Neural Network (GENN) models [45].\nGradient-Enhanced Neural Networks (GENN) are fully connected multi-\nlayer perceptrons whose training process was modified to account for\ngradient information. Specifically, the model is trained to minimize not\nonly the prediction error of the response but also the prediction error\nof the partial derivatives: the chief benefit of gradient enhancement is\nbetter accuracy with fewer training points. Note that GENN applies to\nregression (single-output or multi-output), but not classification since\nthere is no gradient in that case. The implementation is fully vectorized\nand uses ADAM optimization, mini-batch, and L2-norm regularization.\nFor example, GENN can be used to learn airfoil geometries from a\ndatabase. This usage is documented in SMT 2.0.13\n6.3. Contributions to Applications\nKriging trajectory and sampling. Sampling a GP with high resolution\nis usually expensive due to the large dimension of the associated\ncovariance matrix. Several methods are proposed to draw samples\nof a GP on a given set of points. To sample a conditioned GP, the\nclassic method consists in using a Cholesky decomposition (or eigende-\ncomposition) of the conditioned covariance matrix of the process but\nsome numerical computational errors can lead to non SPD matrix. A\nmore recent approach based on Karhunen\u2013Lo\u00e8ve decomposition of the\ncovariance kernel with the Nystr\u00f6m method has been proposed in [83]\n13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_\nparameters/learning_airfoil_parameters.html\nwhere the paths can be sampled by generating independent standard\nNormal distributed samples. The different methods are documented in\nthe tutorial Gaussian Process Trajectory Sampling [84].\nParallel Bayesian optimization. Due to the recent progress made in\nhardware configurations, it has been of high interest to perform parallel\noptimizations. A parallel criterion called qEI [85] was developed to\nperform Efficient Global Optimization (EGO): the goal is to be able\nto run batch optimization. At each iteration of the algorithm, multiple\nnew sampling points are extracted from the known ones. These new\nsampling points are then evaluated using a parallel computing environ-\nment. Five criteria are implemented in SMT 2.0: Kriging Believer (KB),\nKriging Believer Upper Bound (KBUB), Kriging Believer Lower Bound\n(KBLB), Kriging Believer Random Bound (KBRand) and Constant Liar\n(CLmin) [86].\n7.\nConclusion\nSMT 2.0 introduces significant upgrades to the Surrogate Modeling\nToolbox. This new release adds support for hierarchical and mixed\nvariables and improves the surrogate models with a particular focus\non Kriging (Gaussian process) models. SMT 2.0 is distributed through\nan open-source license and is freely available online.14 We provide\ndocumentation that caters to both users and potential developers.15\nSMT 2.0 enables users and developers collaborating on the same\nproject to have a common surrogate modeling tool that facilitates the\nexchange of methods and reproducibility of results.\nSMT has been widely used in aerospace and mechanical modeling\napplications. Moreover, the toolbox is general and can be useful for\nanyone who needs to use or develop surrogate modeling techniques,\nregardless of the targeted applications. SMT is currently the only open-\nsource toolbox that can build hierarchical and mixed surrogate models.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-\ncial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nData will be made available on request. Results can be reproduced\nfreely online at https://colab.research.google.com/github/SMTorg/smt/\nblob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\nAcknowledgments\nWe want to thank all those who contribute to this release. Namely,\nM. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde\nArenzana, S. Dubreuil, A. F. L\u00f3pez-Lopera, M. Meliani, M. Menz, N.\nMo\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is\npart of the activities of ONERA - ISAE - ENAC joint research group. We\nalso acknowledge the partners institutions: ONERA, NASA Glenn, ISAE-\nSUPAERO, Institut Cl\u00e9ment Ader (ICA), the University of Michigan,\nPolytechnique Montr\u00e9al and the University of California San Diego.\nThe research presented in this paper has been performed in the\nframework of the AGILE 4.0 project (Towards cyber-physical collabo-\nrative aircraft development), funded by the European Union Horizon\n2020 research and innovation framework programme under grant\nagreement n\u25e6 815122 and in the COLOSSUS project (Collaborative\nSystem of Systems Exploration of Aviation Products, Services and\n14 https://github.com/SMTorg/SMT\n15 https://smt.readthedocs.io/en/latest/\nAdvances in Engineering Software 188 (2024) 103571\n14\nP. Saves et al.\nBusiness Models) funded by the European Union Horizon Europe re-\nsearch and innovation framework programme under grant agreement\nn\u25e6 101097120.\nWe also are grateful to E. Hall\u00e9-Hannan from Polytechnique Mon-\ntr\u00e9al for the hierarchical variables framework.\nAppendix A. Toy test function\nThis Appendix gives the detail of the toy function of Section 5.1.16\nFirst, we recall the optimization problem:\nmin \ud835\udc53(\ud835\udc65cat, \ud835\udc65qnt)\nw.r.t. \ud835\udc65cat = \ud835\udc501 \u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\ud835\udc65qnt = \ud835\udc651 \u2208 [0, 1]\n(A.1)\nThe toy function \ud835\udc53 is defined as\n\ud835\udc53(\ud835\udc65, \ud835\udc501) =1\ud835\udc501=0 cos(3.6\ud835\udf0b(\ud835\udc65 \u2212 2)) + \ud835\udc65 \u2212 1\n+1\ud835\udc501=1 2 cos(1.1\ud835\udf0b exp(\ud835\udc65)) \u2212 \ud835\udc65\n2 + 2\n+1\ud835\udc501=2 cos(2\ud835\udf0b\ud835\udc65) + 1\n2\ud835\udc65\n+1\ud835\udc501=3 \ud835\udc65(cos(3.4\ud835\udf0b(\ud835\udc65 \u2212 1)) \u2212 \ud835\udc65 \u2212 1\n2\n)\n+1\ud835\udc501=4 \u2212 \ud835\udc652\n2\n+1\ud835\udc501=5 2 cos(0.25\ud835\udf0b exp(\u2212\ud835\udc654))2 \u2212 \ud835\udc65\n2 + 1\n+1\ud835\udc501=6 \ud835\udc65 cos(3.4\ud835\udf0b\ud835\udc65) \u2212 \ud835\udc65\n2 + 1\n+1\ud835\udc501=7 \u2212 \ud835\udc65(cos(3.5\ud835\udf0b\ud835\udc65) + \ud835\udc65\n2 ) + 2\n+1\ud835\udc501=8 \u2212 \ud835\udc655\n2 + 1\n+1\ud835\udc501=9 \u2212 cos(2.5\ud835\udf0b\ud835\udc65)2\u221a\n\ud835\udc65 \u2212 0.5 ln(\ud835\udc65 + 0.5) \u2212 1.3\n(A.2)\nAppendix B. Hierarchical Goldstein test function\nThis Appendix gives the detail of the hierarchical Goldstein problem\nof Section 5.2.17 First, we recall the optimization problem:\nmin \ud835\udc53(\ud835\udc65cat\nneu, \ud835\udc65qnt\nneu, \ud835\udc65cat\n\ud835\udc5a , \ud835\udc65qnt\ndec)\nw.r.t. \ud835\udc65cat\nneu = \ud835\udc642 \u2208 {0, 1}\n\ud835\udc65qnt\nneu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) \u2208 [0, 100]3 \u00d7 {0, 1, 2}2\n\ud835\udc65cat\n\ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3}\n\ud835\udc65qnt\ndec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) \u2208 [0, 100]2 \u00d7 {0, 1, 2}2\n(B.1)\nThe hierarchical and mixed function \ud835\udc53 is defined as a hierarchical\nfunction that depends on \ud835\udc530, \ud835\udc531, \ud835\udc532 and \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as describes in the\nfollowing.\n\ud835\udc53(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc641, \ud835\udc642) =\n1\ud835\udc641=0\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc641=1\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc641=2\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc641=3\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642).\n(B.2)\n16 https://github.com/jbussemaker/SBArchOpt\n17 https://github.com/jbussemaker/SBArchOpt\nThen, the functions \ud835\udc530, \ud835\udc531 and \ud835\udc532 are defined as mixed variants of\n\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as such\n\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =\n1\ud835\udc672=0\n( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) )\n1\ud835\udc672=1\n( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) )\n1\ud835\udc672=2\n( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) )\n(B.3)\n\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =\n1\ud835\udc672=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc672=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc672=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =\n1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, 50, \ud835\udc652, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\n+ 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)\nTo finish with, the function \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont is given by\n\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 53.3108 + 0.184901\ud835\udc651\n\u2212 5.02914\ud835\udc651\n3.10\u22126 + 7.72522\ud835\udc651\n\ud835\udc673.10\u22128 \u2212 0.0870775\ud835\udc652 \u2212 0.106959\ud835\udc653\n+ 7.98772\ud835\udc653\n\ud835\udc674.10\u22126 + 0.00242482\ud835\udc654 + 1.32851\ud835\udc654\n3.10\u22126 \u2212 0.00146393\ud835\udc651\ud835\udc652\n\u2212 0.00301588\ud835\udc651\ud835\udc653 \u2212 0.00272291\ud835\udc651\ud835\udc654 + 0.0017004\ud835\udc652\ud835\udc653 + 0.0038428\ud835\udc652\ud835\udc654\n\u2212 0.000198969\ud835\udc653\ud835\udc654 + 1.86025\ud835\udc651\ud835\udc652\ud835\udc653.10\u22125 \u2212 1.88719\ud835\udc651\ud835\udc652\ud835\udc654.10\u22126\n+ 2.50923\ud835\udc651\ud835\udc653\ud835\udc654.10\u22125 \u2212 5.62199\ud835\udc652\ud835\udc653\ud835\udc654.10\u22125 + \ud835\udc642\n(\n5 cos\n( 2\ud835\udf0b\n100 \ud835\udc655\n)\n\u2212 2\n)\n.\n(B.4)\nAppendix C. Supplementary data\nMore at https://colab.research.google.com/github/SMTorg/smt/blob/\nmaster/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\nSupplementary material related to this article can be found online\nat https://doi.org/10.1016/j.advengsoft.2023.103571.\nReferences\n[1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach\nfor the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\u201373.\n[2] Kennedy M, O\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc\nSer B Stat Methodol 2001;63:425\u201364.\n[3] Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets.\nAerosp Sci Technol 2018;75:74\u201387.\n[4] Martins JRRA, Ning A. Engineering design optimization. Cambridge University\nPress; 2021.\n[5] Bouhlel\nMA,\nHwang\nJT,\nBartoli\nN,\nLafage\nR,\nMorlier\nJ,\nMartins\nJRA.\nA Python surrogate modeling framework with derivatives. Adv Eng Softw\n2019;135:102662.\n[6] Bouhlel\nMA,\nMartins\nJ.\nGradient-enhanced\nkriging\nfor\nhigh-dimensional\nproblems. Eng Comput 2019;35:157\u201373.\n[7] Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikit-learn:\nMachine learning in Python. J Mach Learn Res 2011;12:2825\u201330.\nAdvances in Engineering Software 188 (2024) 103571\n15\nP. Saves et al.\n[8] Lataniotis C, Marelli S, Sudret B. Uqlab 2.0 and uqcloud: open-source vs.\ncloud-based uncertainty quantification. In: SIAM conference on uncertainty\nquantification. 2022.\n[9] Faraci A, Beaurepaire P, Gayton N. Review on Python toolboxes for Kriging\nsurrogate modelling. In: ESREL. 2022.\n[10] Kr\u00fcgener M, Zapata Usandivaras J, Bauerheim M, Urbano A. Coaxial-injector\nsurrogate modeling based on Reynolds-averaged Navier\u2013Stokes simulations using\ndeep learning. J Propuls Power 2022;38:783\u201398.\n[11] Ming D, Williamson D, Guillas S. Deep Gaussian process emulation using\nstochastic imputation. Technometrics 2022;1\u201312.\n[12] Eli\u00e1\u0161 J, Vo\u0159echovsk`y M, Sad\u00edlekv V. Periodic version of the minimax distance\ncriterion for Monte Carlo integration. Adv Eng Softw 2020;149:102900.\n[13] Drouet V, Balesdent M, Brevault L, Dubreuil S, Morio J. Multi-fidelity algo-\nrithm for the sensitivity analysis of multidisciplinary problems. J Mech Des\n2023;145:1\u201322.\n[14] Karban P, P\u00e1nek D, Orosz T, Petr\u00e1\u0161ov\u00e1 I, Dole\u017eel I. FEM based robust design\noptimization with Agros and \u00afArtap. Comput Math Appl 2021;81:618\u201333.\n[15] Kudela J, Matousek R. Recent advances and applications of surrogate models for\nfinite element method computations: a review. Soft Comput 2022;26:13709\u201333.\n[16] Chen Y, Dababneh F, Zhang B, Kassaee S, Smith BT, Liu K, et al. Surrogate mod-\neling for capacity planning of charging station equipped with photovoltaic panel\nand hydropneumatic energy storage. J Energy Res Technol 2020;142:050907.\n[17] Jasa\nJ,\nBortolotti\nP,\nZalkind\nD,\nBarter\nG.\nEffectively\nusing\nmultifidelity\noptimization for wind turbine design. Wind Energy Sci 2022;7:991\u20131006.\n[18] Wang W, Tao G, Ke D, Luo J, Cui J. Transpiration cooling of high pres-\nsure turbine vane with optimized porosity distribution. Appl Therm Eng\n2023;223:119831.\n[19] Savage T, Almeida-Trasvina HF, del R\u00edo-Chanona EA, Smith R, Zhang D.\nAn adaptive data-driven modelling and optimization framework for complex\nchemical process design. Comput Aided Chem Eng 2020;48:73\u20138.\n[20] Chan A, Pires AF, Polacsek T. Trying to elicit and assign goals to the right actors.\nIn: Conceptual modeling: 41st international conference. 2022.\n[21] Hutter F, Osborne MA. A kernel for hierarchical parameter spaces. 2013, arXiv.\n[22] Bussemaker JH, Ciampa PD, Nagel B. System architecture design space explo-\nration: An approach to modeling and optimization. In: AIAA aviation 2020 forum.\n2020.\n[23] Fouda MEA, Adler EJ, Bussemaker J, Martins JRRA, Kurtulus DF, Boggero L,\net al. Automated hybrid propulsion model construction for conceptual aircraft\ndesign and optimization. In: 33rd congress of the international council of the\naeronautical sciences. 2022.\n[24] Bussemaker JH, Bartoli N, Lefebvre T, Ciampa PD, Nagel B. Effectiveness of\nsurrogate-based optimization algorithms for system architecture optimization. In:\nAIAA aviation 2021 forum. 2021.\n[25] Balandat M, Karrer B, Jiang D, Daulton S, Letham B, Wilson A, et al. BoTorch:\nA framework for efficient Monte-Carlo Bayesian optimization. Adv Neural Inf\nProcess Syst 2020;33:21524\u201338.\n[26] Adams B, Bohnhoff W, Dalbey K, Ebeida M, Eddy J, Eldred M, et al. Dakota,\na multilevel parallel object-oriented framework for design optimization, pa-\nrameter estimation, uncertainty quantification, and sensitivity analysis: Version\n6.13 user\u2019s manual. Technical report, Albuquerque, NM (United States: Sandia\nNational Lab.(SNL-NM); 2020.\n[27] Roustant O, Ginsbourger D, Deville Y. DiceKriging, DiceOptim: Two R packages\nfor the analysis of computer experiments by Kriging-based metamodeling and\noptimization. J Stat Softw 2012;51:1\u201355.\n[28] Zhang Y, Tao S, Chen W, Apley D. A latent variable approach to Gaus-\nsian process modeling with qualitative and quantitative factors. Technometrics\n2020;62:291\u2013302.\n[29] Chang TH, Wild SM. ParMOO: A Python library for parallel multiobjective\nsimulation optimization. J Open Source Softw 2023;8:4468.\n[30] Garrido-Merch\u00e1n\nEC,\nHern\u00e1ndez-Lobato\nD.\nDealing\nwith\ncategorical\nand\ninteger-valued variables in Bayesian optimization with Gaussian processes.\nNeurocomputing 2020;380:20\u201335.\n[31] Halstrup M. Black-box optimization of mixed discrete-continuous optimization\nproblems (Ph.D. thesis), TU Dortmund; 2016.\n[32] Roustant O, Padonou E, Deville Y, Cl\u00e9ment A, Perrin G, Giorla J, et al.\nGroup kernels for gaussian process metamodels with categorical inputs. SIAM\nJ Uncertain Quant 2020;8:775\u2013806.\n[33] Zhou Q, Qian PZG, Zhou S. A simple approach to emulation for computer models\nwith qualitative and quantitative factors. Technometrics 2011;53:266\u201373.\n[34] Saves P, Diouane Y, Bartoli N, Lefebvre T, Morlier J. A mixed-categorical\ncorrelation kernel for Gaussian process. Neurocomputing 2023;550:126472.\n[35] Pelamatti\nJ,\nBrevault\nL,\nBalesdent\nM,\nTalbi\nE-G,\nGuerin\nY.\nEfficient\nglobal optimization of constrained mixed variable problems. J Global Optim\n2019;73:583\u2013613.\n[36] Horn D, Stork J, ler N-JS, Zaefferer M. Surrogates for hierarchical search spaces:\nThe Wedge-Kernel and an automated analysis. In: Proceedings of the genetic and\nevolutionary computation conference. 2019.\n[37] Hung Y, Joseph VR, Melkote SN. Design and analysis of computer experiments\nwith branching and nested factors. Technometrics 2009;51:354\u201365.\n[38] Audet C, Hall\u00e9-Hannan E, Le Digabel S. A general mathematical framework\nfor constrained mixed-variable blackbox optimization problems with meta and\ncategorical variables. Oper Res Forum 2023;4:1\u201337.\n[39] Saves P, Nguyen Van E, Bartoli N, Diouane Y, Lefebvre T, David C, Defoort S,\nMorlier J. Bayesian optimization for mixed variables using an adaptive dimension\nreduction process: applications to aircraft design. In: AIAA scitech 2022. 2022.\n[40] Conde Arenzana R, L\u00f3pez-Lopera A, Mouton S, Bartoli N, Lefebvre T. Multi-\nfidelity Gaussian process model for CFD and wind tunnel data fusion. In:\nECCOMAS aerobest. 2021.\n[41] Rufato RC, Diouane Y, Henry J, Ahlfeld R, Morlier J. A mixed-categorical\ndata-driven approach for prediction and optimization of hybrid discontinuous\ncomposites performance. In: AIAA aviation 2022 forum. 2022.\n[42] Gorissen D, Crombecq K, Couckuyt I, Dhaene T, Demeester P. A surrogate\nmodeling and adaptive sampling toolbox for computer based design. J Mach\nLearn Res 2010;11:2051\u20135.\n[43] Williams CK, Rasmussen CE. Gaussian processes for machine learning. MA: MIT\npress Cambridge; 2006.\n[44] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. Efficient Global Opti-\nmization for high-dimensional constrained problems by using the Kriging models\ncombined with the Partial Least Squares method. Eng Optim 2018;50:2038\u201353.\n[45] Bouhlel MA, He S, Martins J. Scalable gradient-enhanced artificial neural\nnetworks for airfoil shape design in the subsonic and transonic regimes. Struct\nMultidiscip Optim 2020;61:1363\u201376.\n[46] Kwan LS, Pitrou A, Seibert S. Numba: A LLVM-based python JIT compiler. In:\nProceedings of the second workshop on the LLVM compiler infrastructure in\nHPC. 2015.\n[47] Zaefferer M, Horn D. A first analysis of kernels for Kriging-based optimization\nin hierarchical search spaces. 2018, arXiv.\n[48] Jin R, Chen W, Sudjianto A. An efficient algorithm for constructing optimal\ndesign of computer experiments. J Statist Plann Inference 2005;2:545\u201354.\n[49] Garnett R, Osborne M, Hennig P. Active learning of linear embeddings for\nGaussian processes. In: Uncertainty in artificial intelligence - Proceedings of the\n30th conference. 2013.\n[50] Jones D. A taxonomy of global optimization methods based on response surfaces.\nJ Global Optim 2001;21:345\u201383.\n[51] Lafage R. egobox, a Rust toolbox for efficient global optimization. J Open Source\nSoftw 2022;7:4737.\n[52] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive\nblack-box functions. J Global Optim 1998;13:455\u201392.\n[53] Deng X, Lin CD, Liu K, Rowe RK. Additive Gaussian process for computer models\nwith qualitative and quantitative factors. Technometrics 2017;59:283\u201392.\n[54] Cuesta-Ramirez J, Le Riche R, Roustant O, Perrin G, Durantin C, Gliere A. A\ncomparison of mixed-variables Bayesian optimization approaches. Adv Model\nSimul Eng Sci 2021;9:1\u201329.\n[55] Rebonato R, Jaeckel P. The most general methodology to create a valid\ncorrelation matrix for risk management and option pricing purposes. J Risk\n2001;2:17\u201327.\n[56] Rapisarda F, Brigo D, Mercurio F. Parameterizing correlations: a geometric\ninterpretation. IMA J Manag Math 2007;18:55\u201373.\n[57] Bouhlel\nMA,\nBartoli\nN,\nRegis\nR,\nOtsmane\nA,\nMorlier\nJ.\nAn\nimproved\napproach for estimating the hyperparameters of the Kriging model for high-\ndimensional problems through the Partial Least Squares method. Math Probl\nEng 2016;2016:6723410.\n[58] Cheng GH, Younis A, Hajikolaei KH, Wang GG. Trust region based mode pursuing\nsampling method for global optimization of high dimensional design problems.\nJ Mech Des 2015;137:021407.\n[59] Karlsson R, Bliek L, Verwer S, de Weerdt M. Continuous surrogate-based\noptimization algorithms are well-suited for expensive discrete problems. In:\nArtificial intelligence and machine learning. 2021.\n[60] Pelamatti J, Brevault L, Balesdent M, Talbi E-G, Guerin Y. Bayesian optimization\nof variable-size design space problems. Opt Eng 2021;22:387\u2013447.\n[61] Hebbal A, Brevault L, Balesdent M, Talbi E-G, Melab N. Bayesian optimization\nusing deep Gaussian processes with applications to aerospace system design. Opt\nEng 2021;22:321\u201361.\n[62] Wildberger N. A rational approach to trigonometry. Math Horiz 2007;15:16\u201320.\n[63] Cho H, Kim Y, Lee E, Choi D, Lee Y, Rhee W. Basic enhancement strategies when\nusing bayesian optimization for hyperparameter tuning of deep neural networks.\nIEEE Access 2020;8:52588\u2013608.\n[64] Zuniga MM, Sinoquet D. Global optimization for mixed categorical-continuous\nvariables based on Gaussian process models with a randomized categorical space\nexploration step. INFOR Inf Syst Oper Res 2020;58:310\u201341.\n[65] Lindauer M, Eggensperger K, Feurer M, AB, Deng D, Benjamins C, et al. SMAC3:\nA versatile Bayesian optimization package for hyperparameter optimization. J\nMach Learn Res 2022;23:1\u20139.\n[66] Picheny V, Berkeley J, Moss H, Stojic H, Granta U, Ober S, et al. Trieste:\nEfficiently exploring the depths of black-box functions with TensorFlow. 2023,\narXiv.\n[67] Cowen-Rivers AI, Ly W, Wang Z, Tutunov R, Jianye H, Wang J, et al. HEBO:\nHeteroscedastic evolutionary Bayesian optimisation. 2020, arXiv.\n[68] Jiang H, Shen Y, Li Y, Zhang W, Zhang C, Cui B. OpenBox: A Python toolkit for\ngeneralized black-box optimization. 2023, arXiv.\nAdvances in Engineering Software 188 (2024) 103571\n16\nP. Saves et al.\n[69] Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et\nal. Tuning hyperparameters without grad students: Scalable and robust bayesian\noptimisation with dragonfly. J Mach Learn Res 2020;21:3098\u2013124.\n[70] Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient\nglobal optimization algorithm with multiple infill strategy - Applied to a wing\ntopology optimization problem. In: AIAA scitech 2019 forum. 2019.\n[71] M\u00fcller J, Shoemaker CA, Pich\u00e9 R. SO-MI: A surrogate model algorithm for\ncomputationally expensive nonlinear mixed-integer black-box global optimization\nproblems. Comput Oper Res 2013;40:1383\u2013400.\n[72] Tran T, Sinoquet D, Da Veiga S, Mongeau M. Derivative-free mixed binary\nnecklace optimization for cyclic-symmetry optimal design problems. Opt Eng\n2021.\n[73] Meliani M, Bartoli N, Lefebvre T, Bouhlel MA, Martins JRRA, Morlier J. Multi-\nfidelity efficient global optimization: Methodology and application to airfoil\nshape design. In: AIAA aviation 2019 forum. 2019.\n[74] Lee H. Gaussian processes. Springer Berlin Heidelberg; 2011, p. 575\u20137.\n[75] L\u00f3pez-Lopera AF, Idier D, Rohmer J, Bachoc F. Multioutput Gaussian processes\nwith functional data: A study on coastal flood hazard assessment. Reliab Eng\nSyst Saf 2022;218:108139.\n[76] Berthelin G, Dubreuil S, Sala\u00fcn M, Bartoli N, Gogu C. Disciplinary proper\northogonal decomposition and interpolation for the resolution of parameterized\nmultidisciplinary analysis. Internat J Numer Methods Engrg 2022;123:3594\u2013626.\n[77] Cardoso I, Dubreuil S, Bartoli N, Gogu C, Sala\u00fcn M, Lafage R. Disciplinary\nsurrogates for gradient-based optimization of multidisciplinary systems. In:\nECCOMAS Aerobest. 2023.\n[78] Platt J, Penny S, Smith T, Chen T, Abarbanel H. A systematic exploration of\nreservoir computing for forecasting complex spatiotemporal dynamics. Neural\nNetw 2022;153:530\u201352.\n[79] Charayron R, Lefebvre T, Bartoli N, Morlier J. Multi-fidelity Bayesian optimiza-\ntion strategy applied to overall drone design. In: AIAA scitech 2023 forum.\n2023.\n[80] Charayron R, Lefebvre T, Bartoli N, Morlier J. Towards a multi-fidelity and\nmulti-objective Bayesian optimization efficient algorithm. Aerosp Sci Technol\n2023;142:108673.\n[81] Wold H. Soft modelling by latent variables: The non-linear iterative partial least\nsquares (NIPALS) approach. J Appl Probab 1975;12:117\u201342.\n[82] Priem R, Diouane Y, Bartoli N, Dubreuil S, Saves P. High-dimensional efficient\nglobal optimization using both random and supervised embeddings. In: AIAA\naviation 2023 forum. 2023.\n[83] Betz W, Papaioannou I, Straub D. Numerical methods for the discretization of\nrandom fields by means of the Karhunen\u2013Lo\u00e8ve expansion. Comput Methods\nAppl Mech Engrg 2014;271:109\u201329.\n[84] Menz M, Dubreuil S, Morio J, Gogu C, Bartoli N, Chiron M. Variance based sen-\nsitivity analysis for Monte Carlo and importance sampling reliability assessment\nwith Gaussian processes. Struct Saf 2021;93:102116.\n[85] Ginsbourger D, Le Riche R, Carraro L. Kriging is well-suited to parallelize\noptimization. Springer Berlin Heidelberg; 2010, p. 131\u201362.\n[86] Roux E, Tillier Y, Kraria S, Bouchard P-O. An efficient parallel global opti-\nmization strategy based on Kriging properties suitable for material parameters\nidentification. Arch Mech Eng 2020;67.\n",
    "pdf_url": "/media/Article_13_d9faeab0352f463bbc61b5c10e4bdb5b.pdf",
    "references": [
      "[1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach",
      "for the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\u201373.",
      "[2] Kennedy M, O\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc",
      "Ser B Stat Methodol 2001;63:425\u201364.",
      "[3] Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets."
    ],
    "publication_date": "2023-12-07T00:00:00",
    "corrected": 0,
    "_id": "nJ02bI0BmS06k2rerDWH"
  }
]